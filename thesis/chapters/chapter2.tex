\chapter{\acl{sota}}
\label{chapter:sota}

\section{Datasets}
\label{section:sota:datasets}

Training an autonomous vehicle to be capable of driving himself is a complex challenge with multiple parts and problems to be solve: recognizing other users of the road (persons, vehicles, cyclists); understanding traffic signs; track and estimate the movement of the road users; plan the path; accurate modelling of the environment; among many others. To both train neural networks and other genetic algorithms, but also to intensevely test systems and other software, huge amounts of data are required.

To accelerate this progress, a collaborative effort is being made to release datasets online for free public usage under permissive licenses. This datasets vary in their objective, sensory data available, conditions whose data has been acquired, driving conditions and format on which they are provided, among other aspects. 

Despite this work having no intentions to study autonomous driving, the algorithms developed for calibration, sensor fusion and computing the correspondence between objects of interest in image and point cloud are not meant to be particular applied on datasets with interference. Therefore, using public available datasets not only allow the development of those algorithms before experimental data can be gathered, but also wides the aplicability of such work, without losing their applicability to the particular situation of \ac{lidar} interference.

During the months when this research work was carried, several new datasets were made available online, such as nuScenes~\cite{nuScenes2019} and Waymo~\cite{Waymo}. Since no new contribute was expectd from their usage besides the ones already provided by the datasets already in use on this research, no further reseach on them was carried. 

For the purposes of this research, several datasets were considered and tested. On the next sections, a brief summary is given about the ones that have sensory data from camera and \ac{lidar}. Those were the candidates to be used on this work.

\subsection{Ford Campus \ac{lidar} dataset}
Gathered in 2009, in Michigan, the Ford Campus \ac{lidar} dataset contains camera, \ac{lidar}, \ac{imu} and \ac{gps} data. The dataset consists of two test scenarios, one inside the Ford Research campus and another on downtown Dearborn. A small subset of the former test scenario is also provided.

The data is provided in raw format, accompanied by log files with timestamps, \ac{gps} and \ac{lcm}\footnote{For more information on \acf{lcm} protocol for estimating delays between the registered of sensory data on master-slave systems, see~\cite{VelodyneHDL64}.} logs with all raw data. The images from an omnidirectional camera, a Point Grey Ladybug3, are stored on \ac{ppm}; \ac{lidar} data from a Velodyne HDL-64E on \ac{pcap} format, from the \ac{tcp} connection socket; two Riegl LMS-Q120 \acp{lidar} also provide more information on the scan.

The data is rectified and synced under \ac{matlab} \textit{.mat} files. Along with the raw data and synced and rectified \textit{.mat} files, source code is also provided for parsing the raw data, visualizing the \ac{lcm} logs and pre-processed data on \ac{matlab}. A C and \ac{opengl} software that can render textured point clouds is also available.

The modified Ford F-250 pickup truck, which can be seen on figure~\ref{fig:sota:ford_sensors}, uses 3 sensors for navigation~\cite{Pandey2011}: one 3D \ac{lidar}, Velodyne HDL-64E \ac{lidar}~\cite{VelodyneHDL64}; one camera: Point Grey Ladybug3 omnidirectional camera; and two 2D \acp{lidar}: Riegl LMS-Q120 lidar. More details about the sensors, their relative positioning and data formats and files can be found~\cite{Pandey2011}.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{img/sensor_fusion/ford_sensors.jpg}
	\caption{Ford 250 pickup equipped with the sensors descibed in the previous paragraphs. On the top, the Ladybug omnidirectional camera, on the back the \ac{imu} and \ac{gps} unit. }
	\label{fig:sota:ford_sensors}
\end{figure}


\subsection{\ac{kitti}}
A well known dataset for researchers of computer vision, autonomous driving and \ac{ml}, \ac{kitti} was recorded in 2011 and released for the public in 2013. This dataset contains various driving scenarios: suburban, highways, residential and campus areas; with trucks, cars, cyclists and persons. Alongside with data for testing, calibration measures are provided for all sensors.

The test car, a Volkswagen Passat is equipped with two stereo pairs, one with color and other with gray cameras, a \ac{lidar}, an \ac{imu} and a\ac{gps} sensor. Data from all four cameras is stored on \ac{png} format, \ac{lidar} measurements as a binary float matrix, \ac{gps} and {imu} data textually. Additionally to the raw data, logs containing the timestamps and the transforms between the sensors are also provided. Labeled data is also available for some test scenarios on \ac{xml} files.

Along with the data and calibration parameters, several tools written in C++ or \ac{matlab} are also provided. The dataset offers two types of data categories: (1) unsynced and unrectified data or (2) synced and rectified data. 

The sensory apparatus contains 2 PointGray Flea2 greyscale and color cameras, a Velodyne HDL-64E \ac{lidar}~\cite{VelodyneHDL64}, among others less relevant sensors~\cite{Geiger2013a}.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{img/sensor_fusion/passat_sensors.jpg}
	\caption{Volkswagen Passat equipped with the sensors descibed in the previous paragraphs, for the \ac{kitti} dataset. On the top, the Velodyne \ac{lidar} and below the 2 stereo pairs (color and grey). On the back, the \ac{imu} and {gps} systems are present.}
	\label{fig:sota:kitti_sensors}
\end{figure}


\subsection{Udacity Self-Driving Car Nanodegree Dataset}
Available publicly is also the data from Udacity online course on self-driving car. This dataset dates back to 2016 and was gathered with the intent to develop a level 4 autonomy vehicle, containing much more data diversity than the previous two. The data available contains images from 3 color cameras, a \ac{lidar} data, \ac{imu} and \ac{gps}, among other sensory data, such as speed, braking, etc.

Their data is not available in raw format, only structured in \ac{ros} \textit{.bag} files. Some tools for visualizing and interacting with their data are provided for \ac{ros}~\cite{udacity}. 

Not much information is publicaly available, and the dataset contains to calibration data between its sensors.

\subsection{Summary}
Table~\ref{tab:sota:datasets_comparison} summarizes all the relevant data from the datasets~\cite{udacity, Pandey2011, Geiger2013a}. One notices that while there are few differences between the relevant types of data gather, major differences can be observed on the format in which they are provided. 

The diversity of scenarios and size of the dataset is also an aspect to considered. On this matter, \ac{kitti} and Udacity's are superior to Ford dataset, with \ac{kitti} providing the largest dataset in quantity, with already rectified and synced data. 

While Udacity dataset is the newest and provides direct out-off-the-shelf integration with \ac{ros}, that type of integration can also be achieved for \ac{kitti} by using other tools, such as \textit{kitti2bag}~\cite{TomasKrejci}. 
	
\begin{table}[H]
	 \rowcolors{4}{gray!10}{white}
	 \renewcommand{\arraystretch}{1.2}
	 \centering
	\begin{tabular}{llccc}
																& & \multicolumn{3}{c}{Datasets} \\ \cline{3-5}
																& & Ford Campus  & \acs{kitti} & Udacity \\ \midrule
																%
																& \ac{lidar}	   & \checkmark  & \checkmark & \checkmark \\ 
																& Color Camera	 & \checkmark  & \checkmark & \checkmark  \\
																& Grey Cameras   &             & \checkmark &  \\
																& Stereo Camera  &             & \checkmark & \checkmark  \\
																& Omnidirectional Camera &  \checkmark  &  &  \\
																& \acs{imu}      & \checkmark  & \checkmark & \checkmark  \\
																& \acs{gps}      & \checkmark  & \checkmark & \checkmark  \\
		\rowcolor{white}\multirow{-8}{*}{Sensors and Data} & Driving data\footnotemark & & & \checkmark \\ \hline
		\multirow{5}{*}{Data Formats and Tools} & Raw data available & \checkmark & \checkmark &  \\
																			 & Data parsing tools & \checkmark & \checkmark & \checkmark  \\
																			 & Rectified data & \checkmark & \checkmark & \checkmark \\
																			 & Synced data & \checkmark & \checkmark &  \checkmark  \\ 
																			 & Calibration parameters & \checkmark  & \checkmark  & \checkmark  \\\hline 
		\multicolumn{2}{l}{Raw data for sensors calibration} & \checkmark & \checkmark & \\
		\multicolumn{2}{l}{Direct \ac{ros} data compatibility} &  & \checkmark & \checkmark \\
		\multicolumn{2}{l}{Data of acquisition} & 2009  & 2011 & 2016 \\
		\bottomrule
	\end{tabular}
\caption{Comparison between the datasets more appropriated to this thesis objectives}
\label{tab:sota:datasets_comparison}
\end{table}
\footnotetext{Other driving data includes, but it is not limited to: vehicle speed, joints states, twist, brakes, suspension, fuel level, \ac{can} bus data, steering, tire pressure, among many others.}

Comparing the 3 datasets, not only using table~\ref{tab:sota:datasets_comparison} but also the previous sections, Udacity and \ac{kitti} are more suited to the purposes of this work. They provide a larger dataset than Ford, have \ac{ros} compatibility and the tools provided are open-source and not developed to be used on proprietary software.

Since Udacity dataset integrates more easily with \ac{ros} than \ac{kitti}, providing also some tools for \ac{ros} preliminar tests, learning and earlier development stages will occur with this dataset. Later, due to the lack of calibration parameters and raw data for sensors calibration\footnote{Note that, despite Udacity dataset not providing data to ease the calibration of its sensors, such as camera intrinsic calibration or the extrinsic calibration between \ac{lidar} and camera, such calibration parameters can be obtained from this data. Those parameters are not accurate as those obtained using a proper calibration setup and are out the scope of this research (being another research topic on itself) and therefore no effort will be dedicated to this topic.}, this research will use the \ac{kitti} dataset.

\section{Camera}
Vision is the human sensor that only is the most relevant on who we perceive the world, but also on how we navigate it~\cite{Ekstrom2015}. Replicating this ability on machines, through the usage of cameras, has been widely researched topic on computer vision and instrumentation~\citeneeded. 

Cameras, as our eyes, take advantage of the pinhole effect: a small hole (or pin), that is used to spatially filter the non-focused light beams through an aperture~\citeneeded, that otherwise will reduce the \ac{dof} at which an object is focused.

Therefore, to proper use a camera on the field of computer vision, some notions of photography are required. Since this research is focused on the usage of industrial cameras and consumer cameras, such notions will be simplified to the minimum necessary. 

\subsection{Focusing a Camera}
Focusing a camera can only be attained for an exact distance from the camera at a time. This exact distance, measured perpendicularly to the plane of the \ac{cmos} or \ac{ccd} chip is called the plan of focus. Therefore, on every image, there is a focus plan and what ``looks focused'' is actually ``acceptably sharp focus'', since precise focus is only theoretical possible.

``Acceptably sharp focus'' means that a point in the real world would not result in a point on the image (as happens in precise focus), but will result in a blur spot that is circular (due to the form of the aperture. If the size of blur is small enough, little to no differences can be perceived and the image is considered to be focused. The maximum size at which this blur is not noticed by the viewer (given a specific sensor size, dimension of the viewed photo, viewing distance and acuity of the viewer) is called the \ac{coc}.

The first step in focusing an image requires the calculation of the hyperfocal distance, $H$: the distance at which the camera is focused to ensure objects from half of this distance to the infinity are in a ``acceptably sharp focus''. This can be calculated using the equation~\ref{eq:hyperfocal_distance}, below, where $f$ is the focal length, $N$ is the F-number and $c$ the \ac{coc} limit. Hyperfocal near limit is defined as $H_{near} = \frac{H}{2}$

\begin{equation}
	\label{eq:hyperfocal_distance}
	H = \frac{f^2}{Nc} + f \approx \frac{f^2}{Nc} 
\end{equation}

Known the hyperfocal distance, the \acf{dof} can be calculated. \ac{dof} is measured in meters and is the subtraction of the farthest and nearest distance at each objects are in ``acceptably sharp focus'' (see equation~\ref{eq:dof}), indicating the distance between this two points. The nearest and farthest points can be calculated using the equations~\ref{eq:dof_near} and~\ref{eq:dof_far}, respectively.

\begin{subequations}
	\label{eq:dof_all}
	\begin{align}
		DoF & = DoF_{far} - DoF_{near} \label{eq:dof} \\
		DoF_{far} & = \frac{H\times d}{H - (d - f)} \label{eq:dof_far} \\
		DoF_{near} & = \frac{H\times d}{H + (d - f))} \label{eq:dof_near} 
	\end{align}
\end{subequations}

Using equations~\ref{eq:dof_all} is possible to select a desired \acl{dof} for an image, guaranting that all the objects of interest are ``acceptably sharp focus''. Taking in consideration that smaller apertures (bigger F-numbers) will increase the exposition time, a target distance can be selected with the garantee that all the objects from the near \ac{dof} point to the far \ac{dof} will be sharp.

\section{Camera Geometry}
A world point in a 3-dimensional Euclidean space can be represented by a vector with 3 real coordinates: $(X, Y, Z)$ and an image pixel as an element of a 2D matrix with integer coordinates $(u, v)$. From a mathematical standpoint, a camera can be considered as a mapping tool between the tridimensionality of the world into a 2D image plan, such as depicted in equation~\ref{eq:camera_transform}. 

\begin{equation}
	\label{eq:camera_transform}
	(X, Y, Z) \xrightarrow[]{\text{transformation}} (u, v)
\end{equation}

Several camera models exist \citeneeded, but on this work only the pinhole camera model will be considered. The pinhole camera model is based on the pinhole effect, which states that a small hole (or aperture) can be used to spatially filter the light rays by angle of incidence, reducing the  overlapping of the light rays coming from different incident angles throught the pinhole. 

This model describes the perspective transformation on equation~\ref{eq:camera_transform} onto a central projection where all light rays meet, known as the camera center, $\mathcal{F_c}$. A diagram of the pinhole camera is shown on figure~\ref{fig:pinhole_camera_model}. On this model, the z axis is along optical axis\footnote{Also called principal axis} (which is at the same orientation that the camera is facing). The image plane is at the coordinates $z = f$, where $f$ is the focal length, being intersect by the optical axis on the principial point, with coordinates $(c_x, c_y)$ on the image plane axis. Note that the principial point is not the origin of the coordinates on the image plane, but the middle point, being the origin located on the top left corner, with a downward yy axis.

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{img/camera/pinhole_camera_model.png}
\caption{Pinhole camera model}
\label{fig:pinhole_camera_model}
\end{figure}

When operating with a pinhole camera model, is more convenient to use a Projective Space instead of the more common Euclidean Space\citeneeded, which has the advantage of all of its points being oriented through a single point, following the visual effect of perspective. By using a projective space instead of an Euclidian for addressing the transformations of a pinhole camera model, mathematics become more intuitive and relatable to the the actual geometry of the model. Projective spaces use homogeneous coordinates and have non-Euclidian geometry, but that allow for changes between the two.

A tridimensional Euclidian Space can be represented on a Projective space using 3+1 coordinates. Therefore, the previous tridimensional vector can be rewritten in homogeneous coordinates as $(wX, wY, wZ, w)$. If $w \neq 0$, we can transform from the Projective to Euclidian Space, obtaining the world representation of such points, by dividing the the homogeneous point by $w$. For the cases in which $w =  0$, we have purely projective points/planes that only exist in Projective Space named Points at Infinity\citeneeded (in this case, at an infinite plane). A point at infinity in projection geometry, is the projection of the picture plane of the point at infinity of a class of parallel lines is called their vanishing point. 


The relation depicted in equation~\ref{eq:camera_transform} can expressed in projective geometry as:

\begin{equation}
	\begin{bmatrix}
		u \\ v \\ 1
	\end{bmatrix}
= P \times 
\begin{bmatrix}
		X \\ Y \\ Z \\ 1
\end{bmatrix}
\end{equation}

Where $P$ is the projection matrix and $w = 1$. The projection matrix in a pinhole camera model is the result of three other matrices: the camera matrix (or matrix of the camera intrinsic parameters), $K$, and a joint rotation, $R$, and translation matrix, $t$, represented as $[R|t]$. The combination of this matrices are represented on equation~\ref{eq:projective_matrix} and the full camera transform is given on equation~\ref{eq:camera_transform_full}.

\begin{equation}
	\label{eq:projective_matrix}
	P = K[R|t]
\end{equation}

\begin{equation}
	\label{eq:camera_transform_full}
	\begin{bmatrix}
		u \\
		v \\
		1
	\end{bmatrix}
	= 
	\overbrace{
		\underbrace{
			\begin{bmatrix}
				f_x & 0 & c_x \\
				0 & f_y & c_y \\
				0 & 0 & 1 
			\end{bmatrix}
		}_\text{\large\textbf{K}}
		\underbrace{
			\left[
				\begin{array}{ccc|c}
					r_{xx} & r_{yx} & r_{zx} & t_x \\
					r_{xy} & r_{yy} & r_{zy} & t_y \\
					r_{xz} & r_{yz} & r_{zz} & t_z 
				\end{array}
		\right]
		}_\text{\large\textbf{[R|t]}}
	}^\text{\large\textbf{P}}
	\begin{bmatrix}
		X \\
		Y \\
		Z \\
		1
	\end{bmatrix}
\end{equation}


In the pinhole camera model depicted in figure~\ref{fig:pinhole_camera_model}, 


\section{Camera Intrinsic Calibration}

\section{Computer Vision}

