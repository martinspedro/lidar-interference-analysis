Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Penate-Sanchez2013a,
abstract = {We propose a novel approach for the estimation of the pose and focal length of a camera from a set of 3D-to-2D point correspondences. Our method compares favorably to competing approaches in that it is both more accurate than existing closed form solutions, as well as faster and also more accurate than iterative ones. Our approach is inspired on the EPnP algorithm, a recent O(n) solution for the calibrated case. Yet we show that considering the focal length as an additional unknown renders the linearization and relinearization techniques of the original approach no longer valid, especially with large amounts of noise. We present new methodologies to circumvent this limitation termed exhaustive linearization and exhaustive relinearization which perform a systematic exploration of the solution space in closed form. The method is evaluated on both real and synthetic data, and our results show that besides producing precise focal length estimation, the retrieved camera pose is almost as accurate as the one computed using the EPnP, which assumes a calibrated camera. {\textcopyright} 1979-2012 IEEE.},
author = {Penate-Sanchez, Adrian and Andrade-Cetto, Juan and Moreno-Noguer, Francesc},
doi = {10.1109/TPAMI.2013.36},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Penate-Sanchez, Andrade-Cetto, Moreno-Noguer/Exhaustive linearization for robust camera pose and focal length estimation. Penate-Sanchez, Andrade-Cetto, Moreno-Noguer. 2013.pdf:pdf},
issn = {01628828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
keywords = {Camera calibration,perspective-n-point problem},
number = {10},
pages = {2387--2400},
title = {{Exhaustive linearization for robust camera pose and focal length estimation}},
volume = {35},
year = {2013}
}
@article{Guindel2018a,
abstract = {Sensor setups consisting of a combination of 3D range scanner lasers and stereo vision systems are becoming a popular choice for on-board perception systems in vehicles; however, the combined use of both sources of information implies a tedious calibration process. We present a method for extrinsic calibration of lidar-stereo camera pairs without user intervention. Our calibration approach is aimed to cope with the constraints commonly found in automotive setups, such as low-resolution and specific sensor poses. To demonstrate the performance of our method, we also introduce a novel approach for the quantitative assessment of the calibration results, based on a simulation environment. Tests using real devices have been conducted as well, proving the usability of the system and the improvement over the existing approaches. Code is available at http://wiki.ros.org/velo2cam{\_}calibration},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.04085v3},
author = {Guindel, Carlos and Beltran, Jorge and Martin, David and Garcia, Fernando},
doi = {10.1109/ITSC.2017.8317829},
eprint = {arXiv:1705.04085v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Guindel et al/Automatic extrinsic calibration for lidar-stereo vehicle sensor setups. Guindel et al.. 2018(2).pdf:pdf},
isbn = {9781538615256},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
number = {October},
pages = {1--6},
title = {{Automatic extrinsic calibration for lidar-stereo vehicle sensor setups}},
volume = {2018-March},
year = {2018}
}
@article{DeSilva2018,
abstract = {Autonomous robots that assist humans in day to day living tasks are becoming increasingly popular. Autonomous mobile robots operate by sensing and perceiving their surrounding environment to make accurate driving decisions. A combination of several different sensors such as LiDAR, radar, ultrasound sensors and cameras are utilized to sense the surrounding environment of autonomous vehicles. These heterogeneous sensors simultaneously capture various physical attributes of the environment. Such multimodality and redundancy of sensing need to be positively utilized for reliable and consistent perception of the environment through sensor data fusion. However, these multimodal sensor data streams are different from each other in many ways, such as temporal and spatial resolution, data format, and geometric alignment. For the subsequent perception algorithms to utilize the diversity offered by multimodal sensing, the data streams need to be spatially, geometrically and temporally aligned with each other. In this paper, we address the problem of fusing the outputs of a Light Detection and Ranging (LiDAR) scanner and a wide-angle monocular image sensor for free space detection. The outputs of LiDAR scanner and the image sensor are of different spatial resolutions and need to be aligned with each other. A geometrical model is used to spatially align the two sensor outputs, followed by a Gaussian Process (GP) regression-based resolution matching algorithm to interpolate the missing data with quantifiable uncertainty. The results indicate that the proposed sensor data fusion framework significantly aids the subsequent perception steps, as illustrated by the performance improvement of a uncertainty aware free space detection algorithm},
archivePrefix = {arXiv},
arxivId = {1710.06230},
author = {{De Silva}, Varuna and Roche, Jamie and Kondoz, Ahmet},
doi = {10.3390/s18082730},
eprint = {1710.06230},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/De Silva, Roche, Kondoz/Robust fusion of LiDAR and wide-angle camera data for autonomous mobile robots. De Silva, Roche, Kondoz. 2018.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Assistive robots,Autonomous vehicles,Depth sensing,Free space detection,Gaussian process regression,LiDAR,Sensor data fusion},
number = {8},
title = {{Robust fusion of LiDAR and wide-angle camera data for autonomous mobile robots}},
volume = {18},
year = {2018}
}
@article{Pusztai2018,
abstract = {As autonomous driving attracts more and more attention these days, the algorithms and sensors used for machine perception become popular in research, as well. This paper investigates the extrinsic calibration of two frequently-applied sensors: the camera and Light Detection and Ranging (LiDAR). The calibration can be done with the help of ordinary boxes. It contains an iterative refinement step, which is proven to converge to the box in the LiDAR point cloud, and can be used for system calibration containing multiple LiDARs and cameras. For that purpose, a bundle adjustment-like minimization is also presented. The accuracy of the method is evaluated on both synthetic and real-world data, outperforming the state-of-the-art techniques. The method is general in the sense that it is both LiDAR and camera-type independent, and only the intrinsic camera parameters have to be known. Finally, a method for determining the 2D bounding box of the car chassis from LiDAR point clouds is also presented in order to determine the car body border with respect to the calibrated sensors.},
author = {Pusztai, Zolt{\'{a}}n and Eichhardt, Iv{\'{a}}n and Hajder, Levente},
doi = {10.3390/s18072139},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pusztai, Eichhardt, Hajder/Accurate calibration of multi-lidar-multi-camera systems. Pusztai, Eichhardt, Hajder. 2018.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Autonomous driving,Camera,Extrinsic calibration,LiDAR,LiDAR camera system,Machine perception},
number = {7},
pages = {1--22},
title = {{Accurate calibration of multi-lidar-multi-camera systems}},
volume = {18},
year = {2018}
}
@article{Gao2003,
abstract = {We use two approaches to solve the perspective-three-point (P3P) problem: the algebraic approach and the geometric approach. In the algebraic approach, we use Wu-Ritt's zero decomposition algorithm to give a complete triangular decomposition for the P3P equation system. This decomposition provides the first complete analytical solution to the P3P problem. We also give a complete solution classification for the P3P equation system, i.e., we give explicit criteria for the P3P problem to have one, two, three, and four solutions. Combining the analytical solutions with the criteria, we provide an algorithm, CASSC, which may be used to find complete and robust numerical solutions to the P3P problem. In the geometric approach, we give some pure geometric criteria for the number of real physical solutions.},
author = {Gao, Xiao Shan and Hou, Xiao Rong and Tang, Jianliang and Cheng, Hang Fei},
doi = {10.1109/TPAMI.2003.1217599},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gao et al/Complete solution classification for the perspective-three-point problem. Gao et al.. 2003.pdf:pdf},
issn = {01628828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
keywords = {Analytical solutions,Geometric criteria,Perspective-Three-Point problem,Pose determination,Solution classification,Wu-Ritt's zero decomposition method},
number = {8},
pages = {930--943},
title = {{Complete solution classification for the perspective-three-point problem}},
volume = {25},
year = {2003}
}
@article{Shaukat2016,
abstract = {{\textcopyright} 2016 by the authors; licensee MDPI,Basel,Switzerland. In recent decades,terrain modelling and reconstruction techniques have increased research interest in precise short and long distance autonomous navigation,localisation and mapping within field robotics. One of the most challenging applications is in relation to autonomous planetary exploration using mobile robots. Rovers deployed to explore extraterrestrial surfaces are required to perceive and model the environment with little or no intervention from the ground station. Up to date,stereopsis represents the state-of-the art method and can achieve short-distance planetary surface modelling. However,future space missions will require scene reconstruction at greater distance,fidelity and feature complexity,potentially using other sensors like Light Detection And Ranging (LIDAR). LIDAR has been extensively exploited for target detection,identification,and depth estimation in terrestrial robotics,but is still under development to become a viable technology for space robotics. This paper will first review current methods for scene reconstruction and terrain modelling using cameras in planetary robotics and LIDARs in terrestrial robotics; then we will propose camera-LIDAR fusion as a feasible technique to overcome the limitations of either of these individual sensors for planetary exploration. A comprehensive analysis will be presented to demonstrate the advantages of camera-LIDAR fusion in terms of range,fidelity,accuracy and computation.},
author = {Shaukat, Affan and Blacker, Peter C. and Spiteri, Conrad and Gao, Yang},
doi = {10.3390/s16111952},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Shaukat et al/Towards camera-LIDAR fusion-based terrain modelling for planetary surfaces Review and analysis. Shaukat et al.. 2016.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {3-D reconstruction,Hybrid vision systems,LIDAR-camera fusion,Planetary surface perception,Terrain modelling},
number = {11},
title = {{Towards camera-LIDAR fusion-based terrain modelling for planetary surfaces: Review and analysis}},
volume = {16},
year = {2016}
}
@article{Park2014,
abstract = {Calibration between color camera and 3D Light Detection And Ranging (LIDAR) equipment is an essential process for data fusion. The goal of this paper is to improve the calibration accuracy between a camera and a 3D LIDAR. In particular, we are interested in calibrating a low resolution 3D LIDAR with a relatively small number of vertical sensors. Our goal is achieved by employing a new methodology for the calibration board, which exploits 2D-3D correspondences. The 3D corresponding points are estimated from the scanned laser points on the polygonal planar board with adjacent sides. Since the lengths of adjacent sides are known, we can estimate the vertices of the board as a meeting point of two projected sides of the polygonal board. The estimated vertices from the range data and those detected from the color image serve as the corresponding points for the calibration. Experiments using a low-resolution LIDAR with 32 sensors show robust results. {\textcopyright} 2014 by the authors; licensee MDPI, Basel, Switzerland.},
author = {Park, Yoonsu and Yun, Seokmin and Won, Chee Sun and Cho, Kyungeun and Um, Kyhyun and Sim, Sungdae},
doi = {10.3390/s140305333},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Park et al/Calibration between color camera and 3D LIDAR instruments with a polygonal planar board. Park et al.. 2014.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {3D LIDAR,3D point clouds,Calibration board,Calibration matrix,Camera calibration,Sensor fusion},
number = {3},
pages = {5333--5353},
title = {{Calibration between color camera and 3D LIDAR instruments with a polygonal planar board}},
volume = {14},
year = {2014}
}
@article{Taylor2013,
abstract = {This paper is about automatic calibration of a camera-lidar system. The method presented is designed to be as general as possible allowing it to be used in a large range of systems and applications. The approach uses normalized mutual information to compare camera images with lidar scans of the same area. A camera model that takes into account orientation, location and focal length is used to create a 2D lidar image, with the intensity of the pixels representing a feature of the lidar scan that is chosen depending on the application. Particle swarm optimization is used to find the optimal model parameters. The method presented is successfully validated on a variety of cameras, lidars and locations, including scans of both urban and natural environments},
author = {Taylor, Zachary and Nieto, Juan},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Taylor, Nieto/Automatic Calibration of Lidar and Camera Images using Normalized Mutual Information. Taylor, Nieto. 2013.pdf:pdf},
journal = {IEEE Int. Conf. Robot. Autom.},
title = {{Automatic Calibration of Lidar and Camera Images using Normalized Mutual Information}},
url = {https://pdfs.semanticscholar.org/fc08/d41be320a827c3f045c4429e13495da5a1cc.pdf{\%}0Ahttp://www-personal.acfr.usyd.edu.au/jnieto/Publications{\_}files/TaylorICRA2013.pdf},
year = {2013}
}
@article{Ishikawa2018,
abstract = {In this paper, we propose a method of targetless and automatic Camera-LiDAR calibration. Our approach is an extension of hand-eye calibration framework to 2D-3D calibration. By using the sensor fusion odometry method, the scaled camera motions are calculated with high accuracy. In addition to this, we clarify the suitable motion for this calibration method. The proposed method only requires the three-dimensional point cloud and the camera image and does not need other information such as reflectance of LiDAR and to give initial extrinsic parameter. In the experiments, we demonstrate our method using several sensor configurations in indoor and outdoor scenes to verify the effectiveness. The accuracy of our method achieves more than other comparable state-of-the-art methods.},
author = {Ishikawa, Ryoichi and Oishi, Takeshi and Ikeuchi, Katsushi},
doi = {10.1109/IROS.2018.8593360},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ishikawa, Oishi, Ikeuchi/LiDAR and Camera Calibration Using Motions Estimated by Sensor Fusion Odometry. Ishikawa, Oishi, Ikeuchi. 2018(2).pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
number = {October 2018},
pages = {7342--7349},
title = {{LiDAR and Camera Calibration Using Motions Estimated by Sensor Fusion Odometry}},
year = {2018}
}
@article{Jeong2019,
abstract = {This paper presents a framework for the targetless extrinsic calibration of stereo cameras and Light Detection and Ranging (LiDAR) sensors with a non-overlapping Field of View (FOV). In order to solve the extrinsic calibrations problem under such challenging configuration, the proposed solution exploits road markings as static and robust features among the various dynamic objects that are present in urban environment. First, this study utilizes road markings that are commonly captured by the two sensor modalities to select informative images for estimating the extrinsic parameters. In order to accomplish stable optimization, multiple cost functions are defined, including Normalized Information Distance (NID), edge alignment and, plane fitting cost. Therefore a smooth cost curve is formed for global optimization to prevent convergence to the local optimal point. We further evaluate each cost function by examining parameter sensitivity near the optimal point. Another key characteristic of extrinsic calibration, repeatability, is analyzed by conducting the proposed method multiple times with varying randomly perturbed initial points.},
archivePrefix = {arXiv},
arxivId = {1902.10586},
author = {Jeong, Jinyong and Cho, Lucas Y. and Kim, Ayoung},
eprint = {1902.10586},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Jeong, Cho, Kim/Road is Enough! Extrinsic Calibration of Non-overlapping Stereo Camera and LiDAR using Road Information. Jeong, Cho, Kim. 2019.pdf:pdf},
title = {{Road is Enough! Extrinsic Calibration of Non-overlapping Stereo Camera and LiDAR using Road Information}},
url = {http://arxiv.org/abs/1902.10586},
year = {2019}
}
@article{Fremont2013,
author = {Fremont, Vincent and Alberto, Sergio and Florez, Rodriguez and Bonnifait, Philippe and Targets, Circular and Video, Alignment and Sensors, Lidar and Robotics, Advanced and Stm, Francis},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fremont et al/Circular Targets for 3D Alignment of Video and Lidar Sensors To cite this version Circular Targets for 3D Alignment of Video and Lidar.pdf:pdf},
title = {{Circular Targets for 3D Alignment of Video and Lidar Sensors To cite this version : Circular Targets for 3D Alignment of Video and Lidar Sensors}},
year = {2013}
}
@article{Unnikrishnan2005,
abstract = {External calibration of a camera to a laser rangefinder is a common pre-requisite on todays multi-sensor mobile robot platforms. However, the process of doing so is relatively poorly documented and almost always time-consuming. This document outlines an easy and portable technique for external calibration of a camera to a laser rangefinder. It describes the usage of the Laser-Camera Calibration Toolbox (LCCT), aMatlab R -based graphical user interface that is meant to accompany this document and facilitates the calibration procedure. We also summarize the math behind its development. The software is accessible online at as well as at the VMR Lab Software page at software.html .},
author = {Unnikrishnan, Ranjith and Hebert, Martial},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Unnikrishnan, Hebert/Fast Extrinsic Calibration of a Laser Rangefinder to a Camera. Unnikrishnan, Hebert. 2005.pdf:pdf},
isbn = {CMU-RI-TR-05-09},
journal = {Robotics},
number = {July 2005},
pages = {23},
title = {{Fast Extrinsic Calibration of a Laser Rangefinder to a Camera}},
url = {http://www.ri.cmu.edu/pub{\_}files/pub4/unnikrishnan{\_}ranjith{\_}2005{\_}3/unnikrishnan{\_}ranjith{\_}2005{\_}3.pdf https://www.cs.cmu.edu/{~}ranjith/lcct.html},
volume = {2005},
year = {2005}
}
@article{Hesch2011,
author = {Hesch, Joel A and Roumeliotis, Stergios I},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hesch, Roumeliotis/A Direct Least-Squares ( DLS ) Method for PnP. Hesch, Roumeliotis. 2011.pdf:pdf},
isbn = {9781457711022},
pages = {383--390},
title = {{A Direct Least-Squares ( DLS ) Method for PnP}},
year = {2011}
}
@article{Liao2019,
abstract = {Fusion of heterogeneous exteroceptive sensors is the most efficient and effective path to the representation of the environment precisely, as it can compromise various drawbacks of each homogeneous sensor. The rigid transformation (aka. extrinsic parameters) of heterogeneous sensory systems is the prerequisite of fusing the multi-sensor information. Researchers have proposed several approaches to estimate the extrinsic parameters. However, these approaches neither rely on human interventions or specifically designed auxiliary object or do not provide the library which makes it hard to test or benchmark. In this paper, we propose a novel extrinsic calibration approach for the extrinsic calibration of a Lidar (Laser Range Finder) and a camera which only based on a polygon board and we offer the relevant tools. In this paper, we firstly track and extract the target polygon from both the image and point-cloud. Then we try to match the polygon between the 2D and 3D feature spaces. With the associated polygon, we are able to get multiple constraints to optimize the extrinsic parameters. At the end, we validate our approach by four configurations , including the simulation, 16/32-beam Lidar and 100-line MEMS-Lidar. The outcome indicates high-precision extrinsic calibration performance.},
author = {Liao, Qinghai and Chen, Zhenyong and Liu, Yang and Wang, Zhe and Liu, Ming},
doi = {10.1109/ROBIO.2018.8665256},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Liao et al/Extrinsic Calibration of Lidar and Camera with Polygon. Liao et al.. 2019.pdf:pdf},
isbn = {9781728103761},
journal = {2018 IEEE Int. Conf. Robot. Biomimetics, ROBIO 2018},
pages = {200--205},
publisher = {IEEE},
title = {{Extrinsic Calibration of Lidar and Camera with Polygon}},
year = {2019}
}
@article{Geiger2012a,
abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera- to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experi- ments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions. I.},
author = {Geiger, Andreas and Moosmann, Frank and Car, {\"{O}}mer and Schuster, Bernhard},
doi = {10.1109/ICRA.2012.6224570},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger et al/Automatic camera and range sensor calibration using a single shot. Geiger et al.. 2012.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {3936--3943},
title = {{Automatic camera and range sensor calibration using a single shot}},
year = {2012}
}
@article{Lepetit2009,
abstract = {We propose a non-iterative solution to the PnP problem—the estimation of the pose of a calibrated camera from n 3D-to-2D point correspondences—whose computational complexity grows linearly with n. This is in contrast to state-of-the-art methods that are O(n 5) or even O(n 8), without being more accurate. Our method is applicable for all n≥4 and handles properly both planar and non-planar configurations. Our central idea is to express the n 3D points as a weighted sum of four virtual control points. The problem then reduces to estimating the coordinates of these control points in the camera referential, which can be done in O(n) time by expressing these coordinates as weighted sum of the eigenvectors of a 12×12 matrix and solving a small constant number of quadratic equations to pick the right weights. Furthermore, if maximal precision is required, the output of the closed-form solution can be used to initialize a Gauss-Newton scheme, which improves accuracy with negligible amount of additional time. The advantages of our method are demonstrated by thorough testing on both synthetic and real-data.},
author = {Lepetit, Vincent and Moreno-Noguer, Francesc and Fua, Pascal},
doi = {10.1007/s11263-008-0152-6},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lepetit, Moreno-Noguer, Fua/EPnP An accurate O(n) solution to the PnP problem. Lepetit, Moreno-Noguer, Fua. 2009.pdf:pdf},
issn = {09205691},
journal = {Int. J. Comput. Vis.},
keywords = {Absolute orientation,Perspective-n-Point,Pose estimation},
number = {2},
pages = {155--166},
title = {{EPnP: An accurate O(n) solution to the PnP problem}},
volume = {81},
year = {2009}
}
@article{Pusztai2018a,
abstract = {As autonomous driving attracts more and more attention these days, the algorithms and sensors used for machine perception become popular in research, as well. This paper investigates the extrinsic calibration of two frequently-applied sensors: the camera and Light Detection and Ranging (LiDAR). The calibration can be done with the help of ordinary boxes. It contains an iterative refinement step, which is proven to converge to the box in the LiDAR point cloud, and can be used for system calibration containing multiple LiDARs and cameras. For that purpose, a bundle adjustment-like minimization is also presented. The accuracy of the method is evaluated on both synthetic and real-world data, outperforming the state-of-the-art techniques. The method is general in the sense that it is both LiDAR and camera-type independent, and only the intrinsic camera parameters have to be known. Finally, a method for determining the 2D bounding box of the car chassis from LiDAR point clouds is also presented in order to determine the car body border with respect to the calibrated sensors.},
author = {Pusztai, Zolt{\'{a}}n and Eichhardt, Iv{\'{a}}n and Hajder, Levente},
doi = {10.3390/s18072139},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pusztai, Eichhardt, Hajder/Accurate calibration of multi-lidar-multi-camera systems. Pusztai, Eichhardt, Hajder. 2018.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Autonomous driving,Camera,Extrinsic calibration,LiDAR,LiDAR camera system,Machine perception},
number = {7},
pages = {1--22},
title = {{Accurate calibration of multi-lidar-multi-camera systems}},
volume = {18},
year = {2018}
}
@article{Slabaugh,
author = {Slabaugh, Gregory G},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Slabaugh/Computing Euler angles from a rotation matrix. Slabaugh. Unknown.pdf:pdf},
pages = {1--7},
title = {{Computing Euler angles from a rotation matrix}}
}
@article{Wang2018a,
abstract = {We propose a new method for fusing LIDAR point cloud and camera-captured images in deep convolutional neural networks (CNN). The proposed method constructs a new layer called sparse non-homogeneous pooling layer to transform features between bird's eye view and front view. The sparse point cloud is used to construct the mapping between the two views. The pooling layer allows efficient fusion of the multi-view features at any stage of the network. This is favorable for 3D object detection using camera-LIDAR fusion for autonomous driving. A corresponding one-stage detector is designed and tested on the KITTI bird's eye view object detection dataset, which produces 3D bounding boxes from the bird's eye view map. The fusion method shows significant improvement on both speed and accuracy of the pedestrian detection over other fusion-based object detection networks.},
archivePrefix = {arXiv},
arxivId = {1711.06703},
author = {Wang, Zining and Zhan, Wei and Tomizuka, Masayoshi},
doi = {10.1109/IVS.2018.8500387},
eprint = {1711.06703},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wang, Zhan, Tomizuka/Fusing Bird's Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection. Wang, Zhan, Tomizuka. 2018.pdf:pdf},
isbn = {9781538644522},
journal = {IEEE Intell. Veh. Symp. Proc.},
pages = {834--839},
title = {{Fusing Bird's Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection}},
volume = {2018-June},
year = {2018}
}
@article{Chien2017,
abstract = {{\textcopyright} 2016 IEEE. Recently LiDAR-camera systems have rapidly emerged in many applications. The integration of laser range-finding technologies into existing vision systems enables a more comprehensive understanding of 3D structure of the environment. The advantage, however, relies on a good geometrical calibration between the LiDAR and the image sensors. In this paper we consider visual odometry, a discipline in computer vision and robotics, in the context of recently emerging online sensory calibration studies. By embedding the online calibration problem into a LiDAR-monocular visual odometry technique, the temporal change of extrinsic parameters can be tracked and compensated effectively.},
author = {Chien, Hsiang Jen and Klette, Reinhard and Schneider, Nick and Franke, Uwe},
doi = {10.1109/ICPR.2016.7900068},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Chien et al/Visual odometry driven online calibration for monocular LiDAR-camera systems. Chien et al.. 2017.pdf:pdf},
isbn = {9781509048472},
issn = {10514651},
journal = {Proc. - Int. Conf. Pattern Recognit.},
month = {apr},
pages = {2848--2853},
publisher = {IEEE},
title = {{Visual odometry driven online calibration for monocular LiDAR-camera systems}},
year = {2017}
}
@article{Bileschi2009,
author = {Bileschi, Stanley},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Bileschi/Fully automatic calibration of LIDAR and video streams from a vehicle The MIT Faculty has made this article openly available . Please.pdf:pdf},
isbn = {9781424444410},
journal = {Comput. Vis. Work. (ICCV Work.},
keywords = {automatic calibration},
mendeley-tags = {automatic calibration},
pages = {1457--1464},
title = {{Fully automatic calibration of LIDAR and video streams from a vehicle The MIT Faculty has made this article openly available . Please share Bileschi , S . “ Fully automatic calibration of LIDAR and video Publisher Version Accessed Citable Link Terms of Us}},
year = {2009}
}
@article{Guindel2018,
abstract = {Sensor setups consisting of a combination of 3D range scanner lasers and stereo vision systems are becoming a popular choice for on-board perception systems in vehicles; however, the combined use of both sources of information implies a tedious calibration process. We present a method for extrinsic calibration of lidar-stereo camera pairs without user intervention. Our calibration approach is aimed to cope with the constraints commonly found in automotive setups, such as low-resolution and specific sensor poses. To demonstrate the performance of our method, we also introduce a novel approach for the quantitative assessment of the calibration results, based on a simulation environment. Tests using real devices have been conducted as well, proving the usability of the system and the improvement over the existing approaches. Code is available at http://wiki.ros.org/velo2cam{\_}calibration},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.04085v3},
author = {Guindel, Carlos and Beltran, Jorge and Martin, David and Garcia, Fernando},
doi = {10.1109/ITSC.2017.8317829},
eprint = {arXiv:1705.04085v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Guindel et al/Automatic extrinsic calibration for lidar-stereo vehicle sensor setups. Guindel et al.. 2018.pdf:pdf},
isbn = {9781538615256},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
pages = {1--6},
title = {{Automatic extrinsic calibration for lidar-stereo vehicle sensor setups}},
volume = {2018-March},
year = {2018}
}
@article{Pereira2016,
abstract = {Autonomous navigation is an important field of research and, given the complexity of real world environments, most of the systems rely on a complex perception system combining multiple sensors on board, which reinforces the concern of sensor calibration. Most calibration methods rely on manual or semi-automatic interactive procedures, but reliable fully automatic methods are still missing. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. The idea proposed in this paper is to use a ball in motion in front of a set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation among all pairs of sensors. This paper proposes and describes such a method with results demonstrating its validity.},
author = {Pereira, Marcelo and Silva, David and Santos, Vitor and Dias, Paulo},
doi = {10.1016/j.robot.2016.05.010},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pereira et al/Self calibration of multiple LIDARs and cameras on autonomous vehicles. Pereira et al.. 2016.pdf:pdf},
issn = {09218890},
journal = {Rob. Auton. Syst.},
keywords = {3D data fitting,Extrinsic calibration,Point cloud},
pages = {326--337},
title = {{Self calibration of multiple LIDARs and cameras on autonomous vehicles}},
volume = {83},
year = {2016}
}
@article{Pereira2016,
abstract = {Autonomous navigation is an important field of research and, given the complexity of real world environments, most of the systems rely on a complex perception system combining multiple sensors on board, which reinforces the concern of sensor calibration. Most calibration methods rely on manual or semi-automatic interactive procedures, but reliable fully automatic methods are still missing. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. The idea proposed in this paper is to use a ball in motion in front of a set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation among all pairs of sensors. This paper proposes and describes such a method with results demonstrating its validity.},
author = {Pereira, Marcelo and Silva, David and Santos, Vitor and Dias, Paulo},
doi = {10.1016/j.robot.2016.05.010},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pereira et al/Self calibration of multiple LIDARs and cameras on autonomous vehicles. Pereira et al.. 2016.pdf:pdf},
issn = {09218890},
journal = {Rob. Auton. Syst.},
keywords = {3D data fitting,Extrinsic calibration,Point cloud},
pages = {326--337},
title = {{Self calibration of multiple LIDARs and cameras on autonomous vehicles}},
volume = {83},
year = {2016}
}
@article{Gong2013,
abstract = {This paper presents a novel way to address the extrinsic calibration problem for a system composed of a 3D LIDAR and a camera. The relative transformation between the two sensors is calibrated via a nonlinear least squares (NLS) problem, which is formulated in terms of the geometric constraints associated with a trihedral object. Precise initial estimates of NLS are obtained by dividing it into two sub-problems that are solved individually. With the precise initializations, the calibration parameters are further refined by iteratively optimizing the NLS problem. The algorithm is validated on both simulated and real data, as well as a 3D reconstruction application. Moreover, since the trihedral target used for calibration can be either orthogonal or not, it is very often present in structured environments, making the calibration convenient.},
author = {Gong, Xiaojin and Lin, Ying and Liu, Jilin},
doi = {10.3390/s130201902},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gong, Lin, Liu/3D LIDAR-camera extrinsic calibration using an arbitrary trihedron. Gong, Lin, Liu. 2013.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {3D LIDAR-camera system,Extrinsic calibration,Sensor fusion},
number = {2},
pages = {1902--1918},
title = {{3D LIDAR-camera extrinsic calibration using an arbitrary trihedron}},
volume = {13},
year = {2013}
}
@article{Dai2015,
abstract = {This paper reviews the Euler-Rodrigues formula in the axis-angle representation of rotations, studies its variations and derivations in different mathematical forms as vectors, quaternions and Lie groups and investigates their intrinsic connections. The Euler-Rodrigues formula in the Taylor series expansion is presented and its use as an exponential map of Lie algebras is discussed particularly with a non-normalized vector. The connection between Euler-Rodrigues parameters and the Euler-Rodrigues formula is then demonstrated through quaternion conjugation and the equivalence between quaternion conjugation and an adjoint action of the Lie group is subsequently presented. The paper provides a rich reference for the Euler-Rodrigues formula, the variations and their connections and for their use in rigid body kinematics, dynamics and computer graphics.},
author = {Dai, Jian S.},
doi = {10.1016/j.mechmachtheory.2015.03.004},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Dai/Euler-Rodrigues formula variations, quaternion conjugation and intrinsic connections. Dai. 2015.pdf:pdf},
issn = {0094114X},
journal = {Mech. Mach. Theory},
keywords = {Euler-Rodrigues formula,Exponential map,Kinematics,Lie algebras,Lie groups,Quaternions},
pages = {144--152},
publisher = {The Author},
title = {{Euler-Rodrigues formula variations, quaternion conjugation and intrinsic connections}},
url = {http://dx.doi.org/10.1016/j.mechmachtheory.2015.03.004},
volume = {92},
year = {2015}
}
@article{Pandey2012,
abstract = {This paper reports on a mutual information (MI) based algo- rithm for automatic extrinsic calibration of a 3D laser scan- ner and optical camera system. By using MI as the regis- tration criterion, our method is able to work in situ without the need for any specific calibration targets, which makes it practical for in-field calibration. The calibration parameters are estimated by maximizing the mutual information obtained between the sensor-measured surface intensities. We calcu- late the Cramer-Rao-Lower-Bound (CRLB) and show that the sample variance of the estimated parameters empirically ap- proaches the CRLB for a sufficient number of views. Fur- thermore, we compare the calibration results to independent ground-truth and observe that the mean error also empirically approaches to zero as the number of views are increased. This indicates that the proposed algorithm, in the limiting case, calculates a minimum variance unbiased (MVUB) estimate of the calibration parameters. Experimental results are pre- sented for data collected by a vehicle mounted with a 3D laser scanner and an omnidirectional camera system.},
author = {Pandey, Gaurav and McBride, James R and Savarese, Silvio and Eustice, Ryan M},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pandey et al/Automatic targetless extrinsic calibration of a 3d lidar and camera by maximizing mutual information. Pandey et al.. 2012.pdf:pdf},
journal = {Proc. {\{}AAAI{\}} Natl. Conf. Artif. Intell.},
keywords = {calibration,ladybug,omnidirectional,velodyne},
pages = {2053--2059},
title = {{Automatic targetless extrinsic calibration of a 3d lidar and camera by maximizing mutual information}},
year = {2012}
}
@article{Wang2018a,
abstract = {We propose a new method for fusing LIDAR point cloud and camera-captured images in deep convolutional neural networks (CNN). The proposed method constructs a new layer called sparse non-homogeneous pooling layer to transform features between bird's eye view and front view. The sparse point cloud is used to construct the mapping between the two views. The pooling layer allows efficient fusion of the multi-view features at any stage of the network. This is favorable for 3D object detection using camera-LIDAR fusion for autonomous driving. A corresponding one-stage detector is designed and tested on the KITTI bird's eye view object detection dataset, which produces 3D bounding boxes from the bird's eye view map. The fusion method shows significant improvement on both speed and accuracy of the pedestrian detection over other fusion-based object detection networks.},
archivePrefix = {arXiv},
arxivId = {1711.06703},
author = {Wang, Zining and Zhan, Wei and Tomizuka, Masayoshi},
doi = {10.1109/IVS.2018.8500387},
eprint = {1711.06703},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wang, Zhan, Tomizuka/Fusing Bird's Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection. Wang, Zhan, Tomizuka. 2018.pdf:pdf},
isbn = {9781538644522},
journal = {IEEE Intell. Veh. Symp. Proc.},
pages = {834--839},
title = {{Fusing Bird's Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection}},
volume = {2018-June},
year = {2018}
}
@article{MartinVelas2013,
abstract = {Quinolone-resistant clinical Escherichia coli isolates were examined for mutations in the marRAB operon of the multiple antibiotic resistance (mar) locus. Among 23 strains evaluated, 8 were chosen for further study: 3 that showed relatively high levels of uninduced, i.e., constitutive, expression of the operon and 5 with variable responses to induction by salicylate or tetracyclines. The marR genes, specifying the repressor of the operon, cloned from the three strains constitutively expressing the operon did not reduce the level of expression of beta-galactosidase from a marO::lacZ transcriptional fusion and were therefore mutant; however, marR genes cloned from the five other clinical strains repressed LacZ expression and were wild type. All three mutant marR genes contained more than one mutation: a deletion and a point mutation. Inactivation of the mar locus in the three known marR mutant strains with a kanamycin resistance cassette introduced by homologous recombination reduced resistance to quinolones and multiple antibiotics. These findings indicate that mar operon mutations exist in quinolone-resistant clinical E. coli isolates and contribute to quinolone and multidrug resistance.},
archivePrefix = {arXiv},
arxivId = {1505.00729},
author = {Velas, Martin and Spanel, Michal and Materna, Zdenek and Herout, Adam},
doi = {10.1016/B978-0-08-098242-7.00008-0},
eprint = {1505.00729},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Martin Velas, Michal Spanel, Zdenek Materna/Calibration of RGB Camera With Velodyne LiDAR. Martin Velas, Michal Spanel, Zdenek Materna. 2013.pdf:pdf},
isbn = {9781461434801},
issn = {0066-4804},
journal = {Flight Dyn. Princ.},
keywords = {L{\'{i}}DAR,Velodyne,calibration,camera,marker},
month = {jul},
number = {7},
pages = {219--241},
pmid = {8807064},
publisher = {Elsevier},
title = {{Calibration of RGB Camera With Velodyne LiDAR}},
url = {https://linkinghub.elsevier.com/retrieve/pii/B9780080982427000080},
volume = {40},
year = {2013}
}
@article{Scaramuzza,
author = {Scaramuzza, Davide and Harati, Ahad and Siegwart, Roland},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Scaramuzza, Harati, Siegwart/Extrinsic self calibration of a camera and a 3D Laser Range Finder. Scaramuzza, Harati, Siegwart. Unknown.pdf:pdf},
keywords = {camera calibration,computer vision,laser calibration,omndirectional vision,omnidirectional camera,p3p algorithm,pnp algorithm,reprojection error,robotics,self calibration},
title = {{Extrinsic self calibration of a camera and a 3D Laser Range Finder}}
}
@article{Mirzaei2012,
abstract = {In this paper we address the problem of estimating the intrinsic parameters of a 3D LIDAR while at the same time computing its extrinsic calibration with respect to a rigidly connected camera. Existing approaches to solve this nonlinear estimation problem are based on iterative minimization of nonlinear cost functions. In such cases, the accuracy of the resulting solution hinges on the availability of a precise initial estimate, which is often not available. In order to address this issue, we divide the problem into two least-squares sub-problems, and analytically solve each one to determine a precise initial estimate for the unknown parameters. We further increase the accuracy of these initial estimates by iteratively minimizing a batch nonlinear least-squares cost function. In addition, we provide the minimal identifiability conditions, under which it is possible to accurately estimate the unknown parameters. Experimental results consisting of photorealistic 3D reconstruction of indoor and outdoor scenes, as well as standard metrics of the calibration errors, are used to assess the validity of our approach.},
author = {Mirzaei, Faraz M. and Kottas, Dimitrios G. and Roumeliotis, Stergios I.},
doi = {10.1177/0278364911435689},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Mirzaei, Kottas, Roumeliotis/3D LIDAR-camera intrinsic and extrinsic calibration Identifiability and analytical least-squares-based initialization. Mirzaei, Kottas,.pdf:pdf},
issn = {02783649},
journal = {Int. J. Rob. Res.},
keywords = {Sensing and perception,calibration and identification,computer vision,range sensing},
number = {4},
pages = {452--467},
title = {{3D LIDAR-camera intrinsic and extrinsic calibration: Identifiability and analytical least-squares-based initialization}},
volume = {31},
year = {2012}
}
@techreport{Foote2014,
author = {Foote, Tully and Purvis, Mike},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Foote, Purvis/Coordinate Frame Conventions. Foote, Purvis. 2014.pdf:pdf},
pages = {1--5},
title = {{Coordinate Frame Conventions}},
year = {2014}
}
@article{Scaramuzza,
author = {Scaramuzza, Davide and Harati, Ahad and Siegwart, Roland},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Scaramuzza, Harati, Siegwart/Extrinsic self calibration of a camera and a 3D Laser Range Finder. Scaramuzza, Harati, Siegwart. Unknown.pdf:pdf},
keywords = {camera calibration,computer vision,laser calibration,omndirectional vision,omnidirectional camera,p3p algorithm,pnp algorithm,reprojection error,robotics,self calibration},
title = {{Extrinsic self calibration of a camera and a 3D Laser Range Finder}}
}
@misc{Unnikrishnan,
author = {Unnikrishnan, Ranjith and Hebert, Martial},
title = {{Laser-Camera Calibration Toolbox}},
url = {https://www.cs.cmu.edu/{~}ranjith/lcct.html}
}
@phdthesis{brabec2014,
address = {Prague},
author = {Brabec, Jan},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Brabec/Automated camera calibration from laser scanning data in natural environments. Brabec. 2014.pdf:pdf},
school = {Czech Technical University in Prague},
title = {{Automated camera calibration from laser scanning data in natural environments}},
year = {2014}
}
@article{Naroditsky2011,
abstract = {We propose a new method for extrinsic calibration of a line-scan LIDAR with a perspective projection camera. Our method is a closed-form, minimal solution to the problem. The solution is a symbolic template found via variable elimination and the multi-polynomial Macaulay resultant. It does not require initialization, and can be used in an automatic calibration setting when paired with RANSAC and least-squares refinement. We show the efficacy of our approach through a set of simulations and a real calibration. {\textcopyright} 2011 IEEE.},
author = {Naroditsky, Oleg and {Patterson IV}, Alexander and Daniilidis, Kostas},
doi = {10.1109/ICRA.2011.5980513},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Naroditsky, Patterson IV, Daniilidis/Automatic alignment of a camera with a line scan LIDAR system. Naroditsky, Patterson IV, Daniilidis. 2011.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {3429--3434},
publisher = {IEEE},
title = {{Automatic alignment of a camera with a line scan LIDAR system}},
year = {2011}
}
@article{Silva2018,
author = {Silva, Varuna De and Roche, Jamie and Kondoz, Ahmet},
doi = {10.3390/s18082730},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Silva, Roche, Kondoz/Robust Fusion of LiDAR and Wide-Angle Camera. Silva, Roche, Kondoz. 2018.pdf:pdf},
issn = {1424-8220},
keywords = {assistive robots,autonomous vehicles,depth sensing,detection,free space,gaussian process regression,lidar,sensor data fusion},
title = {{Robust Fusion of LiDAR and Wide-Angle Camera}},
year = {2018}
}
@article{Larusso2013a,
abstract = {Abstract: "A common need in machine vision is to compute the 3-D rigid transformation that exists between two sets of points in a coordinate system for which corresponding pairs have been determined. Several solutions to this problem have been devised. In this paper a comparative analysis of four popular algorithms is given. Each of them computes the translation and rotation of the transform in closed-form, as the solution to a least squares formulation of the problem. They differ in terms of the representation of the transform and the method of solution, using respectively: singular value decomposition of a matrix, orthonormal matrices, unit quaternions and dual quaternions. This comparison presents results of several experiments designed to determine (1) the accuracy of each algorithm in the presence of different levels of noise, (2) the stability of each technique with respect to degenerate data sets, and (3) the relative computation time of each approach for different sizes and forms of data."},
author = {Larusso, A and Eggert, DW and Fisher, RB},
doi = {10.5244/c.9.24},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Larusso, Eggert, Fisher/A Comparison of Four Algorithms for Estimating 3-D Rigid Transformations.. Larusso, Eggert, Fisher. 2013(2).pdf:pdf},
pages = {24.1--24.10},
title = {{A Comparison of Four Algorithms for Estimating 3-D Rigid Transformations.}},
year = {2013}
}
