Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Castorena2016,
abstract = {We present a new method for joint automatic extrinsic calibration and sensor fusion for a multimodal sensor system comprising a LIDAR and an optical camera. Our approach exploits the natural alignment of depth and intensity edges when the calibration parameters are correct. Thus, in contrast to a number of existing approaches, we do not require the presence or identification of known alignment targets. On the other hand, the characteristics of each sensor modality, such as sampling pattern and information measured, are significantly different, making direct edge alignment difficult. To overcome this difficulty, we jointly fuse the data and estimate the calibration parameters. In particular, the joint processing evaluates and optimizes both the quality of edge alignment and the performance of the fusion algorithm using a common cost function on the output. We demonstrate accurate calibration in practical configurations in which depth measurements are sparse and contain no reflectivity information. Experiments on synthetic and real data obtained with a three-dimensional LIDAR sensor demonstrate the effectiveness of our approach.},
author = {Castorena, Juan and Kamilov, Ulugbek S. and Boufounos, Petros T.},
doi = {10.1109/ICASSP.2016.7472200},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Castorena, Kamilov, Boufounos/Autocalibration of lidar and optical cameras via edge alignment. Castorena, Kamilov, Boufounos. 2016.pdf:pdf},
isbn = {9781479999880},
issn = {15206149},
journal = {ICASSP, IEEE Int. Conf. Acoust. Speech Signal Process. - Proc.},
keywords = {Multimodal calibration,depth superresolution,intersensor registration,sensor fusion,total variation},
pages = {2862--2866},
publisher = {IEEE},
title = {{Autocalibration of lidar and optical cameras via edge alignment}},
volume = {2016-May},
year = {2016}
}
@article{Park2019,
abstract = {We present a deep convolutional neural network (CNN) architecture for high-precision depth estimation by jointly utilizing sparse 3D LiDAR and dense stereo depth information. In this network, the complementary characteristics of sparse 3D LiDAR and dense stereo depth are simultaneously encoded in a boosting manner. Tailored to the LiDAR and stereo fusion problem, the proposed network differs from previous CNNs in the incorporation of a compact convolution module, which can be deployed with the constraints of mobile devices. As training data for the LiDAR and stereo fusion is rather limited, we introduce a simple yet effective approach for reproducing the raw KITTI dataset. The raw LIDAR scans are augmented by adapting an off-the-shelf stereo algorithm and a confidence measure. We evaluate the proposed network on the KITTI benchmark and data collected by our multi-sensor acquisition system. Experiments demonstrate that the proposed network generalizes across datasets and is significantly more accurate than various baseline approaches.},
author = {Park, Kihong and Kim, Seungryong and Sohn, Kwanghoon},
doi = {10.1109/tits.2019.2891788},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Park, Kim, Sohn/High-Precision Depth Estimation Using Uncalibrated LiDAR and Stereo Fusion. Park, Kim, Sohn. 2019.pdf:pdf},
issn = {1524-9050},
journal = {IEEE Trans. Intell. Transp. Syst.},
pages = {1--15},
publisher = {IEEE},
title = {{High-Precision Depth Estimation Using Uncalibrated LiDAR and Stereo Fusion}},
volume = {PP},
year = {2019}
}
@article{Li2016,
abstract = {Detailed 3D modeling of indoor scene has become an important topic in many research fields. It can provide extensive information about the environment and boost various location based services, such as interactive gaming and indoor navigation. This paper presents an indoor scene construction approach using 2D line-scan LiDAR and entry-level digital camera. Both devices are mounted rigidly on a robotic servo, which sweeps vertically to cover the third dimension. Fiducial target based extrinsic calibration is applied to acquire transformation matrices between LiDAR and camera. Based on the transformation matrix, we perform registration to fuse the color images from the camera with the 3D points cloud from the LiDAR. The whole system setup has much lower cost as compared to systems using 3D LiDAR and omnidirectional camera. Using pre-calculated transformation matrices instead of feature extraction techniques such as SIFT or SURF in registration gives better fusion result and lower computational complexity. The experiments carried out in office building environment show promising results of our approach.},
author = {Li, Juan and He, Xiang and Li, Jia},
doi = {10.1109/NAECON.2015.7443100},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Li, He, Li/2D LiDAR and camera fusion in 3D modeling of indoor environment. Li, He, Li. 2016.pdf:pdf},
isbn = {9781467375658},
issn = {23792027},
journal = {Proc. IEEE Natl. Aerosp. Electron. Conf. NAECON},
keywords = {2D line-scan LiDAR,3D indoor modeling,digital camera,extrinsic calibration,sensor fusion},
number = {June 2015},
pages = {379--383},
title = {{2D LiDAR and camera fusion in 3D modeling of indoor environment}},
volume = {2016-March},
year = {2016}
}
@article{Yang2017,
abstract = {Combining active and passive imaging sensors enables creating a more detailed 3D model of the real world. Then, these 3D data can be used for various applications, such as city mapping, indoor navigation, autonomous vehicles, etc. Typically, LiDAR and camera as imaging sensors are installed on these systems. Both of these sensors have advantages and drawbacks. Thus, LiDAR sensor directly provides relatively accurate 3D point cloud, but LiDAR point cloud barely contains the surface textures and details, such as traffic signs and alpha numeric information on facades. As opposed to LiDAR, deriving 3D point cloud from images require more computational resources, and in many cases, the accuracy and point density might be lower due to poor visual or light conditions. This paper investigates a workflow which utilizes factor graph SLAM, dense 3D reconstruction and ICP to efficiently generate the LiDAR and camera point clouds, and then, co-register in a navigation frame to provide a consistent and more detailed reconstruction of the environment. The workflow consists of three processing steps. First, we use factor graph SLAM, GPS/INS odometry and 6DOF scan matching to register the LiDAR point cloud. Then, the stereo images are processed by stereo-scan dense 3D reconstruction technique to generate dense point cloud. Finally, ICP method is used to co-register LiDAR and photogrammetric point clouds into one frame. The proposed method is tested with the KITTI dataset. The results show that data fusion of two point clouds can improve the quality of the 3D model.},
author = {Yang, Yuan and Koppanyi, Zoltan and Toth, Charles K},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yang, Koppanyi, Toth/Stereo Image Point Cloud and Lidar Point Cloud Fusion for the 3D Street Mapping. Yang, Koppanyi, Toth. 2017.pdf:pdf},
journal = {IGTF 2017 â€“ Imaging Geospatial Technol. Forum 2017-ASPRS Annu. Conf.},
keywords = {ICP,LiDAR point cloud,data fusion,factor graph,stereo image 3D reconstruction},
title = {{Stereo Image Point Cloud and Lidar Point Cloud Fusion for the 3D Street Mapping}},
url = {https://pdfs.semanticscholar.org/7c2e/a37a24b6a09266dd75936b5e4d53953d1179.pdf},
year = {2017}
}
@article{Wei2018,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@article{Liang2019,
abstract = {In this paper we propose to exploit multiple related tasks for accurate multi-sensor 3D object detection. Towards this goal we present an end-to-end learnable architecture that reasons about 2D and 3D object detection as well as ground estimation and depth completion. Our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels. Importantly, our approach leads the KITTI benchmark on 2D, 3D and bird's eye view object detection, while being real-time.},
author = {Liang, Ming and Yang, Bin and Chen, Yun and Hu, Rui and Urtasun, Raquel},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Liang et al/Multi-Task Multi-Sensor Fusion for 3D Object Detection. Liang et al.. 2019.pdf:pdf},
journal = {IEEE Conf. Comput. Vis. Pattern Recognition, Proceedings, CVPR},
keywords = {3d object detection,autonomous,multi-sensor fusion},
pages = {7345--7353},
title = {{Multi-Task Multi-Sensor Fusion for 3D Object Detection}},
url = {https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Multi-Task-Multi-Sensor-Fusion-for-3D-Object-Detection.pdf},
year = {2019}
}
@article{Scaramuzza,
author = {Scaramuzza, Davide and Harati, Ahad and Siegwart, Roland},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Scaramuzza, Harati, Siegwart/Extrinsic self calibration of a camera and a 3D Laser Range Finder. Scaramuzza, Harati, Siegwart. Unknown.pdf:pdf},
keywords = {camera calibration,computer vision,laser calibration,omndirectional vision,omnidirectional camera,p3p algorithm,pnp algorithm,reprojection error,robotics,self calibration},
title = {{Extrinsic self calibration of a camera and a 3D Laser Range Finder}}
}
