Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Castorena2016,
abstract = {We present a new method for joint automatic extrinsic calibration and sensor fusion for a multimodal sensor system comprising a LIDAR and an optical camera. Our approach exploits the natural alignment of depth and intensity edges when the calibration parameters are correct. Thus, in contrast to a number of existing approaches, we do not require the presence or identification of known alignment targets. On the other hand, the characteristics of each sensor modality, such as sampling pattern and information measured, are significantly different, making direct edge alignment difficult. To overcome this difficulty, we jointly fuse the data and estimate the calibration parameters. In particular, the joint processing evaluates and optimizes both the quality of edge alignment and the performance of the fusion algorithm using a common cost function on the output. We demonstrate accurate calibration in practical configurations in which depth measurements are sparse and contain no reflectivity information. Experiments on synthetic and real data obtained with a three-dimensional LIDAR sensor demonstrate the effectiveness of our approach.},
author = {Castorena, Juan and Kamilov, Ulugbek S. and Boufounos, Petros T.},
doi = {10.1109/ICASSP.2016.7472200},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Autocalibration of lidar and optical cameras via edge alignment. Castorena, Kamilov, Boufounos.pdf:pdf},
isbn = {9781479999880},
issn = {15206149},
journal = {ICASSP, IEEE Int. Conf. Acoust. Speech Signal Process. - Proc.},
keywords = {Multimodal calibration,depth superresolution,intersensor registration,sensor fusion,total variation},
pages = {2862--2866},
publisher = {IEEE},
title = {{Autocalibration of lidar and optical cameras via edge alignment}},
volume = {2016-May},
year = {2016}
}
@misc{Camerona,
abstract = {Retrofit a Car for self-driving using LIDAR, camera and Radar},
author = {Cameron, Oliver and Voyage},
keywords = {LIDAR,Self-Driving Cars,Sensor Fusion},
mendeley-tags = {LIDAR,Self-Driving Cars,Sensor Fusion},
title = {{The Story of Homer: Voyage's First Self-Driving Taxi}},
url = {https://news.voyage.auto/the-story-of-homer-voyages-first-self-driving-taxi-f0a6466718af},
urldate = {2018-10-09}
}
@inproceedings{Yue2018,
abstract = {3D LiDAR scanners are playing an increasingly important role in autonomous driving as they can generate depth information of the environment. However, creating large 3D LiDAR point cloud datasets with point-level labels requires a significant amount of manual annotation. This jeopardizes the efficient development of supervised deep learning algorithms which are often data-hungry. We present a framework to rapidly create point clouds with accurate point-level labels from a computer game. The framework supports data collection from both auto-driving scenes and user-configured scenes. Point clouds from auto-driving scenes can be used as training data for deep learning algorithms, while point clouds from user-configured scenes can be used to systematically test the vulnerability of a neural network, and use the falsifying examples to make the neural network more robust through retraining. In addition, the scene images can be captured simultaneously in order for sensor fusion tasks, with a method proposed to do automatic calibration between the point clouds and captured scene images. We show a significant improvement in accuracy (+9{\%}) in point cloud segmentation by augmenting the training dataset with the generated synthesized data. Our experiments also show by testing and retraining the network using point clouds from user-configured scenes, the weakness/blind spots of the neural network can be fixed.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1804.00103},
author = {Yue, Xiangyu and Wu, Bichen and Seshia, Sanjit A. and Keutzer, Kurt and Sangiovanni-Vincentelli, Alberto L.},
booktitle = {Proc. 2018 ACM Int. Conf. Multimed. Retr. - ICMR '18},
doi = {10.1145/3206025.3206080},
eprint = {1804.00103},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A LiDAR Point Cloud Generator. Yue et al.pdf:pdf},
isbn = {9781450350464},
month = {mar},
number = {Nips},
pages = {458--464},
publisher = {ACM Press},
title = {{A LiDAR Point Cloud Generator}},
url = {http://dl.acm.org/citation.cfm?doid=3206025.3206080 http://arxiv.org/abs/1804.00103},
year = {2018}
}
@misc{Cameron,
abstract = {Retrofit a Car for self-driving using LIDAR, camera and Radar},
author = {Cameron, Oliver and Voyage},
keywords = {LIDAR,Self-Driving Cars,Sensor Fusion},
mendeley-tags = {LIDAR,Self-Driving Cars,Sensor Fusion},
title = {{The Story of Homer: Voyage's First Self-Driving Taxi}},
url = {https://news.voyage.auto/the-story-of-homer-voyages-first-self-driving-taxi-f0a6466718af},
urldate = {2018-10-09}
}
@article{Fischler1981,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/ smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing and analysis conditions. Implementation details and computational examples are also presented.},
archivePrefix = {arXiv},
arxivId = {3629719},
author = {Fischler, Martin A. and Bolles, Robert C.},
doi = {10.1145/358669.358692},
eprint = {3629719},
isbn = {0934613338},
issn = {00010782},
journal = {Commun. ACM},
month = {jun},
number = {6},
pages = {381--395},
pmid = {4650729},
title = {{Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography}},
url = {http://portal.acm.org/citation.cfm?doid=358669.358692},
volume = {24},
year = {2002}
}
@article{EvansT.C.GavrilovichE.MihaiR.C.andIsbasescuI.2014,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0403007},
author = {{Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isbasescu, I.}, Easyg Llc and Thelen, Darryl and Martin, J A and Allen, S M and SA, Slane},
doi = {10.1037/t24245-000},
eprint = {0403007},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isbasescu, I. et al/( 12 ) Patent Application Publication ( 10 ) Pub . No . US 2006 0222585 A1 Figure 1. Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isba.pdf:pdf},
isbn = {2009023471},
issn = {13871811},
number = {15},
pages = {354},
pmid = {23110556},
primaryClass = {arXiv:physics},
title = {{( 12 ) Patent Application Publication ( 10 ) Pub . No .: US 2006 / 0222585 A1 Figure 1}},
volume = {002},
year = {2014}
}
@article{Fersch2017,
abstract = {{\textcopyright} 2017 IEEE. We present the application of optical code division multiple access (OCDMA) modulation based on optical orthogonal codes for automotive time-of-flight light detection and ranging (LiDAR) system sensors using avalanche photodiode (APD) detectors. The modulation opens the possibility to discriminate single laser transmissions. This allows the realization of additional features like enhancing the systems' interference robustness or accelerating their scanning rate. The requirements on automotive LiDAR OCDMA modulation differs from telecommunication's demands in several ways: The sensor must guarantee absolute laser safety; front facing devices must be able to provide reliable long range results up to a distance of 150m and beyond; and the signal is transmitted through free space. After outlining the basic functionality of the sort of sensors in consideration, the properties of the optical orthogonal codes (OOC) modulation are compared with the state of the art theoretically. The influence of OOC parameters with respect to scanning LiDAR systems is examined. The modulation technique is then demonstrated experimentally with two examples: The detection and separation of coded and traditional signals is shown using a matched filter detection algorithm. In the same way, differently coded signals can be separated from each other. Impediments of the interference suppression quality are discussed. Finally, an overview of possible applications of the proposed technique in automotive LiDAR systems is given, which are enabled by the OCDMA modulation in the first place.},
author = {Fersch, Thomas and Weigel, Robert and Koelpin, Alexander},
doi = {10.1109/JSEN.2017.2688126},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems. Fersch, Weigel, Koelpin.pdf:pdf},
issn = {1530-437X},
journal = {IEEE Sens. J.},
keywords = {Laser radar,LiDAR,OCDMA modulation,time-of-flight},
month = {jun},
number = {11},
pages = {3507--3516},
title = {{A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems}},
url = {http://ieeexplore.ieee.org/document/7887680/},
volume = {17},
year = {2017}
}
@inproceedings{Kim2015c,
abstract = {The LIDAR scanner is at the heart of object detection of the self-driving car. Mutual interference between LIDAR scanners has not been regarded as a problem because the percentage of vehicles equipped with LIDAR scanners was very rare. With the growing number of autonomous vehicle equipped with LIDAR scanner operated close to each other at the same time, the LIDAR scanner may receive laser pulses from other LIDAR scanners. In this paper, three types of experiments and their results are shown, according to the arrangement of two LIDAR scanners. We will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some typical mutual interference scenario and report an analysis of the interference mechanism. {\textcopyright} 2015 COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Seonghyeon and Park, Yongwan},
booktitle = {Phot. Count. Appl. 2015},
doi = {10.1117/12.2178502},
editor = {Prochazka, Ivan and Sobolewski, Roman and James, Ralph B.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Occurrence and characteristics of mutual interference between LIDAR scanners. Kim et al.. 2015.pdf:pdf},
keywords = {LIDAR scanner,ghost target,mutual interference,self-driving car,time of flight},
month = {may},
pages = {95040K},
publisher = {International Society for Optics and Photonics},
title = {{Occurrence and characteristics of mutual interference between LIDAR scanners}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2178502},
volume = {9504},
year = {2015}
}
@book{1385,
author = {Richard, Hartley and Andrew, Zisserman},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Multiple View Geometry in computer vision. غلامحسین.pdf:pdf},
isbn = {9780521540513},
pages = {302},
title = {{Multiple View Geometry in computer vision}},
year = {1385}
}
@article{Giuggioli2015,
abstract = {Animal coordinated movement interactions are commonly explained by assuming unspecified social forces of attraction, repulsion and alignment with parameters drawn from observed movement data. Here we propose and test a biologically realistic and quantifiable biosonar movement interaction mechanism for echolocating bats based on spatial perceptual bias, i.e. actual sound field, a reaction delay, and observed motor constraints in speed and acceleration. We found that foraging pairs of bats flying over a water surface swapped leader-follower roles and performed chases or coordinated manoeuvres by copying the heading a nearby individual has had up to 500 ms earlier. Our proposed mechanism based on the interplay between sensory-motor constraints and delayed alignment was able to recreate the observed spatial actor-reactor patterns. Remarkably, when we varied model parameters (response delay, hearing threshold and echolocation directionality) beyond those observed in nature, the spatio-temporal interaction patterns created by the model only recreated the observed interactions, i.e. chases, and best matched the observed spatial patterns for just those response delays, hearing thresholds and echolocation directionalities found to be used by bats. This supports the validity of our sensory ecology approach of movement coordination, where interacting bats localise each other by active echolocation rather than eavesdropping.},
author = {Giuggioli, Luca and McKetterick, Thomas J. and Holderied, Marc},
doi = {10.1371/journal.pcbi.1004089},
editor = {Ayers, Joseph},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Giuggioli, McKetterick, Holderied/Delayed Response and Biosonar Perception Explain Movement Coordination in Trawling Bats. Giuggioli, McKetterick, Holderied. 2015.pdf:pdf},
issn = {1553-7358},
journal = {PLOS Comput. Biol.},
month = {mar},
number = {3},
pages = {e1004089},
title = {{Delayed Response and Biosonar Perception Explain Movement Coordination in Trawling Bats}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1004089},
volume = {11},
year = {2015}
}
@article{Hast2013,
abstract = {Optimal RANSAC - Towards a Repeatable Algorithm for Finding the Optimal Set},
author = {Hast, Anders and Nysj{\"{o}}, Johan and Marchetti, Andrea},
issn = {12136972},
journal = {J. WSCG},
keywords = {3D planes,Feature matching,Image stitching,Local optimisation,Optimal set,RANSAC,Repeatable},
number = {1},
pages = {21--30},
title = {{Optimal RANSAC - Towards a repeatable algorithm for finding the optimal set}},
url = {http://www.cb.uu.se/{~}aht/articles/A53-full.pdf},
volume = {21},
year = {2013}
}
@article{Kim2015b,
abstract = {LIDAR scanners are essential components of intelligent vehicles capable of autonomous travel. Mutual interference between LIDAR scanners has not been regarded as a problem yet. Mutual interference was identified as a problem of increased importance because of the appearance of safety functions and the increasing rate of vehicles equipped with LIDAR scanner. This paper will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some generic interference scenarios and report on the current status of the analysis of interference mechanisms.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
doi = {10.1109/ITNG.2015.113},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/An Experiment of Mutual Interference between Automotive LIDAR Scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {9781479988273},
journal = {Proc. - 12th Int. Conf. Inf. Technol. New Gener. ITNG 2015},
keywords = {LIDAR scanner,autonomous vehicle,ghost target,mutual interference,obstacle detection},
pages = {680--685},
title = {{An Experiment of Mutual Interference between Automotive LIDAR Scanners}},
year = {2015}
}
@article{KresimirKusevicOttawaCA;PaulMrstikOttawaCA;CraigLenGlennieSpring2017,
author = {{Kresimir Kusevic, Ottawa (CA); Paul Mrstik, Ottawa (CA); Craig Len Glennie, Spring}, TX (US)},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Method and System for Aligninga Line Scan Camerawith a Lidar Scanner for Real Time Data Fusion in Three Dimiensions. Kresimir Kusevic, O.pdf:pdf},
isbn = {2222222222},
number = {19},
title = {{Method and System for Aligninga Line Scan Camerawith a Lidar Scanner for Real Time Data Fusion in Three Dimiensions}},
volume = {1},
year = {2017}
}
@article{Wei2018a,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@article{Fersch2017b,
abstract = {{\textcopyright} 2017 IEEE. We present the application of optical code division multiple access (OCDMA) modulation based on optical orthogonal codes for automotive time-of-flight light detection and ranging (LiDAR) system sensors using avalanche photodiode (APD) detectors. The modulation opens the possibility to discriminate single laser transmissions. This allows the realization of additional features like enhancing the systems' interference robustness or accelerating their scanning rate. The requirements on automotive LiDAR OCDMA modulation differs from telecommunication's demands in several ways: The sensor must guarantee absolute laser safety; front facing devices must be able to provide reliable long range results up to a distance of 150m and beyond; and the signal is transmitted through free space. After outlining the basic functionality of the sort of sensors in consideration, the properties of the optical orthogonal codes (OOC) modulation are compared with the state of the art theoretically. The influence of OOC parameters with respect to scanning LiDAR systems is examined. The modulation technique is then demonstrated experimentally with two examples: The detection and separation of coded and traditional signals is shown using a matched filter detection algorithm. In the same way, differently coded signals can be separated from each other. Impediments of the interference suppression quality are discussed. Finally, an overview of possible applications of the proposed technique in automotive LiDAR systems is given, which are enabled by the OCDMA modulation in the first place.},
author = {Fersch, Thomas and Weigel, Robert and Koelpin, Alexander},
doi = {10.1109/JSEN.2017.2688126},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems. Fersch, Weigel, Koelpin.pdf:pdf},
issn = {1530437X},
journal = {IEEE Sens. J.},
keywords = {Laser radar,LiDAR,OCDMA modulation,time-of-flight},
number = {11},
pages = {3507--3516},
title = {{A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems}},
volume = {17},
year = {2017}
}
@misc{CameronOliver2017,
abstract = {Why did LIDAR take off with self-driving cars? In a word: mapping. LIDAR allows you to generate huge 3D maps (its original application!), which you can then navigate the car or robot predictably within. By using a LIDAR to map and navigate an environment, you can know ahead of time the bounds of a lane, or that there is a stop sign or traffic light 500m ahead. This kind of predictability is exactly what a technology like self-driving cars requires, and has been a big reason for the progress over the last 5 years.
},
author = {{Cameron Oliver}},
booktitle = {Voyage},
keywords = {LIDAR,Self-Driving Cars},
mendeley-tags = {LIDAR,Self-Driving Cars},
title = {{An Introduction to LIDAR: The Key Self-Driving Car Sensor}},
url = {https://news.voyage.auto/an-introduction-to-lidar-the-key-self-driving-car-sensor-a7e405590cff},
urldate = {2018-10-09},
year = {2017}
}
@incollection{MartinVelasMichalSpanelZdenekMaterna2013,
abstract = {Quinolone-resistant clinical Escherichia coli isolates were examined for mutations in the marRAB operon of the multiple antibiotic resistance (mar) locus. Among 23 strains evaluated, 8 were chosen for further study: 3 that showed relatively high levels of uninduced, i.e., constitutive, expression of the operon and 5 with variable responses to induction by salicylate or tetracyclines. The marR genes, specifying the repressor of the operon, cloned from the three strains constitutively expressing the operon did not reduce the level of expression of beta-galactosidase from a marO::lacZ transcriptional fusion and were therefore mutant; however, marR genes cloned from the five other clinical strains repressed LacZ expression and were wild type. All three mutant marR genes contained more than one mutation: a deletion and a point mutation. Inactivation of the mar locus in the three known marR mutant strains with a kanamycin resistance cassette introduced by homologous recombination reduced resistance to quinolones and multiple antibiotics. These findings indicate that mar operon mutations exist in quinolone-resistant clinical E. coli isolates and contribute to quinolone and multidrug resistance.},
archivePrefix = {arXiv},
arxivId = {1505.00729},
author = {{Martin Velas, Michal Spanel, Zdenek Materna}, Adam Herout},
booktitle = {Flight Dyn. Princ.},
doi = {10.1016/B978-0-08-098242-7.00008-0},
eprint = {1505.00729},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Calibration of RGB Camera With Velodyne LiDAR. Martin Velas, Michal Spanel, Zdenek Materna.pdf:pdf},
isbn = {9781461434801},
issn = {0066-4804},
keywords = {L{\'{i}}DAR,Velodyne,calibration,camera,marker},
month = {jul},
number = {7},
pages = {219--241},
pmid = {8807064},
publisher = {Elsevier},
title = {{Calibration of RGB Camera With Velodyne LiDAR}},
url = {https://linkinghub.elsevier.com/retrieve/pii/B9780080982427000080},
volume = {40},
year = {2013}
}
@article{Wei2018,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@article{Kim2017,
author = {Kim, Gunzung and Eom, Jeongsook and Choi, Jeonghee and Park, Yongwan},
doi = {10.14372/iemek.2017.12.1.43},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Mutual Interference on Mobile Pulsed Scanning LIDAR. Kim et al.. 2017.pdf:pdf},
issn = {1975-5066},
journal = {IEMEK J. Embed. Syst. Appl.},
month = {feb},
number = {1},
pages = {43--62},
title = {{Mutual Interference on Mobile Pulsed Scanning LIDAR}},
url = {http://koreascience.or.kr/journal/view.jsp?kj=OBDDBE{\&}py=2017{\&}vnc=v12n1{\&}sp=43},
volume = {12},
year = {2017}
}
@article{Roca2006a,
archivePrefix = {arXiv},
arxivId = {arXiv:1208.5721},
author = {Roca, Sergi Figueras and Cited, References},
doi = {10.1038/incomms1464},
eprint = {arXiv:1208.5721},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Roca, Cited/( 12 ) United States Patent. Roca, Cited. 2006.pdf:pdf},
isbn = {1020080012051},
issn = {2004001828},
number = {12},
pmid = {1000182772},
title = {{( 12 ) United States Patent}},
volume = {2},
year = {2006}
}
@misc{CameronOliver2017a,
abstract = {Why did LIDAR take off with self-driving cars? In a word: mapping. LIDAR allows you to generate huge 3D maps (its original application!), which you can then navigate the car or robot predictably within. By using a LIDAR to map and navigate an environment, you can know ahead of time the bounds of a lane, or that there is a stop sign or traffic light 500m ahead. This kind of predictability is exactly what a technology like self-driving cars requires, and has been a big reason for the progress over the last 5 years.
},
author = {{Cameron Oliver}},
booktitle = {Voyage},
keywords = {LIDAR,Self-Driving Cars},
mendeley-tags = {LIDAR,Self-Driving Cars},
title = {{An Introduction to LIDAR: The Key Self-Driving Car Sensor}},
url = {https://news.voyage.auto/an-introduction-to-lidar-the-key-self-driving-car-sensor-a7e405590cff},
urldate = {2018-10-09},
year = {2017}
}
@inproceedings{Javaheri2017,
abstract = {The increasing availability of point cloud data in recent years is demanding for high performance denoising methods and compression schemes. When point cloud data is directly obtained from depth sensors or extracted from images acquired from different viewpoints, imprecisions on the depth acquisition or in the 3D reconstruction techniques result in noisy point clouds which may include a significant number of outliers. Moreover, the quality assessment of point clouds is a challenging problem since this 3D representation format is unstructured and it is typically not directly visualized. In this paper, selected objective quality metrics are evaluated regarding their correlation with human quality assessment and thus human perception. As far as the authors know, this is the first paper performing the subjective assessment of point cloud denoising algorithms and the evaluation of most used point cloud objective quality metrics. Experimental results show that graph-based denoising algorithms can improve significantly the point cloud quality data and that objective metrics that model the underlying point cloud surface can correlate better with human perception.},
author = {Javaheri, Alireza and Brites, Catarina and Pereira, Fernando and Ascenso, Jo{\~{a}}o},
booktitle = {2017 IEEE 19th Int. Work. Multimed. Signal Process. MMSP 2017},
doi = {10.1109/MMSP.2017.8122239},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Subjective and objective quality evaluation of compressed point clouds. Javaheri et al.pdf:pdf},
isbn = {9781509036493},
issn = {1047-2797},
keywords = {Point cloud compression,Quality metrics,Subjective quality assessment},
pages = {1--6},
pmid = {10037558},
title = {{Subjective and objective quality evaluation of compressed point clouds}},
volume = {2017-Janua},
year = {2017}
}
@article{Roca2006,
archivePrefix = {arXiv},
arxivId = {arXiv:1208.5721},
author = {Roca, Sergi Figueras and Cited, References},
doi = {10.1038/incomms1464},
eprint = {arXiv:1208.5721},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Roca, Cited/( 12 ) United States Patent. Roca, Cited. 2006.pdf:pdf},
isbn = {1020080012051},
issn = {2004001828},
number = {12},
pmid = {1000182772},
title = {{( 12 ) United States Patent}},
volume = {2},
year = {2006}
}
@inproceedings{Kim2015,
abstract = {LIDAR scanners are essential components of intelligent vehicles capable of autonomous travel. Mutual interference between LIDAR scanners has not been regarded as a problem yet. Mutual interference was identified as a problem of increased importance because of the appearance of safety functions and the increasing rate of vehicles equipped with LIDAR scanner. This paper will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some generic interference scenarios and report on the current status of the analysis of interference mechanisms.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
booktitle = {2015 12th Int. Conf. Inf. Technol. - New Gener.},
doi = {10.1109/ITNG.2015.113},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/An Experiment of Mutual Interference between Automotive LIDAR Scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {978-1-4799-8828-0},
keywords = {LIDAR scanner,autonomous vehicle,ghost target,mutual interference,obstacle detection},
month = {apr},
pages = {680--685},
publisher = {IEEE},
title = {{An Experiment of Mutual Interference between Automotive LIDAR Scanners}},
url = {http://ieeexplore.ieee.org/document/7113553/},
year = {2015}
}
@article{Fischler2002,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/ smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing and analysis conditions. Implementation details and computational examples are also presented.},
archivePrefix = {arXiv},
arxivId = {3629719},
author = {Fischler, Martin A. and Bolles, Robert C.},
doi = {10.1145/358669.358692},
eprint = {3629719},
isbn = {0934613338},
issn = {00010782},
journal = {Commun. ACM},
month = {jun},
number = {6},
pages = {381--395},
pmid = {4650729},
title = {{Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography}},
url = {http://portal.acm.org/citation.cfm?doid=358669.358692},
volume = {24},
year = {2002}
}
@misc{Condliffe2015,
author = {Condliffe, Jamie},
booktitle = {Gizmodo},
title = {{A {\$}60 Hack Can Fool the LIDAR Sensors Used on Most Self-Driving Cars}},
url = {https://gizmodo.com/a-60-hack-can-fool-the-lidar-sensors-used-on-most-self-1729272292},
urldate = {2018-11-28},
year = {2015}
}
@article{Christopher2015,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0403007},
author = {Christopher, Inventors and Ramsey, Paul and Chant, Garry Richard and Lockley, Andrew Robert and Gb, Wantage and Fields, Brian and Watson, Martin John and Rachel, Eleanor and Hyde, Ann and Gb, Wantage and Jasper, A},
doi = {10.1016/j.(73)},
eprint = {0403007},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Christopher et al/( 12 ) United States Patent ( 10 ) Patent No .. Christopher et al.. 2015.pdf:pdf},
isbn = {2010512510},
issn = {2470-0010},
number = {12},
pmid = {1000182772},
primaryClass = {arXiv:physics},
title = {{( 12 ) United States Patent ( 10 ) Patent No .:}},
volume = {2},
year = {2015}
}
@article{Steder,
author = {Steder, Bastian},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/The Point Cloud Library PCL Thanks to Radu Rusu from Willow Garage for some of the slides!. Steder.pdf:pdf},
title = {{The Point Cloud Library PCL Thanks to Radu Rusu from Willow Garage for some of the slides!}},
url = {http://ais.informatik.uni-freiburg.de/teaching/ws10/robotics2/pdfs/rob2-12-ros-pcl.pdf}
}
@misc{Simonite2017,
author = {Simonite, Tom},
booktitle = {MT Technol. Rev.},
title = {{Self-Driving Cars' Spinning-Laser Problem - MIT Technology Review}},
url = {https://www.technologyreview.com/s/603885/autonomous-cars-lidar-sensors/},
urldate = {2019-01-23},
year = {2017}
}
@inproceedings{Yoneda2014,
abstract = {In recent years, automated vehicle researches move on to the next stage, that is auto-driving experiments on public roads. Major challenge is how to robustly drive at complicated situations such as narrow or non-featured road. In order to realize practical performance, some static information should be kept on memory such as road topology, building shape, white line, curb, traffic light and so on. Currently, some measurement companies have already begun to prepare map database for automated vehicles. They are able to provide highly-precise 3-D map for robust automated driving. This study focuses on what kind of data should be observed during automated driving with such precise database. In particular, we focus on the accurate localization based on the use of lidar data and precise 3-D map, and propose a feature quantity for scan data based on distribution of clusters. Localization experiment shows that our method can measure surrounding uncertainty and guarantee accurate localization.},
author = {Yoneda, Keisuke and Tehrani, Hossein and Ogawa, Takashi and Hukuyama, Naohisa and Mita, Seiichi},
booktitle = {IEEE Intell. Veh. Symp. Proc.},
doi = {10.1109/IVS.2014.6856596},
isbn = {9781479936380},
issn = {10901981},
pages = {1345--1350},
pmid = {12693519},
title = {{Lidar scan feature for localization with highly precise 3-D map}},
year = {2014}
}
@article{Chum2005,
abstract = {The problem of model parameters estimation from data with a presence of outlier measurements often arises in computer vision and methods of robust estimation have to be used. The RANSAC algorithm introduced by Fishler and Bolles in 1981 is the a widely used robust estimator in the field of computer vision. The algorithm is capable of providing good estimates from data contaminated by large (even significantly more than 50{\%}) fraction of outliers. RANSAC is an optimization method that uses a data-driven random sampling of the parameter space to find the extremum of the cost function. Samples of data define points of the parameter space in which the cost function is evaluated and model parameters with the best score are output. This thesis provides a detailed analysis of RANSAC, which is recast as time-constrained op- timization – a solution that is optimal with certain confidence is sought in the shortest possible time. Next, the concept of randomized cost function evaluation in RANSAC is introduced and its superiority over the deterministic evaluation is shown. A provably optimal strategy for the ran- domized cost function evaluation is derived. A known discrepancy, caused by noise on inliers, between theoretical prediction of the time required to find the solution and practically observed running times is traced to a tacit assump- tions of RANSAC. The proposed LO-RANSAC algorithm reaches almost perfect agreement with theoretical predictions without any negative impact on the time complexity. A unified method of estimation of model and its degenerate configuration (epipolar geome- try and homography of a dominant plane) at the same time without a priori knowledge of the presence of the degenerate configuration (dominant plane) is derived. Next, it is shown that using oriented geometric constraints that arise from a realistic model of physical camera devices, saves non-negligible fraction of computational time. No negative side effect are related to the application of the oriented constraints. An algorithm exploiting (possibly noisy) match quality to modify the sampling strategy is introduced. The quality of a match is an often freely available quantity in the matching prob- lem. The approach increases the efficiency of the algorithm while keeping the same robustness as RANSAC in the worst-case situation (when the match quality is unrelated to whether a corre- spondence is a mismatch or not). Most of the algorithms in the thesis are motivated by (and presented on) estimation of a multi- view geometry. The algorithms are, however, general robust estimation techniques and can be easily used in other application areas too.},
author = {Chum, Ondrej},
doi = {10.1016/S0921-8777(99)00013-0},
isbn = {3015947974},
issn = {1213-2365},
journal = {Phd Thesis},
pmid = {10422537},
title = {{Two-View Geometry Estimation by Random Sample and Consensus}},
url = {http://cmp.felk.cvut.cz/{~}chum/papers/Chum-PhD.pdf},
year = {2005}
}
@article{Yue2018a,
abstract = {3D LiDAR scanners are playing an increasingly important role in autonomous driving as they can generate depth information of the environment. However, creating large 3D LiDAR point cloud datasets with point-level labels requires a significant amount of manual annotation. This jeopardizes the efficient development of supervised deep learning algorithms which are often data-hungry. We present a framework to rapidly create point clouds with accurate point-level labels from a computer game. The framework supports data collection from both auto-driving scenes and user-configured scenes. Point clouds from auto-driving scenes can be used as training data for deep learning algorithms, while point clouds from user-configured scenes can be used to systematically test the vulnerability of a neural network, and use the falsifying examples to make the neural network more robust through retraining. In addition, the scene images can be captured simultaneously in order for sensor fusion tasks, with a method proposed to do automatic calibration between the point clouds and captured scene images. We show a significant improvement in accuracy (+9{\%}) in point cloud segmentation by augmenting the training dataset with the generated synthesized data. Our experiments also show by testing and retraining the network using point clouds from user-configured scenes, the weakness/blind spots of the neural network can be fixed.},
archivePrefix = {arXiv},
arxivId = {1804.00103},
author = {Yue, Xiangyu and Wu, Bichen and Seshia, Sanjit A. and Keutzer, Kurt and Sangiovanni-Vincentelli, Alberto L.},
doi = {10.1145/3206025.3206080},
eprint = {1804.00103},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A LiDAR Point Cloud Generator. Yue et al.pdf:pdf},
isbn = {9781450350464},
journal = {Proc. 2018 ACM Int. Conf. Multimed. Retr.  - ICMR '18},
number = {Nips},
pages = {458--464},
title = {{A LiDAR Point Cloud Generator}},
url = {http://dl.acm.org/citation.cfm?doid=3206025.3206080},
year = {2018}
}
@article{Simon2018,
abstract = {Lidar based 3D object detection is inevitable for autonomous driving, because it directly links to environmental understanding and therefore builds the base for prediction and motion planning. The capacity of inferencing highly sparse 3D data in real-time is an ill-posed problem for lots of other application areas besides automated vehicles, e.g. augmented reality, personal robotics or industrial automation. We introduce Complex-YOLO, a state of the art real-time 3D object detection network on point clouds only. In this work, we describe a network that expands YOLOv2, a fast 2D standard object detector for RGB images, by a specific complex regression strategy to estimate multi-class 3D boxes in Cartesian space. Thus, we propose a specific Euler-Region-Proposal Network (E-RPN) to estimate the pose of the object by adding an imaginary and a real fraction to the regression network. This ends up in a closed complex space and avoids singularities, which occur by single angle estimations. The E-RPN supports to generalize well during training. Our experiments on the KITTI benchmark suite show that we outperform current leading methods for 3D object detection specifically in terms of efficiency. We achieve state of the art results for cars, pedestrians and cyclists by being more than five times faster than the fastest competitor. Further, our model is capable of estimating all eight KITTI-classes, including Vans, Trucks or sitting pedestrians simultaneously with high accuracy.},
annote = {Novel approach to detect 3D objects in ponit clouds
Usam uma rede neuronal baseada na Darknet. Adicionam-lhe uma estiam{\c{c}}{\~{a}}o usando Euler e angulos complexos para a Region Proposal Network, em vez de uma CNN (Convolutional Neural Network)

As caracter{\'{i}}sticas da point cloud s{\~{a}}o mapeadas para 3 coordenadas espec{\'{i}}ficas, ap{\'{o}}s ter sido efetuado um pr{\'{e}}-processamento que agrupa e reduz o tamanho da point cloud. Nessas 3 coordenadas encontram-se:
G - maximum height
B - maximum intensity
R - Normalized density points

A altura dos objetos n{\~{a}}o {\'{e}} atribu{\'{i}}da pela rede neuronal, sendo computada pela m{\'{e}}dia das alturas po classe de objectos.

Faz a an{\'{a}}lise em birds eye view},
archivePrefix = {arXiv},
arxivId = {1803.06199},
author = {Simon, Martin and Milz, Stefan and Amende, Karl and Gross, Horst-Michael},
eprint = {1803.06199},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Simon et al/Complex-YOLO Real-time 3D Object Detection on Point Clouds. Simon et al.. 2018.pdf:pdf},
keywords = {3d object detection,au-,lidar,point cloud processing},
pages = {1--14},
title = {{Complex-YOLO: Real-time 3D Object Detection on Point Clouds}},
url = {http://arxiv.org/abs/1803.06199},
year = {2018}
}
@inproceedings{Chen2014,
abstract = {Airborne Light Detection and Ranging (LiDAR) has the ability of acquiring high-resolution and high-accuracy point clouds. The processing on point clouds has thus become an important research topic and has drawn increasing attention in the fields of remote sensing. An increasing number of 3D building models have been available in the Internet with the development of Web 2.0 techniques and scanning equipment. Many web-based data-sharing platforms, such as Google 3D Warehouse and MakerBot Thingiverse, provide functions for users to upload and share their models. Therefore, a fitting approach is proposed to construct building models using airborne LiDAR data. An iterative approach consists of three main parts, geometric analysis, point cloud segmentation, and model refinement, in proposed the experimental result shows that the proposed approach can generate 3D building models efficiently.},
author = {Chen, Y.-C. and Chen, J.-Y. and Tsao, H.-C. and Lin, C.-H.},
booktitle = {35th Asian Conf. Remote Sens. 2014, ACRS 2014 Sens. Reintegration Soc.},
keywords = {3D building modeling,Model refinement,Point cloud reconstruction},
title = {{Constraint-base lidar point cloud fitting}},
year = {2014}
}
@article{Gomes2013,
abstract = {Recent hardware technologies have enabled acquisition of 3D point clouds from real world scenes in real time. A variety of interactive applications with the 3D world can be developed on top of this new technological scenario. However, a main problem that still remains is that most processing techniques for such 3D point clouds are computationally intensive, requiring optimized approaches to handle such images, especially when real time performance is required. As a possible solution, we propose the use of a 3D moving fovea based on a multiresolution technique that processes parts of the acquired scene using multiple levels of resolution. Such approach can be used to identify objects in point clouds with efficient timing. Experiments show that the use of the moving fovea shows a seven fold performance gain in processing time while keeping 91.6{\%} of true recognition rate in comparison with state-of-the-art 3D object recognition methods. {\textcopyright} 2013 The Authors. Published by Elsevier Ltd. All rights reserved.},
author = {Gomes, Rafael Beserra and {Da Silva}, Bruno Marques Ferreira and {De Medeiros Rocha}, Lourena Karin and Aroca, Rafael Vidal and Velho, Luiz Carlos Pacheco Rodrigues and Gon{\c{c}}alves, Luiz Marcos Garcia},
doi = {10.1016/j.cag.2013.03.005},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gomes et al/Efficient 3D object recognition using foveated point clouds. Gomes et al.. 2013.pdf:pdf},
issn = {00978493},
journal = {Comput. Graph.},
keywords = {3D object recognition,Moving fovea,Point cloud},
number = {5},
pages = {496--508},
publisher = {Elsevier},
title = {{Efficient 3D object recognition using foveated point clouds}},
url = {http://dx.doi.org/10.1016/j.cag.2013.03.005},
volume = {37},
year = {2013}
}
@misc{Gonzalez2002,
author = {Gonzalez, Rafael C.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gonzalez/Digital{\_}Image{\_}Processing{\_}2ndEd.pdf. Gonzalez. 2002.pdf:pdf},
title = {{Digital{\_}Image{\_}Processing{\_}2ndEd.pdf}},
year = {2002}
}
@article{Zhou2014,
abstract = {In this paper, we present a new minimal solution for the extrinsic calibration of a 2D light detection and ranging (LIDAR) sensor and a perspective camera. This problem is formulated as registering three planes and the corresponding coplanar lines. All existing algorithms solve this problem by its geometric structure in the original or the dual 3D space. In contrast, our algorithm directly exploits the algebraic structure of the polynomial system to resolve this problem. This new algorithm is more abstract, however, and provides a more broadly applicable method to other problems that need to handle the similar polynomial system. The rotation matrix is estimated first. Then, the translation vector can be calculated by solving a system of three linear equations. Although the new approach is conceptually simple, it has 720 different versions caused by different permutations of variables. This results in different computational orders and affects the numerical behavior of the algorithm. A simple heuristic scheme is proposed to select the permutation of variables that yields numerically stable computational order with respect to the given input. Simulation and experimental results show that the proposed algorithm outperforms the existing state-of-the-art algorithms in terms of accuracy and numerical stability.},
author = {Zhou, Lipu and Deng, Zhidong},
doi = {10.1088/0957-0233/25/6/065107},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Zhou, Deng/A new algorithm for the extrinsic calibration of a 2D LIDAR and a camera. Zhou, Deng. 2014.pdf:pdf},
issn = {13616501},
journal = {Meas. Sci. Technol.},
keywords = {extrinsic calibration,minimal solution,orthonormal constraints,sensor fusion},
number = {6},
title = {{A new algorithm for the extrinsic calibration of a 2D LIDAR and a camera}},
volume = {25},
year = {2014}
}
@article{ROS1989,
author = {ROS},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/ROS/Coodinate Frames, Transforms, and TF. ROS. 1989.pdf:pdf},
pages = {2--3},
title = {{Coodinate Frames, Transforms, and TF}},
url = {http://www.ros.org/wiki/tf/Overview/Transformations},
year = {1989}
}
@article{Menze2015a,
abstract = {This paper proposes a novel model and dataset for 3D scene flow estimation with an application to autonomous driving. Taking advantage of the fact that outdoor scenes often decompose into a small number of independently mov- ing objects, we represent each element in the scene by its rigid motion parameters and each superpixel by a 3D plane as well as an index to the corresponding object. This minimal representation increases robustness and leads to a discrete-continuous CRF where the data term decomposes into pairwise potentials between superpixels and objects. Moreover, our model intrinsically segments the scene into its constituting dynamic components. We demonstrate the performance of our model on existing benchmarks as well as a novel realistic dataset with scene flow ground truth. We obtain this dataset by annotating 400 dynamic scenes from the KITTI raw data collection using detailed 3D CAD models for all vehicles in motion. Our experiments also re- veal novel challenges which cannot be handled by existing methods. 1.},
author = {Menze, Moritz and Geiger, Andreas},
doi = {10.1109/CVPR.2015.7298925},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Menze, Geiger/Object scene flow for autonomous vehicles. Menze, Geiger. 2015.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {3061--3070},
title = {{Object scene flow for autonomous vehicles}},
volume = {07-12-June},
year = {2015}
}
@book{AlliedVisionTechnologies2010,
author = {{Allied Vision Technologies}},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Allied Vision Technologies/AVT Manta Hardware Installation Guide. Allied Vision Technologies. 2010.pdf:pdf},
isbn = {6048758855},
number = {January},
pages = {1--53},
title = {{AVT Manta Hardware Installation Guide}},
year = {2010}
}
@article{Miknis2015,
abstract = {Real-time 3D data processing is important in robotics, video games, environmental mapping, medical and many other fields. In this paper we propose a novel optimisation approach for the open source Point Cloud Library (PCL) that is frequently used for processing 3D data. Three aspects of the PCL are discussed: point cloud creation from disparity of colour image pairs, voxel grid downsample filtering to simplify point clouds and passthrough filtering to adjust the size of the point cloud. Additionally, rendering is examined. An optimisation technique based on CPU cycle measurement is proposed and applied in order to optimise those parts of the processing chain where measured performance is worst. The PCL modules thus optimised show on average an improvement in speed of 2.4x for point cloud creation, 91x for voxel grid filtering and 7.8x for the passthrough filter.},
author = {Miknis, Marius and Davies, Ross and Plassmann, Peter and Ware, Andrew},
doi = {10.1109/IWSSIP.2015.7314200},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Miknis et al/Near real-time point cloud processing using the PCL. Miknis et al.. 2015.pdf:pdf},
isbn = {9781467383530},
journal = {2015 22nd Int. Conf. Syst. Signals Image Process. - Proc. IWSSIP 2015},
keywords = {PCL,Point clouds,Real-time},
number = {section C},
pages = {153--156},
publisher = {IEEE},
title = {{Near real-time point cloud processing using the PCL}},
year = {2015}
}
@misc{Condliffe2015,
author = {Condliffe, Jamie},
booktitle = {Gizmodo},
title = {{A {\$}60 Hack Can Fool the LIDAR Sensors Used on Most Self-Driving Cars}},
url = {https://gizmodo.com/a-60-hack-can-fool-the-lidar-sensors-used-on-most-self-1729272292},
urldate = {2018-11-28},
year = {2015}
}
@misc{Al.2013,
abstract = {An array-based light detection and ranging (LiDAR) unit includes an array of emitter/detector sets configured to cover a field of view for the unit. Each emitter/detector set emits and receives light energy on a specific coincident axis unique for that emitter/detector set. A control system coupled to the array of emitter/detector sets controls initiation of light energy from each emitter and processes time of flight information for light energy received on the coincident axis by the corresponding detector for the emitter/detector set. The time of flight information provides imaging information corresponding to the field of view. Interference among light energy is reduced with respect to detectors in the LiDAR unit not corresponding to the specific coincident axis, and with respect to other LiDAR units and ambient sources of light energy. In one embodiment, multiple array-based LiDAR units are used as part of a control system for an autonomous vehicle.},
author = {Retterath, Jamie E. and Laumeyer, Robert A.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Unknown/Methods and Apparatus for Array Based Lidar Systems with Reduced Interference. Unknown. 2013.pdf:pdf},
institution = {Facet Technology Corp.,},
month = {nov},
title = {{Methods and Apparatus for Array Based Lidar Systems with Reduced Interference}},
url = {https://patents.google.com/patent/US20150131080A1/en},
year = {2015}
}
@article{Pereira2016,
abstract = {Autonomous navigation is an important field of research and, given the complexity of real world environments, most of the systems rely on a complex perception system combining multiple sensors on board, which reinforces the concern of sensor calibration. Most calibration methods rely on manual or semi-automatic interactive procedures, but reliable fully automatic methods are still missing. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. The idea proposed in this paper is to use a ball in motion in front of a set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation among all pairs of sensors. This paper proposes and describes such a method with results demonstrating its validity.},
author = {Pereira, Marcelo and Silva, David and Santos, Vitor and Dias, Paulo},
doi = {10.1016/j.robot.2016.05.010},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pereira et al/Self calibration of multiple LIDARs and cameras on autonomous vehicles. Pereira et al.. 2016.pdf:pdf},
issn = {09218890},
journal = {Rob. Auton. Syst.},
keywords = {3D data fitting,Extrinsic calibration,Point cloud},
pages = {326--337},
title = {{Self calibration of multiple LIDARs and cameras on autonomous vehicles}},
volume = {83},
year = {2016}
}
@book{Fairchild2016,
abstract = {GitHub - FairchildC/ROS-Robotics-by-Example: ROS Robotics By Example -{\textgreater} source code by chapter},
author = {Fairchild, Carol and Harman, Thomas L.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fairchild, Harman/ROS Robotics By Example. Fairchild, Harman. 2016.pdf:pdf},
isbn = {9781782175193},
pages = {428},
title = {{ROS Robotics By Example}},
year = {2016}
}
@book{GmbH2016a,
author = {GmbH, Allied Vision Technologies},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/GmbH/Manta Technical Manual Manta at a glance. GmbH. 2016(3).pdf:pdf},
isbn = {3642867723},
title = {{Manta Technical Manual Manta at a glance}},
url = {http://dv.ujaen.es/goto{\_}docencia{\_}file{\_}769494{\_}download.html},
year = {2016}
}
@article{Liao2017,
abstract = {Fusion of heterogeneous extroceptive sensors is the most effient and effective way to representing the environment precisely, as it overcomes various defects of each homogeneous sensor. The rigid transformation (aka. extrinsic parameters) of heterogeneous sensory systems should be available before precisely fusing the multisensor information. Researchers have proposed several approaches to estimating the extrinsic parameters. These approaches require either auxiliary objects, like chessboards, or extra help from human to select correspondences. In this paper, we proposed a novel extrinsic calibration approach for the extrinsic calibration of range and image sensors. As far as we know, it is the first automatic approach with no requirement of auxiliary objects or any human interventions. First, we estimate the initial extrinsic parameters from the individual motion of the range finder and the camera. Then we extract lines in the image and point-cloud pairs, to refine the line feature associations by the initial extrinsic parameters. At the end, we discussed the degenerate case which may lead to the algorithm failure and validate our approach by simulation. The results indicate high-precision extrinsic calibration results against the ground-truth.},
archivePrefix = {arXiv},
arxivId = {1703.04391},
author = {Liao, Qinghai and Liu, Ming and Tai, Lei and Ye, Haoyang},
eprint = {1703.04391},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Liao et al/Extrinsic Calibration of 3D Range Finder and Camera without Auxiliary Object or Human Intervention. Liao et al.. 2017.pdf:pdf},
title = {{Extrinsic Calibration of 3D Range Finder and Camera without Auxiliary Object or Human Intervention}},
url = {http://arxiv.org/abs/1703.04391},
year = {2017}
}
@incollection{MartinVelasMichalSpanelZdenekMaterna2013,
abstract = {Quinolone-resistant clinical Escherichia coli isolates were examined for mutations in the marRAB operon of the multiple antibiotic resistance (mar) locus. Among 23 strains evaluated, 8 were chosen for further study: 3 that showed relatively high levels of uninduced, i.e., constitutive, expression of the operon and 5 with variable responses to induction by salicylate or tetracyclines. The marR genes, specifying the repressor of the operon, cloned from the three strains constitutively expressing the operon did not reduce the level of expression of beta-galactosidase from a marO::lacZ transcriptional fusion and were therefore mutant; however, marR genes cloned from the five other clinical strains repressed LacZ expression and were wild type. All three mutant marR genes contained more than one mutation: a deletion and a point mutation. Inactivation of the mar locus in the three known marR mutant strains with a kanamycin resistance cassette introduced by homologous recombination reduced resistance to quinolones and multiple antibiotics. These findings indicate that mar operon mutations exist in quinolone-resistant clinical E. coli isolates and contribute to quinolone and multidrug resistance.},
archivePrefix = {arXiv},
arxivId = {1505.00729},
author = {{Martin Velas, Michal Spanel, Zdenek Materna}, Adam Herout},
booktitle = {Flight Dyn. Princ.},
doi = {10.1016/B978-0-08-098242-7.00008-0},
eprint = {1505.00729},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Calibration of RGB Camera With Velodyne LiDAR. Martin Velas, Michal Spanel, Zdenek Materna.pdf:pdf},
isbn = {9781461434801},
issn = {0066-4804},
keywords = {L{\'{i}}DAR,Velodyne,calibration,camera,marker},
month = {jul},
number = {7},
pages = {219--241},
pmid = {8807064},
publisher = {Elsevier},
title = {{Calibration of RGB Camera With Velodyne LiDAR}},
url = {https://linkinghub.elsevier.com/retrieve/pii/B9780080982427000080},
volume = {40},
year = {2013}
}
@article{Zhang1994a,
abstract = {A heuristic method has been developed for registering two sets of 3-D curves obtained by using an edge-based stereo system, or two dense 3-D maps obtained by using a correlation-based stereo system. Geometric matching in general is a difficult unsolved problem in computer vision. Fortunately, in many practical applications, some a priori knowledge exists which considerably simplifies the problem. In visual navigation, for example, the motion between successive positions is usually approximately known. From this initial estimate, our algorithm computes observer motion with very good precision, which is required for environment modeling (e.g., building a Digital Elevation Map). Objects are represented by a set of 3-D points, which are considered as the samples of a surface. No constraint is imposed on the form of the objects. The proposed algorithm is based on iteratively matching points in one set to the closest points in the other. A statistical method based on the distance distribution is used to deal with outliers, occlusion, appearance and disappearance, which allows us to do subset-subset matching. A least-squares technique is used to estimate 3-D motion from the point correspondences, which reduces the average distance between points in the two sets. Both synthetic and real data have been used to test the algorithm, and the results show that it is efficient and robust, and yields an accurate motion estimate.},
author = {Zhang, Zhengyou},
doi = {10.1007/BF01427149},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Zhang/Iterative point matching for registration of free-form curves and surfaces. Zhang. 1994.pdf:pdf},
issn = {09205691},
journal = {Int. J. Comput. Vis.},
number = {2},
pages = {119--152},
title = {{Iterative point matching for registration of free-form curves and surfaces}},
volume = {13},
year = {1994}
}
@article{Park2014,
abstract = {Calibration between color camera and 3D Light Detection And Ranging (LIDAR) equipment is an essential process for data fusion. The goal of this paper is to improve the calibration accuracy between a camera and a 3D LIDAR. In particular, we are interested in calibrating a low resolution 3D LIDAR with a relatively small number of vertical sensors. Our goal is achieved by employing a new methodology for the calibration board, which exploits 2D-3D correspondences. The 3D corresponding points are estimated from the scanned laser points on the polygonal planar board with adjacent sides. Since the lengths of adjacent sides are known, we can estimate the vertices of the board as a meeting point of two projected sides of the polygonal board. The estimated vertices from the range data and those detected from the color image serve as the corresponding points for the calibration. Experiments using a low-resolution LIDAR with 32 sensors show robust results. {\textcopyright} 2014 by the authors; licensee MDPI, Basel, Switzerland.},
author = {Park, Yoonsu and Yun, Seokmin and Won, Chee Sun and Cho, Kyungeun and Um, Kyhyun and Sim, Sungdae},
doi = {10.3390/s140305333},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Park et al/Calibration between color camera and 3D LIDAR instruments with a polygonal planar board. Park et al.. 2014.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {3D LIDAR,3D point clouds,Calibration board,Calibration matrix,Camera calibration,Sensor fusion},
number = {3},
pages = {5333--5353},
title = {{Calibration between color camera and 3D LIDAR instruments with a polygonal planar board}},
volume = {14},
year = {2014}
}
@article{Fu2019,
abstract = {LiDAR-camera calibration is a precondition for many heterogeneous systems that fuse data from LiDAR and camera. However, the constraint from the common field of view and the requirement for strict time synchronization make the calibration a challenging problem. In this paper, we propose a hybrid LiDAR-camera calibration method aiming to solve these two difficulties. The configuration between LiDAR and camera is free from their common field of view as we move the camera to cover the scenario observed by LiDAR. 3D visual reconstruction of the environment can be achieved from the sequential visual images obtained by the moving camera, which later can be aligned with the single 3D laser scan captured when both the scene and the equipment are stationary. Under this design, our method can further get rid of the influence from time synchronization between LiDAR and camera. Moreover, the extended field of view obtained by the moving camera can improve the calibration accuracy. We derive the conditions of minimal observability for our method and discuss the influence on calibration accuracy from different placements of chessboards, which can be utilized as a guideline for designing high-accuracy calibration procedures. We validate our method on both simulation platform and real-world datasets. Experiments show that our method can achieve higher accuracy than other comparable methods.},
archivePrefix = {arXiv},
arxivId = {1903.06141},
author = {Fu, Bo and Wang, Yue and Jiao, Yanmei and Ding, Xiaqing and Tang, Li and Xiong, Rong},
eprint = {1903.06141},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fu et al/Spatiotemporal Decoupling Based LiDAR-Camera Calibration under Arbitrary Configurations. Fu et al.. 2019.pdf:pdf},
pages = {1--12},
title = {{Spatiotemporal Decoupling Based LiDAR-Camera Calibration under Arbitrary Configurations}},
url = {http://arxiv.org/abs/1903.06141},
year = {2019}
}
@misc{Huang1997,
abstract = {The shape, size, and content of MC carbides in a unidirectional Ni-base superalloy have been studied at different solidification rates using material of normal commercial composition (5 ppm nitrogen) and two melts with additions to produce 22 and 30 ppm nitrogen. M(C,N) was found in nitrogendoped specimens. The increased nitrogen content results in a change in carbide morphology from acicular or Chinese-script type to a blocky one. Carbide size increases from 6.85 to 7.25 $\mu$m at 2.5 $\mu$m/s and from 2.31 to 2.81 $\mu$m at 200 $\mu$m/s as the nitrogen content increases from 5 to 22 ppm; carbide size is reduced to 5.2 $\mu$m at 2.5 $\mu$/s and to 2.1 $\mu$m at 200 $\mu$m/s when nitrogen content is further raised to 30 ppm. There is no evident change in carbide content (area percentage) with increase of nitrogen. Scanning electron microscopy (SEM) and electron probe microanalysis revealed characteristic centers in some of the blocky carbides of TiN, as nuclei of MC or M(C,N) formed in the melt. The influences of nitrogen on carbide at the low solidification rate are stronger than those at the higher rate. With the increase of solidification rate, the carbide turns from bar to Chinese-script type, except for the base alloy at 2.5 $\mu$m/s, and the content and size decrease.},
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0608246v3},
author = {Huang, Xuebing and Zhang, Yun and Liu, Yulin and Hu, Zhuangqi},
booktitle = {Metall. Mater. Trans. A Phys. Metall. Mater. Sci.},
doi = {10.1007/s11661-997-0172-9},
eprint = {0608246v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Huang et al/Effect of small amount of nitrogen on carbide characteristics in unidirectional Ni-base superalloy. Huang et al.. 1997.pdf:pdf},
isbn = {0000000154871},
issn = {10735623},
number = {10},
pages = {2143--2147},
pmid = {27935037},
primaryClass = {arXiv:physics},
title = {{Effect of small amount of nitrogen on carbide characteristics in unidirectional Ni-base superalloy}},
volume = {28},
year = {1997}
}
@article{Mousavian2017a,
abstract = {We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors and sub-category detection. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset.},
archivePrefix = {arXiv},
arxivId = {arXiv:1612.00496v2},
author = {Mousavian, Arsalan and Anguelov, Dragomir and Ko{\v{s}}eck{\'{a}}, Jana and Flynn, John},
doi = {10.1109/CVPR.2017.597},
eprint = {arXiv:1612.00496v2},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Mousavian et al/3D bounding box estimation using deep learning and geometry. Mousavian et al.. 2017(2).pdf:pdf},
isbn = {9781538604571},
journal = {Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017},
pages = {5632--5640},
title = {{3D bounding box estimation using deep learning and geometry}},
volume = {2017-Janua},
year = {2017}
}
@article{Qi2018,
abstract = {In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.},
archivePrefix = {arXiv},
arxivId = {arXiv:1711.08488v2},
author = {Qi, Charles R. and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J.},
doi = {10.1109/CVPR.2018.00102},
eprint = {arXiv:1711.08488v2},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Qi et al/Frustum PointNets for 3D Object Detection from RGB-D Data. Qi et al.. 2018.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {918--927},
title = {{Frustum PointNets for 3D Object Detection from RGB-D Data}},
year = {2018}
}
@book{Vision2017,
author = {Vision, Allied and Gmbh, Technologies},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Vision, Gmbh/GigE Features Reference Legal notice Trademarks. Vision, Gmbh. 2017.pdf:pdf},
isbn = {3642867723},
pages = {1--164},
title = {{GigE Features Reference Legal notice Trademarks}},
year = {2017}
}
@article{Lukin2004,
author = {Lukin, A. and Kubasov, D.},
doi = {10.1023/B:PACS.0000049512.71861.eb},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lukin, Kubasov/High-quality algorithm for bayer pattern interpolation. Lukin, Kubasov. 2004.pdf:pdf},
issn = {03617688},
journal = {Program. Comput. Softw.},
number = {6},
pages = {347--358},
title = {{High-quality algorithm for bayer pattern interpolation}},
volume = {30},
year = {2004}
}
@article{Fridman2017,
abstract = {For the foreseeble future, human beings will likely remain an integral part of the driving task, monitoring the AI system as it performs anywhere from just over 0{\%} to just under 100{\%} of the driving. The governing objectives of the MIT Autonomous Vehicle Technology (MIT-AVT) study are to (1) undertake large-scale real-world driving data collection that includes high-definition video to fuel the development of deep learning based internal and external perception systems, (2) gain a holistic understanding of how human beings interact with vehicle automation technology by integrating video data with vehicle state data, driver characteristics, mental models, and self-reported experiences with technology, and (3) identify how technology and other factors related to automation adoption and use can be improved in ways that save lives. In pursuing these objectives, we have instrumented 23 Tesla Model S and Model X vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6 vehicles for both long-term (over a year per driver) and medium term (one month per driver) naturalistic driving data collection. Furthermore, we are continually developing new methods for analysis of the massive-scale dataset collected from the instrumented vehicle fleet. The recorded data streams include IMU, GPS, CAN messages, and high-definition video streams of the driver face, the driver cabin, the forward roadway, and the instrument cluster (on select vehicles). The study is on-going and growing. To date, we have 122 participants, 15,610 days of participation, 511,638 miles, and 7.1 billion video frames. This paper presents the design of the study, the data collection hardware, the processing of the data, and the computer vision algorithms currently being used to extract actionable knowledge from the data.},
archivePrefix = {arXiv},
arxivId = {1711.06976},
author = {Fridman, Lex and Brown, Daniel E. and Glazer, Michael and Angell, William and Dodd, Spencer and Jenik, Benedikt and Terwilliger, Jack and Patsekin, Aleksandr and Kindelsberger, Julia and Ding, Li and Seaman, Sean and Mehler, Alea and Sipperley, Andrew and Pettinato, Anthony and Seppelt, Bobbie and Angell, Linda and Mehler, Bruce and Reimer, Bryan},
eprint = {1711.06976},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fridman et al/MIT Autonomous Vehicle Technology Study Large-Scale Deep Learning Based Analysis of Driver Behavior and Interaction with Automation. Fri.pdf:pdf},
pages = {1--16},
title = {{MIT Autonomous Vehicle Technology Study: Large-Scale Deep Learning Based Analysis of Driver Behavior and Interaction with Automation}},
url = {http://arxiv.org/abs/1711.06976},
year = {2017}
}
@article{Czajewski2017,
abstract = {This paper describes the results of experiments on detection and recognition of 3D objects in RGB-D images provided by the Microsoft Kinect sensor. While the studies focus on single image use, sequences of frames are also considered and evaluated. Observed objects are categorized based on both geometrical and visual cues, but the emphasis is laid on the performance of the point cloud matching method. To this end, a rarely used approach consisting of independent VFH and CRH descriptors matching, followed by ICP and HV algorithms from the Point Cloud Library is applied. Successfully recognized objects are then subjected to a classical 2D analysis based on color histogram comparison exclusively with objects in the same geometrical category. The proposed two-stage approach allows to distinguish objects of similar geometry and different visual appearance, like soda cans of various brands. By separating geometry and color identification phases, the applied system is still able to categorize objects based on their geometry, even if there is no color match. The recognized objects are then localized in the three-dimensional space and autonomously grasped by a manipulator. To evaluate this approach, a special validation set was created, and additionally a selected scene from the Washington RGB-D Object Dataset was used.},
author = {Czajewski, Witold and Ko{\l}omyjec, Krzysztof},
doi = {10.1515/fcds-2017-0011},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Czajewski, Ko{\l}omyjec/3D Object Detection and Recognition for Robotic Grasping Based on RGB-D Images and Global Features. Czajewski, Ko{\l}omyjec. 2017.pdf:pdf},
journal = {Found. Comput. Decis. Sci.},
keywords = {3d object detection and,crh,icp,images,kinect,point cloud analysis,recognition,rgb-d,vfh},
number = {3},
pages = {219--237},
title = {{3D Object Detection and Recognition for Robotic Grasping Based on RGB-D Images and Global Features}},
volume = {42},
year = {2017}
}
@article{Fremont2012,
abstract = {This article presents a novel approach for solving the problem of 3D alignment between video and lidar sensors. The proposed method concerns intelligent vehicle applications, where the relative distance between sensor frames can be significant. Circular calibration targets are used in order to make full use of the perception properties of both lidar and video cameras, which greatly simplifies the calibration task. The method determines the relative pose in rotation and translation of the sensors, using sets of corresponding circular features acquired for several configurations of the targets. A performance analysis in simulation and an error propagation analysis are carried out. The calibration procedure is tested on different configurations, and the calibration accuracy and estimation of confidence intervals are evaluated on real data. {\textcopyright} 2012 Copyright Taylor {\&} Francis and The Robotics Society of Japan.},
author = {Fremont, Vincent and {Rodriguez F.}, Sergio A. and Bonnifait, Philippe},
doi = {10.1080/01691864.2012.703235},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fremont, Rodriguez F., Bonnifait/Circular targets for 3D alignment of video and lidar sensors. Fremont, Rodriguez F., Bonnifait. 2012.pdf:pdf},
issn = {01691864},
journal = {Adv. Robot.},
keywords = {circular targets,extrinsic calibration,intelligent vehicles,lidar,vision sensor},
number = {18},
pages = {2087--2113},
title = {{Circular targets for 3D alignment of video and lidar sensors}},
volume = {26},
year = {2012}
}
@article{Chai2018,
abstract = {{\textcopyright} 2018 IEEE. With the development of sensor technology, many different kinds of sensors are now utilized in various application fields. Sensors can make machines smarter and multiple-sensor systems can acquire more information to work more stably. When multiple sensors are integrated into one system, calibration, either in time domain or space domain, is very important to merge data from different sensors. In this paper, we propose a novel method to calibrate a LiDAR and a camera using 3d-3d corresponding features using a cube with ArUco Markers. In the LiDAR frame, point data on the three surfaces are selected to fit the plane's equation independently. In this way, the vertex's coordinate in 3d space and the normal vector of each plane can be obtained. In the camera frame, the corresponding point's coordinates and normal vectors of each plane can be obtained by the camera's full 6d pose estimated using ArUco Markers. In this way, we get a set of point cloud per sensor using the data above. Then a rigid body transformation can be computed by Kabsch algorithm[1]. Experiments show that our method can obtain more stable calibration results than the existing method (Ankit's method) without loss of precision.},
author = {Chai, Ziqi and Sun, Yuxin and Xiong, Zhenhua},
doi = {10.1109/AIM.2018.8452339},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Chai, Sun, Xiong/A novel method for LiDAR camera calibration by plane fitting. Chai, Sun, Xiong. 2018.pdf:pdf},
isbn = {9781538618547},
journal = {IEEE/ASME Int. Conf. Adv. Intell. Mechatronics, AIM},
keywords = {3d data,camera,extrinsic calibration,lidar},
number = {April},
pages = {286--291},
publisher = {IEEE},
title = {{A novel method for LiDAR camera calibration by plane fitting}},
volume = {2018-July},
year = {2018}
}
@article{Roca2006a,
archivePrefix = {arXiv},
arxivId = {arXiv:1208.5721},
author = {Roca, Sergi Figueras and Cited, References},
doi = {10.1038/incomms1464},
eprint = {arXiv:1208.5721},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Roca, Cited/( 12 ) United States Patent. Roca, Cited. 2006.pdf:pdf},
isbn = {1020080012051},
issn = {2004001828},
number = {12},
pmid = {1000182772},
title = {{( 12 ) United States Patent}},
volume = {2},
year = {2006}
}
@book{Martinez2015,
abstract = {A practival, instructive, and comprehensive guide to introduce yourself to ROS, the top-notch, leading robotics framework.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Martinez, Aaron and Fern{\'{a}}ndez, Enrique},
booktitle = {Book},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Martinez, Fern{\'{a}}ndez/Learning ROS for Robotics Programming Second Edition. Martinez, Fern{\'{a}}ndez. 2015.pdf:pdf},
isbn = {9781782161448},
issn = {1464-3553},
pages = {458},
pmid = {25128492},
title = {{Learning ROS for Robotics Programming Second Edition}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=2ZL9AAAAQBAJ{\&}oi=fnd{\&}pg=PT12{\&}dq=Learning+ROS+for+Robotics+Programming{\&}ots=VJMhUZ{\_}xwN{\&}sig=N0nBv1htLn3BuBwYb0cP1pZmBJ8},
year = {2015}
}
@article{Xu2018,
author = {Xu, Y},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Xu/GENERATION OF GROUND TRUTH DATASETS FOR THE ANALYSIS OF 3D POINT CLOUDS IN URBAN SCENES ACQUIRED VIA DIFFERENT SENSORS. Xu. 2018.pdf:pdf},
keywords = {3d space labeling,different sensors,multi-resolution voxel structure,point clouds},
pages = {7--10},
title = {{GENERATION OF GROUND TRUTH DATASETS FOR THE ANALYSIS OF 3D POINT CLOUDS IN URBAN SCENES ACQUIRED VIA DIFFERENT SENSORS}},
volume = {XLII},
year = {2018}
}
@article{DelaEscalera2010,
abstract = {There are increasing applications that require precise calibration of cameras to perform accurate measurements on objects located within images, and an automatic algorithm would reduce this time consuming calibration procedure. The method proposed in this article uses a pattern similar to that of a chess board, which is found automatically in each image, when no information regarding the number of rows or columns is supplied to aid its detection. This is carried out by means of a combined analysis of two Hough transforms, image corners and invariant properties of the perspective transformation. Comparative analysis with more commonly used algorithms. demonstrate the viability of the algorithm proposed, as a valuable tool for camera calibration. {\textcopyright} 2010 by the authors.},
author = {de la Escalera, Arturo and Armingol, Jos{\'{e}} Mar{\'{i}}a},
doi = {10.3390/s100302027},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/de la Escalera, Armingol/Automatic chessboard detection for intrinsic and extrinsic camera parameter calibration. de la Escalera, Armingol. 2010.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {Camera calibration,Chessboard detection,Double Hough transform,Pattern recognition},
number = {3},
pages = {2027--2044},
title = {{Automatic chessboard detection for intrinsic and extrinsic camera parameter calibration}},
volume = {10},
year = {2010}
}
@misc{Cameron,
abstract = {Retrofit a Car for self-driving using LIDAR, camera and Radar},
author = {Cameron, Oliver and Voyage},
keywords = {LIDAR,Self-Driving Cars,Sensor Fusion},
mendeley-tags = {LIDAR,Self-Driving Cars,Sensor Fusion},
title = {{The Story of Homer: Voyage's First Self-Driving Taxi}},
url = {https://news.voyage.auto/the-story-of-homer-voyages-first-self-driving-taxi-f0a6466718af},
urldate = {2018-10-09}
}
@article{Wei2018,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@article{Takaya2016a,
abstract = {{\textcopyright} 2016 IEEE. In the process of development a control strategy for mobile robots, simulation is important for testing the software components, robot behavior and control algorithms in different surrounding environments. In this paper we introduce a simulation environment for mobile robots based on ROS and Gazebo. We show that after properly creating the robot models under Gazebo, the code developed for the simulation process can be directly implemented in the real robot without modifications. In this paper autonomous navigation tasks and 3D-mapping simulation using control programs under ROS are presented. Both the simulation and experimental results agree very well and show the usability of the developed environment.},
author = {Takaya, Kenta and Asai, Toshinori and Kroumov, Valeri and Smarandache, Florentin},
doi = {10.1109/ICSTCC.2016.7790647},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Takaya et al/Simulation environment for mobile robots testing using ROS and Gazebo. Takaya et al.. 2016.pdf:pdf},
isbn = {9781509027200},
journal = {2016 20th Int. Conf. Syst. Theory, Control Comput. ICSTCC 2016 - Jt. Conf. SINTES 20, SACCS 16, SIMSIS 20 - Proc.},
keywords = {3D mapping,Gazebo,Mobile robot simulation,ROS},
pages = {96--101},
title = {{Simulation environment for mobile robots testing using ROS and Gazebo}},
year = {2016}
}
@article{Peng2014,
abstract = {{\textcopyright} 2014 IEEE. Object registration has been widely discussed with the development of various range sensing technologies. In most work, however, the point clouds of reference and target are generated by the same technology, such as a Kinect range camera, LiDAR sensor, or Structure from Motion technique. Cases in which reference and target point clouds are generated by different technologies are rarely discussed. Due to the significant differences across various point cloud data in terms of point cloud density, sensing noise, scale, occlusion etc., object registration between such different point clouds becomes extremely difficult. In this study, we address for the first time an even more challenging case in which the differently-sourced point clouds are acquired from a real street view. One is generated on the basis of an image sequence through the SfM process, and the other is produced directly by the Li-DAR system. We propose a two-stage matching and registration algorithm to achieve object registration between these two different point clouds. The experiments are based on real building object point cloud data and demonstrate the effectiveness and efficiency of the proposed solution. The newly proposed solution can be further developed to contribute to several related applications, such as Location Based Service.},
author = {Peng, Furong and Wu, Qiang and Fan, Lixin and Zhang, Jian and You, Yu and Lu, Jianfeng and Yang, Jing Yu},
doi = {10.1109/ICIP.2014.7025406},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Street view cross-sourced point cloud matching and registration. Peng et al.pdf:pdf},
isbn = {9781479957514},
journal = {2014 IEEE Int. Conf. Image Process. ICIP 2014},
keywords = {Cross-source,LiDAR,Point clouds,Registration,SfM},
pages = {2026--2030},
publisher = {IEEE},
title = {{Street view cross-sourced point cloud matching and registration}},
year = {2014}
}
@article{Kim2019,
author = {Kim, Ji Hoon and Kim, Changhyeon and Kim, Kwantae and Yoo, Hoi Jun},
doi = {10.1109/ISCAS.2019.8702698},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/An ultra-low-power analog-digital hybrid CNN face recognition processor integrated with a ciS for always-on mobile devices. Kim et al...pdf:pdf},
isbn = {9781728103976},
issn = {02714310},
journal = {Proc. - IEEE Int. Symp. Circuits Syst.},
keywords = {Always-on,Analog-digital hybrid,CMOS image sensor,CNN,Face detection,Face recognition,Mixed-mode},
pages = {1--5},
title = {{An ultra-low-power analog-digital hybrid CNN face recognition processor integrated with a ciS for always-on mobile devices}},
volume = {2019-May},
year = {2019}
}
@article{Fischler2002,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/ smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing and analysis conditions. Implementation details and computational examples are also presented.},
archivePrefix = {arXiv},
arxivId = {3629719},
author = {Fischler, Martin A. and Bolles, Robert C.},
doi = {10.1145/358669.358692},
eprint = {3629719},
isbn = {0934613338},
issn = {00010782},
journal = {Commun. ACM},
month = {jun},
number = {6},
pages = {381--395},
pmid = {4650729},
title = {{Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography}},
url = {http://portal.acm.org/citation.cfm?doid=358669.358692},
volume = {24},
year = {2002}
}
@article{Almeida2012,
abstract = {The AtlasCar is a prototype that is being developed at the University of Aveiro to research advanced driver assistance systems. The car is equipped with several sensors: 3D and 2D laser scanners, a stereo camera, inertial sensors and GPS. The combination of all these sensor data in useful representations is essential. Therefore, calibration is one of the first problems to tackle. This paper focuses on 3D/2D laser calibration. The proposed method uses a 3D Laser Range Finder (LRF) to produce a reference 3D point cloud containing a known calibration object. Manual input from the user and knowledge of the object geometry are used to register the 3D point cloud with the 2D Lasers. Experimental results with simulated and real data demonstrate the effectiveness of the proposed calibration method. {\textcopyright} 2012 Springer-Verlag.},
author = {Almeida, Miguel and Dias, Paulo and Oliveira, Miguel and Santos, V{\'{i}}tor},
doi = {10.1007/978-3-642-31295-3_37},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Almeida et al/3D-2D laser range finder calibration using a conic based geometry shape. Almeida et al.. 2012.pdf:pdf},
isbn = {9783642312946},
issn = {03029743},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
keywords = {laser range finder,laser to laser calibration,sensor fusion},
number = {PART 1},
pages = {312--319},
title = {{3D-2D laser range finder calibration using a conic based geometry shape}},
volume = {7324 LNCS},
year = {2012}
}
@article{Le2016,
abstract = {{\textcopyright} 2016, Institute of Control, Robotics and Systems and The Korean Institute of Electrical Engineers and Springer-Verlag Berlin Heidelberg. In this paper, a method for the automated reconstruction of architectures from two views of a monocular camera is proposed. While this research topic has been studied over the last few decades, we contend that a satisfactory approach has not yet been devised. Here, a new method to solve the same problem with several points of novelty is proposed. First, reference planes are automatically detected using color, straight lines, and edge/vanishing points. This approach is quite robust and fast even when different views and complicated conditions are presented. Second, the camera pose and 3D points are accurately estimated by a two-view geometry constraint in the convex optimization approach. It has been demonstrated that camera rotations are appropriately estimated, while translations induce a significant error in short baseline images. To overcome this problem, we rely only on reference planes to estimate image homography instead of using the conventional camera pose estimation method. Thus, the problem associated with short baseline images is adequately addressed. The 3D points and translation are then simultaneously triangulated. Furthermore, both the homography and 3D point triangulation are computed via the convex optimization approach. The error of back-projection and measured points is minimized in L∞-norm so as to overcome the local minima problem of the canonical L2-norm method. Consequently, extremely accurate homography and point clouds can be achieved with this scheme. In addition, a robust plane fitting method is introduced to describe a scene. The corners are considered as properties of the plane in order to limit the boundary. Thus, it is necessary to find the exact corresponding corner positions by searching along the epipolar line in the second view. Finally, the texture of faces is mapped from 2D images to a 3D plane. The simulation results demonstrate the effectiveness of the proposed method for scenic images in an outdoor environment.},
author = {Le, My Ha and Trinh, Hoang Hon and Hoang, Van Dung and Jo, Kang Hyun},
doi = {10.1007/s12555-014-0203-4},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Le et al/Automated architectural reconstruction using reference planes under convex optimization. Le et al.. 2016.pdf:pdf},
issn = {20054092},
journal = {Int. J. Control. Autom. Syst.},
keywords = {3D reconstruction,convex optimization,correspondence,plane fitting,plane homography,planes detection,sum of square error differences,two-view geometry},
number = {3},
pages = {814--826},
title = {{Automated architectural reconstruction using reference planes under convex optimization}},
volume = {14},
year = {2016}
}
@article{Liu2017,
abstract = {The time delay calibration between Light Detection and Ranging (LiDAR) and Inertial Measurement Units (IMUs) is an essential prerequisite for its applications. However, the correspondences between LiDAR and IMU measurements are usually unknown, and thus cannot be computed directly for the time delay calibration. In order to solve the problem of LiDAR-IMU time delay calibration, this paper presents a fusion method based on iterative closest point (ICP) and iterated sigma point Kalman filter (ISPKF), which combines the advantages of ICP and ISPKF. The ICP algorithm can precisely determine the unknown transformation between LiDAR-IMU; and the ISPKF algorithm can optimally estimate the time delay calibration parameters. First of all, the coordinate transformation from the LiDAR frame to the IMU frame is realized. Second, the measurement model and time delay error model of LiDAR and IMU are established. Third, the methodology of the ICP and ISPKF procedure is presented for LiDAR-IMU time delay calibration. Experimental results are presented that validate the proposed method and demonstrate the time delay error can be accurately calibrated.},
author = {Liu, Wanli},
doi = {10.3390/s17030539},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Liu/Lidar-imu time delay calibration based on iterative closest point and iterated sigma point kalman filter. Liu. 2017.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Inertial measurement unit,Iterated sigma point Kalman filter,Iterative closest point,LiDAR,Time delay calibration},
number = {3},
title = {{Lidar-imu time delay calibration based on iterative closest point and iterated sigma point kalman filter}},
volume = {17},
year = {2017}
}
@article{Mousavian2017,
abstract = {We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors and sub-category detection. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset.},
archivePrefix = {arXiv},
arxivId = {arXiv:1612.00496v2},
author = {Mousavian, Arsalan and Anguelov, Dragomir and Ko{\v{s}}eck{\'{a}}, Jana and Flynn, John},
doi = {10.1109/CVPR.2017.597},
eprint = {arXiv:1612.00496v2},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Mousavian et al/3D bounding box estimation using deep learning and geometry. Mousavian et al.. 2017.pdf:pdf},
isbn = {9781538604571},
journal = {Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017},
number = {April 2017},
pages = {5632--5640},
title = {{3D bounding box estimation using deep learning and geometry}},
volume = {2017-Janua},
year = {2017}
}
@article{Lyu2019,
abstract = {Recent progress in the automated driving system (ADS) and advanced driver assistant system (ADAS) has shown that the combined use of 3D light detection and ranging (LiDAR) and the camera is essential for an intelligent vehicle to perceive and understand its surroundings. LiDAR-camera fusion requires precise intrinsic and extrinsic calibrations between the sensors. However, due to the limitation of the calibration equipment and susceptibility to noise, algorithms in existing methods tend to fail in finding LiDAR-camera correspondences in long-range. In this paper, we introduced an interactive LiDAR to camera calibration toolbox to estimate the intrinsic and extrinsic transforms. This toolbox automatically detects the corner of a planer board from a sequence of LiDAR frames and provides a convenient user interface for annotating the corresponding pixels on camera frames. Since the toolbox only detects the top corner of the board, there is no need to prepare a precise polygon planar board or a checkerboard with different reflectivity areas as in the existing methods. Furthermore, the toolbox uses genetic algorithms to estimate the transforms and supports multiple camera models such as the pinhole camera model and the fisheye camera model. Experiments using Velodyne VLP-16 LiDAR and Point Grey Chameleon 3 camera show robust results.},
archivePrefix = {arXiv},
arxivId = {1903.02122},
author = {Lyu, Yecheng and Bai, Lin and Elhousni, Mahdi and Huang, Xinming},
eprint = {1903.02122},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lyu et al/An Interactive LiDAR to Camera Calibration. Lyu et al.. 2019.pdf:pdf},
title = {{An Interactive LiDAR to Camera Calibration}},
url = {http://arxiv.org/abs/1903.02122},
year = {2019}
}
@article{Bileschi2009,
author = {Bileschi, Stanley},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fully automatic calibration of LIDAR and video streams from a vehicle The MIT Faculty has made this article openly available . Please.pdf:pdf},
isbn = {9781424444410},
journal = {Comput. Vis. Work. (ICCV Work.},
pages = {1457--1464},
title = {{Fully automatic calibration of LIDAR and video streams from a vehicle The MIT Faculty has made this article openly available . Please share Bileschi , S . “ Fully automatic calibration of LIDAR and video Publisher Version Accessed Citable Link Terms of Us}},
year = {2009}
}
@book{AlliedVisionTechnologiesGmbH2013,
author = {{Allied Vision Technologies GmbH}},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Allied Vision Technologies GmbH/AVT Manta Technical Manual. Allied Vision Technologies GmbH. 2013.pdf:pdf},
isbn = {6048758855},
number = {September},
pages = {1--138},
title = {{AVT Manta Technical Manual}},
year = {2013}
}
@article{Jeong2019,
abstract = {This paper presents a framework for the targetless extrinsic calibration of stereo cameras and Light Detection and Ranging (LiDAR) sensors with a non-overlapping Field of View (FOV). In order to solve the extrinsic calibrations problem under such challenging configuration, the proposed solution exploits road markings as static and robust features among the various dynamic objects that are present in urban environment. First, this study utilizes road markings that are commonly captured by the two sensor modalities to select informative images for estimating the extrinsic parameters. In order to accomplish stable optimization, multiple cost functions are defined, including Normalized Information Distance (NID), edge alignment and, plane fitting cost. Therefore a smooth cost curve is formed for global optimization to prevent convergence to the local optimal point. We further evaluate each cost function by examining parameter sensitivity near the optimal point. Another key characteristic of extrinsic calibration, repeatability, is analyzed by conducting the proposed method multiple times with varying randomly perturbed initial points.},
archivePrefix = {arXiv},
arxivId = {1902.10586},
author = {Jeong, Jinyong and Cho, Lucas Y. and Kim, Ayoung},
eprint = {1902.10586},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Jeong, Cho, Kim/Road is Enough! Extrinsic Calibration of Non-overlapping Stereo Camera and LiDAR using Road Information. Jeong, Cho, Kim. 2019.pdf:pdf},
title = {{Road is Enough! Extrinsic Calibration of Non-overlapping Stereo Camera and LiDAR using Road Information}},
url = {http://arxiv.org/abs/1902.10586},
year = {2019}
}
@article{Gmbh2018a,
author = {Gmbh, Allied Vision},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gmbh/Modular Concept V10.0.0. Gmbh. 2018.pdf:pdf},
keywords = {Additional options for camera mounts, housing desi},
pages = {1--31},
title = {{Modular Concept V10.0.0}},
year = {2018}
}
@article{Larusso2013,
abstract = {Abstract: "A common need in machine vision is to compute the 3-D rigid transformation that exists between two sets of points in a coordinate system for which corresponding pairs have been determined. Several solutions to this problem have been devised. In this paper a comparative analysis of four popular algorithms is given. Each of them computes the translation and rotation of the transform in closed-form, as the solution to a least squares formulation of the problem. They differ in terms of the representation of the transform and the method of solution, using respectively: singular value decomposition of a matrix, orthonormal matrices, unit quaternions and dual quaternions. This comparison presents results of several experiments designed to determine (1) the accuracy of each algorithm in the presence of different levels of noise, (2) the stability of each technique with respect to degenerate data sets, and (3) the relative computation time of each approach for different sizes and forms of data."},
author = {Larusso, A and Eggert, DW and Fisher, RB},
doi = {10.5244/c.9.24},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Larusso, Eggert, Fisher/A Comparison of Four Algorithms for Estimating 3-D Rigid Transformations.. Larusso, Eggert, Fisher. 2013.pdf:pdf},
number = {January},
pages = {24.1--24.10},
title = {{A Comparison of Four Algorithms for Estimating 3-D Rigid Transformations.}},
year = {2013}
}
@article{Guindel2018a,
abstract = {Sensor setups consisting of a combination of 3D range scanner lasers and stereo vision systems are becoming a popular choice for on-board perception systems in vehicles; however, the combined use of both sources of information implies a tedious calibration process. We present a method for extrinsic calibration of lidar-stereo camera pairs without user intervention. Our calibration approach is aimed to cope with the constraints commonly found in automotive setups, such as low-resolution and specific sensor poses. To demonstrate the performance of our method, we also introduce a novel approach for the quantitative assessment of the calibration results, based on a simulation environment. Tests using real devices have been conducted as well, proving the usability of the system and the improvement over the existing approaches. Code is available at http://wiki.ros.org/velo2cam{\_}calibration},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.04085v3},
author = {Guindel, Carlos and Beltran, Jorge and Martin, David and Garcia, Fernando},
doi = {10.1109/ITSC.2017.8317829},
eprint = {arXiv:1705.04085v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Guindel et al/Automatic extrinsic calibration for lidar-stereo vehicle sensor setups. Guindel et al.. 2018(2).pdf:pdf},
isbn = {9781538615256},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
number = {October},
pages = {1--6},
title = {{Automatic extrinsic calibration for lidar-stereo vehicle sensor setups}},
volume = {2018-March},
year = {2018}
}
@article{Ning2018,
abstract = {Outlier removal is a fundamental data processing task to ensure the quality of scanned point cloud data (PCD), which is becoming increasing important in industrial applications and reverse engineering. Acquired scanned PCD is usually noisy, sparse and temporarily incoherent. Thus the processing of scanned data is typically an ill-posed problem. In the paper, we present a simple and effective method based on two geometrical characteristics constraints to trim the noisy points. One of the geometrical characteristics is the local density information and another is the deviation from the local fitting plane. The local density based method provides a preprocessing step, which could remove those sparse outlier and isolated outlier. The non-isolated outlier removal in this paper depends on a local projection method, which placing those points onto objects. There is no doubt that the deviation of any point from the local fitting plane should be a criterion to reduce the noisy points. The experimental results demonstrate the ability to remove the noisy point from various man-made objects consisting of complex outlier.},
author = {Ning, Xiaojuan and Li, Fan and Tian, Ge and Wang, Yinghui},
doi = {10.1371/journal.pone.0201280},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ning et al/An efficient outlier removal method for scattered point cloud data. Ning et al.. 2018.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS One},
number = {8},
pages = {1--22},
title = {{An efficient outlier removal method for scattered point cloud data}},
volume = {13},
year = {2018}
}
@article{Geiger2012a,
abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera- to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experi- ments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions. I.},
author = {Geiger, Andreas and Moosmann, Frank and Car, {\"{O}}mer and Schuster, Bernhard},
doi = {10.1109/ICRA.2012.6224570},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger et al/Automatic camera and range sensor calibration using a single shot. Geiger et al.. 2012.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {3936--3943},
title = {{Automatic camera and range sensor calibration using a single shot}},
year = {2012}
}
@article{Eggert1997,
abstract = {A common need in machine vision is to com-pute the 3-D rigid body transformation that aligns two sets of points for which correspondence is known. A compara-tive analysis is presented here of four popular and efficient algorithms, each of which computes the translational and ro-tational components of the transform in closed form, as the solution to a least squares formulation of the problem. They differ in terms of the transformation representation used and the mathematical derivation of the solution, using respec-tively singular value decomposition or eigensystem compu-tation based on the standard [R, T] representation, and the eigensystem analysis of matrices derived from unit and dual quaternion forms of the transform. This comparison presents both qualitative and quantitative results of several experi-ments designed to determine (1) the accuracy and robust-ness of each algorithm in the presence of different levels of noise, (2) the stability with respect to degenerate data sets, and (3) relative computation time of each approach under different conditions. The results indicate that under " ideal " data conditions (no noise) certain distinctions in accuracy and stability can be seen. But for " typical, real-world " noise levels, there is no difference in the robustness of the final solutions (contrary to certain previously published results). Efficiency, in terms of execution time, is found to be highly dependent on the computer system setup.},
author = {Eggert, D. W. and Lorusso, A. and Fisher, R. B.},
doi = {10.1007/s001380050048},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Eggert, Lorusso, Fisher/Estimating 3-D rigid body transformations A comparison of four major algorithms. Eggert, Lorusso, Fisher. 1997.pdf:pdf},
issn = {09328092},
journal = {Mach. Vis. Appl.},
keywords = {3-D rigid transformations,Motion analysis,Pose estimation},
number = {5-6},
pages = {272--290},
title = {{Estimating 3-D rigid body transformations: A comparison of four major algorithms}},
volume = {9},
year = {1997}
}
@article{Simon2019,
abstract = {Accurate detection of 3D objects is a fundamental problem in computer vision and has an enormous impact on autonomous cars, augmented/virtual reality and many applications in robotics. In this work we present a novel fusion of neural network based state-of-the-art 3D detector and visual semantic segmentation in the context of autonomous driving. Additionally, we introduce Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable evaluation metric for comparison of object detections, which speeds up our inference time up to 20$\backslash${\%} and halves training time. On top, we apply state-of-the-art online multi target feature tracking on the object measurements to further increase accuracy and robustness utilizing temporal information. Our experiments on KITTI show that we achieve same results as state-of-the-art in all related categories, while maintaining the performance and accuracy trade-off and still run in real-time. Furthermore, our model is the first one that fuses visual semantic with 3D object detection.},
archivePrefix = {arXiv},
arxivId = {1904.07537},
author = {Simon, Martin and Amende, Karl and Kraus, Andrea and Honer, Jens and S{\"{a}}mann, Timo and Kaulbersch, Hauke and Milz, Stefan and Gross, Horst Michael},
eprint = {1904.07537},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Simon et al/Complexer-YOLO Real-Time 3D Object Detection and Tracking on Semantic Point Clouds. Simon et al.. 2019.pdf:pdf},
title = {{Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds}},
url = {http://arxiv.org/abs/1904.07537},
year = {2019}
}
@article{Gao2003,
abstract = {We use two approaches to solve the perspective-three-point (P3P) problem: the algebraic approach and the geometric approach. In the algebraic approach, we use Wu-Ritt's zero decomposition algorithm to give a complete triangular decomposition for the P3P equation system. This decomposition provides the first complete analytical solution to the P3P problem. We also give a complete solution classification for the P3P equation system, i.e., we give explicit criteria for the P3P problem to have one, two, three, and four solutions. Combining the analytical solutions with the criteria, we provide an algorithm, CASSC, which may be used to find complete and robust numerical solutions to the P3P problem. In the geometric approach, we give some pure geometric criteria for the number of real physical solutions.},
author = {Gao, Xiao Shan and Hou, Xiao Rong and Tang, Jianliang and Cheng, Hang Fei},
doi = {10.1109/TPAMI.2003.1217599},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gao et al/Complete solution classification for the perspective-three-point problem. Gao et al.. 2003.pdf:pdf},
issn = {01628828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
keywords = {Analytical solutions,Geometric criteria,Perspective-Three-Point problem,Pose determination,Solution classification,Wu-Ritt's zero decomposition method},
number = {8},
pages = {930--943},
title = {{Complete solution classification for the perspective-three-point problem}},
volume = {25},
year = {2003}
}
@article{Huang2009,
abstract = {In this research, we propose a unique multi-planar LIDAR and computer vision calibration algorithm. This method only requires the camera and LIDAR to observe a planar pattern at different positions and orientations. Geometric constraints of the dasiaviewspsila from the LIDAR and camera images are resolved as the coordinate transformation coefficients. The proposed approach consists of two stages: solving a closed-form equation, followed by applying a non-linear algorithm based on a maximum likelihood criterion. To the author's best knowledge, this is the first paper for a multi-planar LIDAR and vision system calibration. Compared with the classical methods which use dasiabeam-visiblepsila cameras or 3D LIDAR systems, this approach is easy to implement at low cost. Additionally, computer simulation and real world testing have been carried out to evaluate the performance of this approach. Lastly, application of the technique for automated navigation is presented.},
author = {Huang, Lili and Barth, Matthew},
doi = {10.1109/IVS.2009.5164263},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Huang, Barth/A novel multi-planar LIDAR and computer vision calibration procedure using 2D patterns for automated navigation. Huang, Barth. 2009.pdf:pdf},
isbn = {9781424435043},
journal = {IEEE Intell. Veh. Symp. Proc.},
number = {July 2009},
pages = {117--122},
title = {{A novel multi-planar LIDAR and computer vision calibration procedure using 2D patterns for automated navigation}},
year = {2009}
}
@article{Chen2017,
abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25{\%} and 30{\%} AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3{\%} higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.07759v3},
author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
doi = {10.1109/CVPR.2017.691},
eprint = {arXiv:1611.07759v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Chen et al/Multi-view 3D object detection network for autonomous driving. Chen et al.. 2017.pdf:pdf},
isbn = {9781538604571},
journal = {Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017},
pages = {6526--6534},
title = {{Multi-view 3D object detection network for autonomous driving}},
volume = {2017-Janua},
year = {2017}
}
@article{Roca2006,
archivePrefix = {arXiv},
arxivId = {arXiv:1208.5721},
author = {Roca, Sergi Figueras and Cited, References},
doi = {10.1038/incomms1464},
eprint = {arXiv:1208.5721},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Roca, Cited/( 12 ) United States Patent. Roca, Cited. 2006.pdf:pdf},
isbn = {1020080012051},
issn = {2004001828},
number = {12},
pmid = {1000182772},
title = {{( 12 ) United States Patent}},
volume = {2},
year = {2006}
}
@inproceedings{Yue2018,
abstract = {3D LiDAR scanners are playing an increasingly important role in autonomous driving as they can generate depth information of the environment. However, creating large 3D LiDAR point cloud datasets with point-level labels requires a significant amount of manual annotation. This jeopardizes the efficient development of supervised deep learning algorithms which are often data-hungry. We present a framework to rapidly create point clouds with accurate point-level labels from a computer game. The framework supports data collection from both auto-driving scenes and user-configured scenes. Point clouds from auto-driving scenes can be used as training data for deep learning algorithms, while point clouds from user-configured scenes can be used to systematically test the vulnerability of a neural network, and use the falsifying examples to make the neural network more robust through retraining. In addition, the scene images can be captured simultaneously in order for sensor fusion tasks, with a method proposed to do automatic calibration between the point clouds and captured scene images. We show a significant improvement in accuracy (+9{\%}) in point cloud segmentation by augmenting the training dataset with the generated synthesized data. Our experiments also show by testing and retraining the network using point clouds from user-configured scenes, the weakness/blind spots of the neural network can be fixed.},
address = {New York, New York, USA},
annote = {- Labeling point clouds is expensive, because most of the work is manual
- Automatic annotation is performed by ray casting, returning the information of the object that was hitted and uses that information to perform the labelling
- KITTI + Synthetic GTA Dataset is better than KITTI alone},
archivePrefix = {arXiv},
arxivId = {1804.00103},
author = {Yue, Xiangyu and Wu, Bichen and Seshia, Sanjit A. and Keutzer, Kurt and Sangiovanni-Vincentelli, Alberto L.},
booktitle = {Proc. 2018 ACM Int. Conf. Multimed. Retr. - ICMR '18},
doi = {10.1145/3206025.3206080},
eprint = {1804.00103},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A LiDAR Point Cloud Generator. Yue et al.pdf:pdf},
isbn = {9781450350464},
month = {mar},
number = {Nips},
pages = {458--464},
publisher = {ACM Press},
title = {{A LiDAR Point Cloud Generator}},
url = {http://dl.acm.org/citation.cfm?doid=3206025.3206080 http://arxiv.org/abs/1804.00103},
year = {2018}
}
@article{Yang2017,
abstract = {Combining active and passive imaging sensors enables creating a more detailed 3D model of the real world. Then, these 3D data can be used for various applications, such as city mapping, indoor navigation, autonomous vehicles, etc. Typically, LiDAR and camera as imaging sensors are installed on these systems. Both of these sensors have advantages and drawbacks. Thus, LiDAR sensor directly provides relatively accurate 3D point cloud, but LiDAR point cloud barely contains the surface textures and details, such as traffic signs and alpha numeric information on facades. As opposed to LiDAR, deriving 3D point cloud from images require more computational resources, and in many cases, the accuracy and point density might be lower due to poor visual or light conditions. This paper investigates a workflow which utilizes factor graph SLAM, dense 3D reconstruction and ICP to efficiently generate the LiDAR and camera point clouds, and then, co-register in a navigation frame to provide a consistent and more detailed reconstruction of the environment. The workflow consists of three processing steps. First, we use factor graph SLAM, GPS/INS odometry and 6DOF scan matching to register the LiDAR point cloud. Then, the stereo images are processed by stereo-scan dense 3D reconstruction technique to generate dense point cloud. Finally, ICP method is used to co-register LiDAR and photogrammetric point clouds into one frame. The proposed method is tested with the KITTI dataset. The results show that data fusion of two point clouds can improve the quality of the 3D model.},
author = {Yang, Yuan and Koppanyi, Zoltan and Toth, Charles K},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yang, Koppanyi, Toth/Stereo Image Point Cloud and Lidar Point Cloud Fusion for the 3D Street Mapping. Yang, Koppanyi, Toth. 2017.pdf:pdf},
journal = {IGTF 2017 – Imaging Geospatial Technol. Forum 2017-ASPRS Annu. Conf.},
keywords = {ICP,LiDAR point cloud,data fusion,factor graph,stereo image 3D reconstruction},
title = {{Stereo Image Point Cloud and Lidar Point Cloud Fusion for the 3D Street Mapping}},
url = {https://pdfs.semanticscholar.org/7c2e/a37a24b6a09266dd75936b5e4d53953d1179.pdf},
year = {2017}
}
@article{Hornung2013,
abstract = {Three-dimensional models provide a volumetric representation of space which is important for a variety of robotic applications including flying robots and robots that are equipped with manipulators. In this paper, we present an open-source framework to generate volumetric 3D environment models. Our mapping approach is based on octrees and uses probabilistic occupancy estimation. It explicitly represents not only occupied space, but also free and unknown areas. Furthermore, we propose an octree map compression method that keeps the 3D models compact. Our framework is available as an open-source C++ library and has already been successfully applied in several robotics projects. We present a series of experimental results carried out with real robots and on publicly available real-world datasets. The results demonstrate that our approach is able to update the representation efficiently and models the data consistently while keeping the memory requirement at a minimum. ? 2013 Springer Science+Business Media New York.},
author = {Hornung, Armin and Wurm, Kai M. and Bennewitz, Maren and Stachniss, Cyrill and Burgard, Wolfram},
doi = {10.1007/s10514-012-9321-0},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hornung et al/OctoMap An efficient probabilistic 3D mapping framework based on octrees. Hornung et al.. 2013.pdf:pdf},
issn = {09295593},
journal = {Auton. Robots},
keywords = {3D,Mapping,Navigation,Probabilistic},
number = {3},
pages = {189--206},
title = {{OctoMap: An efficient probabilistic 3D mapping framework based on octrees}},
volume = {34},
year = {2013}
}
@article{Levinson2013,
abstract = {The combined use of 3D scanning lasers with 2D cameras has become increasingly popular in mobile robotics, as the sparse depth measurements of the former augment the dense color information of the latter. Sensor fusion requires precise 6-DOF transforms between the sensors, but hand-measuring these values is tedious and inaccurate. In addition, autonomous robots can be rendered inoperable if their sensors' calibrations change over time. Yet previously published camera-laser calibration algorithms are offline only, requiring significant amounts of data and/or specific calibration targets; they are thus unable to correct calibration errors that occur during live operation.},
author = {Levinson, Jesse and Thrun, Sebastian},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Levinson, Thrun/Robotics Science and Systems ssss Automatic Online Calibration of Cameras and Lasers. Levinson, Thrun. 2013.pdf:pdf},
journal = {Stanford},
title = {{Robotics: Science and Systems ssss Automatic Online Calibration of Cameras and Lasers}},
url = {http://roboticsproceedings.org/rss09/p29.pdf},
year = {2013}
}
@article{Liang2019,
abstract = {In this paper we propose to exploit multiple related tasks for accurate multi-sensor 3D object detection. Towards this goal we present an end-to-end learnable architecture that reasons about 2D and 3D object detection as well as ground estimation and depth completion. Our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels. Importantly, our approach leads the KITTI benchmark on 2D, 3D and bird's eye view object detection, while being real-time.},
author = {Liang, Ming and Yang, Bin and Chen, Yun and Hu, Rui and Urtasun, Raquel},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Liang et al/Multi-Task Multi-Sensor Fusion for 3D Object Detection. Liang et al.. 2019.pdf:pdf},
journal = {IEEE Conf. Comput. Vis. Pattern Recognition, Proceedings, CVPR},
keywords = {3d object detection,autonomous,multi-sensor fusion},
pages = {7345--7353},
title = {{Multi-Task Multi-Sensor Fusion for 3D Object Detection}},
url = {https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Multi-Task-Multi-Sensor-Fusion-for-3D-Object-Detection.pdf},
year = {2019}
}
@article{Shi2018,
abstract = {In this paper, we propose PointRCNN for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for refining proposals in the canonical coordinates to obtain the final detection results. Instead of generating proposals from RGB image or projecting point cloud to bird's view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of the whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box refinement and confidence prediction. Extensive experiments on the 3D detection benchmark of KITTI dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input. The code is available at https://github.com/sshaoshuai/PointRCNN.},
archivePrefix = {arXiv},
arxivId = {1812.04244},
author = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
eprint = {1812.04244},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Shi, Wang, Li/PointRCNN 3D Object Proposal Generation and Detection from Point Cloud. Shi, Wang, Li. 2018.pdf:pdf},
title = {{PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud}},
url = {http://arxiv.org/abs/1812.04244},
year = {2018}
}
@article{Reyes2018,
abstract = {The main goal of the paper is to provide Pepper with a near real-time object recognition system based on deep neural networks. The proposed system is based on YOLO (You Only Look Once), a deep neural network that is able to detect and recognize objects robustly and at a high speed. In addition, considering that YOLO cannot be run in the Pepper's internal computer in near real-time, we propose to use a Backpack for Pepper, which holds a Jetson TK1 card and a battery. By using this card, Pepper is able to robustly detect and recognize objects in images of 320x320 pixels at about 5 frames per second.},
archivePrefix = {arXiv},
arxivId = {1811.08352},
author = {Reyes, Esteban and G{\'{o}}mez, Cristopher and Norambuena, Esteban and Ruiz-del-Solar, Javier},
eprint = {1811.08352},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Reyes et al/Near Real-Time Object Recognition for Pepper based on Deep Neural Networks Running on a Backpack. Reyes et al.. 2018.pdf:pdf},
keywords = {jetson tk1,pepper robot,ros,yolo},
pages = {1--12},
title = {{Near Real-Time Object Recognition for Pepper based on Deep Neural Networks Running on a Backpack}},
url = {http://arxiv.org/abs/1811.08352},
year = {2018}
}
@inproceedings{Javaheri2017,
abstract = {The increasing availability of point cloud data in recent years is demanding for high performance denoising methods and compression schemes. When point cloud data is directly obtained from depth sensors or extracted from images acquired from different viewpoints, imprecisions on the depth acquisition or in the 3D reconstruction techniques result in noisy point clouds which may include a significant number of outliers. Moreover, the quality assessment of point clouds is a challenging problem since this 3D representation format is unstructured and it is typically not directly visualized. In this paper, selected objective quality metrics are evaluated regarding their correlation with human quality assessment and thus human perception. As far as the authors know, this is the first paper performing the subjective assessment of point cloud denoising algorithms and the evaluation of most used point cloud objective quality metrics. Experimental results show that graph-based denoising algorithms can improve significantly the point cloud quality data and that objective metrics that model the underlying point cloud surface can correlate better with human perception.},
author = {Javaheri, Alireza and Brites, Catarina and Pereira, Fernando and Ascenso, Jo{\~{a}}o},
booktitle = {2017 IEEE 19th Int. Work. Multimed. Signal Process. MMSP 2017},
doi = {10.1109/MMSP.2017.8122239},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Subjective and objective quality evaluation of compressed point clouds. Javaheri et al.pdf:pdf},
isbn = {9781509036493},
issn = {1047-2797},
keywords = {Point cloud compression,Quality metrics,Subjective quality assessment},
pages = {1--6},
pmid = {10037558},
title = {{Subjective and objective quality evaluation of compressed point clouds}},
volume = {2017-Janua},
year = {2017}
}
@article{Ishikawa2018a,
abstract = {In this paper, we propose a method of targetless and automatic Camera-LiDAR calibration. Our approach is an extension of hand-eye calibration framework to 2D-3D calibration. By using the sensor fusion odometry method, the scaled camera motions are calculated with high accuracy. In addition to this, we clarify the suitable motion for this calibration method. The proposed method only requires the three-dimensional point cloud and the camera image and does not need other information such as reflectance of LiDAR and to give initial extrinsic parameter. In the experiments, we demonstrate our method using several sensor configurations in indoor and outdoor scenes to verify the effectiveness. The accuracy of our method achieves more than other comparable state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1804.05178v1},
author = {Ishikawa, Ryoichi and Oishi, Takeshi and Ikeuchi, Katsushi},
doi = {10.1109/IROS.2018.8593360},
eprint = {arXiv:1804.05178v1},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ishikawa, Oishi, Ikeuchi/LiDAR and Camera Calibration Using Motions Estimated by Sensor Fusion Odometry. Ishikawa, Oishi, Ikeuchi. 2018.pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
pages = {7342--7349},
title = {{LiDAR and Camera Calibration Using Motions Estimated by Sensor Fusion Odometry}},
year = {2018}
}
@article{Guindel2018,
abstract = {Sensor setups consisting of a combination of 3D range scanner lasers and stereo vision systems are becoming a popular choice for on-board perception systems in vehicles; however, the combined use of both sources of information implies a tedious calibration process. We present a method for extrinsic calibration of lidar-stereo camera pairs without user intervention. Our calibration approach is aimed to cope with the constraints commonly found in automotive setups, such as low-resolution and specific sensor poses. To demonstrate the performance of our method, we also introduce a novel approach for the quantitative assessment of the calibration results, based on a simulation environment. Tests using real devices have been conducted as well, proving the usability of the system and the improvement over the existing approaches. Code is available at http://wiki.ros.org/velo2cam{\_}calibration},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.04085v3},
author = {Guindel, Carlos and Beltran, Jorge and Martin, David and Garcia, Fernando},
doi = {10.1109/ITSC.2017.8317829},
eprint = {arXiv:1705.04085v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Guindel et al/Automatic extrinsic calibration for lidar-stereo vehicle sensor setups. Guindel et al.. 2018.pdf:pdf},
isbn = {9781538615256},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
pages = {1--6},
title = {{Automatic extrinsic calibration for lidar-stereo vehicle sensor setups}},
volume = {2018-March},
year = {2018}
}
@article{Sattarov2016,
author = {Sattarov, Egor and Gepperth, Alexander and Alberto, Sergio and Florez, Rodriguez and Sattarov, Egor and Gepperth, Alexander and Alberto, Sergio and Florez, Rodriguez and Calibration-, Roger Reynaud and Sattarov, Egor and Gepperth, Alexander and F, Sergio A Rodriguez and Reynaud, Roger},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Sattarov et al/Calibration-free match finding between vision and To cite this version HAL Id hal-01292529 Calibration-free match finding between visi.pdf:pdf},
title = {{Calibration-free match finding between vision and To cite this version : HAL Id : hal-01292529 Calibration-free match finding between vision and LIDAR *}},
year = {2016}
}
@article{Velodyne2016b,
abstract = {Velodyne VLP-16 User's Manual},
author = {Velodyne},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Velodyne/Vlp-16. Velodyne. 2016.pdf:pdf},
pages = {6--9},
title = {{Vlp-16}},
url = {www.velodyne.com},
year = {2016}
}
@article{Nanoparticles2013,
author = {Nanoparticles, Ag},
doi = {10.1039/b000000x},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Nanoparticles/Nanoscale Microlandscaping on Graphene Oxide Film via Localized Decoration of. Nanoparticles. 2013.pdf:pdf},
keywords = {and have enough horsepower,applications,build a robot that,can drive around your,house,ll be able to,map building,navigation,ros,see in 3d,simulation,that turtlebot offers,there are numerous abilities,to create exciting,turtlebot,you},
number = {207890},
title = {{Nanoscale Microlandscaping on Graphene Oxide Film via Localized Decoration of}},
year = {2013}
}
@article{Wenzel2011,
author = {Wenzel, Konrad and Abdel-Wahab, M and Cefalu, Alessandro and Fritsch, Dieter},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A Multi-Camera System for Efficient Point Cloud Recording in Close Range Applications. Wenzel et al.pdf:pdf},
journal = {LC3D Work.},
keywords = {close range,cultural heritage,high resolution,imagery,matching,multisensor,photogrammetry,point cloud},
title = {{A Multi-Camera System for Efficient Point Cloud Recording in Close Range Applications}},
url = {http://www.ifp.uni-stuttgart.de/publications/2011/2011{\_}Wenzel{\_}Berlin.pdf},
year = {2011}
}
@article{ROSWiki2016,
author = {{ROS Wiki}},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/ROS Wiki/ROS C Style Guide. ROS Wiki. 2016.pdf:pdf},
pages = {1--14},
title = {{ROS C ++ Style Guide}},
year = {2016}
}
@article{Bae,
abstract = {Terrestrial laser scanners provide a three-dimensional sampled representation of the surfaces of objects resulting in a very large number of points. The spatial resolution of the data is much higher than that of conventional surveying methods. Since laser scanners have a limited field of view, in order to obtain a complete representation of an object it is necessary to collect data from several different locations that must be transformed into a common coordinate system. Existing registration methods, such as the Iterative Closest Point (ICP) or Chen and Medionis method, work well if good a priori alignment is provided. However, in the case of the registration of partially overlapping and unorganised point clouds without good initial alignment, these methods are not appropriate since it become very difficult to find correspondence. A method based on geometric primitives and neighbourhood search is proposed. The change of geometric curvature and approximate normal vector of the surface formed by a point and its neighbourhood are used to determine the possible correspondence of point clouds. Our method is tested with a simulated point cloud with various levels of noise and two real point clouds.},
author = {Bae, Kwang-ho and Lichti, Derek D and Cloud, Point and Scanning, Laser},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Bae et al/Automated Registration of Unorganised Point Clouds From Terrestrial Laser Scanners. Bae et al.. Unknown.pdf:pdf},
journal = {Update},
keywords = {automation,laser scanning,point cloud,registration,three-dimensional,tls},
title = {{Automated Registration of Unorganised Point Clouds From Terrestrial Laser Scanners}}
}
@article{McManamon2012,
abstract = {Abstract. Ladar is becoming more prominent due to the maturation of its component technologies, especially lasers. There are many forms of ladar. There is simple two-dimensional (2-D) ladar, similar to a passive electro-optic sensor, but with controlled illumination and the ability to see at night even at short wavelengths. There is three-dimensional (3-D) ladar, with angle/angle/range information. 3-D images are very powerful because shape is an invariant. 3-D images can be easily rotated to various perspectives. You can add gray scale or color, just like passive, or 2-D ladar, imaging. You can add precise velocity measurement, including vibrations. Ladar generates orders of magnitude higher frequency change then microwave radar for velocity measurement, because frequency change is proportional to one over the wavelength. Orders of magnitude higher frequency change means you can measure a given velocity orders of magnitude quicker, in many cases making an accurate measurement possible. Polarization can be used. With an active sensor you control both the illumination and the reception, so you can pattern the illumination. Also, because ladar can use narrow band illumination it is easier to easier to coherently combine sub-aperture images to obtain the higher resolution of an array.},
author = {McManamon, Paul F.},
doi = {10.1117/1.oe.51.8.089801},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/McManamon/Errata Review of ladar a historic, yet emerging, sensor technology with rich phenomenology. McManamon. 2012.pdf:pdf},
issn = {0091-3286},
journal = {Opt. Eng.},
number = {8},
pages = {089801--1},
title = {{Errata: Review of ladar: a historic, yet emerging, sensor technology with rich phenomenology}},
volume = {51},
year = {2012}
}
@article{Claderaa,
author = {Cladera, Antoni},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Cladera/The Ultimate Photography Guide to Depth of Field (DoF) PhotoPills. Cladera. Unknown.pdf:pdf},
journal = {PhotoPills},
title = {{The Ultimate Photography Guide to Depth of Field (DoF) | PhotoPills}},
url = {https://www.photopills.com/articles/ultimate-guide-depth-field{\#}step3}
}
@book{Fernandez2015,
abstract = {Second edition. Includes index.},
author = {Fern{\'{a}}ndez, Enrique and Crespo, Luis S{\'{a}}nchez and Mahtani, Anil and Martinez, Aaron},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fern{\'{a}}ndez et al/Learning ROS for Robotics Programming Second Edition Your one-stop guide to the Robot Operating System. Fern{\'{a}}ndez et al.. 2015.pdf:pdf},
isbn = {9781783987580},
title = {{Learning ROS for Robotics Programming Second Edition Your one-stop guide to the Robot Operating System}},
url = {www.itbookshub.com},
year = {2015}
}
@article{Ashburner2003,
abstract = {This chapter opens with a discussion of how images are transformed via the process of resampling. This chapter is about rigid registration of images and therefore it describes the parameterization of rigid body transformations as a subset of the more general affine transformations. Image registration is important in many aspects of functional image analysis. In imaging neuroscience, particularly for fMRI, the signal changes due to any h??modynamic response can be small compared to apparent signal differences that can result from subject movement. Subject head movement in the scanner cannot be completely eliminated, so retrospective motion correction is performed as a preprocessing step. This is especially important for experiments where subjects may move in the scanner in a way that is correlated with the different conditions. It also focuses on the methods of rigid body registration, in both intra and intermodality contexts. Intramodality registration implies registration of images acquired using the same modality and scanning sequence or contrast agent, whereas intermodality registration allows the registration of different modalities. ?? 2004 Elsevier Inc. All rights reserved.},
author = {Ashburner, John and Friston, Karl J},
doi = {10.1016/B978-012264841-0/50054-8},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ashburner, Friston/Rigid Body Registration and Interpolation. Ashburner, Friston. 2003.pdf:pdf},
isbn = {9780122648410},
issn = {1053-8119},
journal = {Hum. Brain Funct.},
keywords = {Arousal,Arousal: physiology,Attention,Attention: physiology,Bayes Theorem,Brain,Brain Mapping,Brain Mapping: methods,Brain: physiology,Computer Simulation,Computer-Assisted,Corpus Striatum,Corpus Striatum: physiopathology,Emission-Computed,Evoked Potentials,Evoked Potentials: physiology,Hemodynamics,Hemodynamics: physiology,Humans,Image Processing,Likelihood Functions,Linear Models,Magnetic Resonance Imaging,Magnetic Resonance Imaging: statistics {\&} numerical,Models,Motion Perception,Motion Perception: physiology,Neural Networks (Computer),Neurological,Nonlinear Dynamics,Parietal Lobe,Parietal Lobe: physiology,Probability,Tomography},
number = {4},
pages = {599--632},
pmid = {10334905},
title = {{Rigid Body Registration and Interpolation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.8768{\&}rep=rep1{\&}type=pdf{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/10334905{\%}0Ahttp://www.ncbi.nlm.nih.gov/pubmed/19398015{\%}0Ahttp://www.ncbi.nlm.nih.gov/pubmed/20497204{\%}0Ahttp://www.fil.ion.ucl.ac.uk/s},
volume = {13},
year = {2003}
}
@techreport{Redmon2018,
author = {Redmon, Joseph and Farhadi, Ali},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Redmon, Farhadi/YOLO v.3. Redmon, Farhadi. 2018.pdf:pdf},
pages = {1--6},
title = {{YOLO v.3}},
url = {https://pjreddie.com/media/files/papers/YOLOv3.pdf},
year = {2018}
}
@article{Fernandez-Moral2014,
abstract = {{\textcopyright} 2014 IEEE. The integration of several range cameras in a mobile platform is useful for applications in mobile robotics and autonomous vehicles that require a large field of view. This situation is increasingly interesting with the advent of low cost range cameras like those developed by Primesense. Calibrating such combination of sensors for any geometric configuration is a problem that has been recently solved through visual odometry (VO) and SLAM. However, this kind of solution is laborious to apply, requiring robust SLAM or VO in controlled environments. In this paper we propose a new uncomplicated technique for extrinsic calibration of range cameras that relies on finding and matching planes. The method that we present serves to calibrate two or more range cameras in an arbitrary configuration, requiring only to observe one plane from different viewpoints. The conditions to solve the problem are studied, and several practical examples are presented covering different geometric configurations, including an omnidirectional RGB-D sensor composed of 8 range cameras. The quality of this calibration is evaluated with several experiments that demonstrate an improvement of accuracy over design parameters, while providing a versatile solution that is extremely fast and easy to apply.},
author = {Fern{\'{a}}ndez-Moral, Eduardo and Gonz{\'{a}}lez-Jim{\'{e}}nez, Javier and Rives, Patrick and Ar{\'{e}}valo, Vicente},
doi = {10.1109/IROS.2014.6942595},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fern{\'{a}}ndez-Moral et al/Extrinsic calibration of a set of range cameras in 5 seconds without pattern. Fern{\'{a}}ndez-Moral et al.. 2014.pdf:pdf},
isbn = {9781479969340},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
pages = {429--435},
title = {{Extrinsic calibration of a set of range cameras in 5 seconds without pattern}},
year = {2014}
}
@article{Quan2011,
author = {Quan, Long and Lan, Zhong-dan and Quan, Long and Linear, Zhong-dan Lan and Pose, N-point Camera and Ieee, Determination},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Quan et al/Linear N-Point Camera Pose Determination To cite this version HAL Id inria-00590105. Quan et al.. 2011.pdf:pdf},
title = {{Linear N-Point Camera Pose Determination To cite this version : HAL Id : inria-00590105}},
year = {2011}
}
@article{Gong2013,
abstract = {This paper presents a novel way to address the extrinsic calibration problem for a system composed of a 3D LIDAR and a camera. The relative transformation between the two sensors is calibrated via a nonlinear least squares (NLS) problem, which is formulated in terms of the geometric constraints associated with a trihedral object. Precise initial estimates of NLS are obtained by dividing it into two sub-problems that are solved individually. With the precise initializations, the calibration parameters are further refined by iteratively optimizing the NLS problem. The algorithm is validated on both simulated and real data, as well as a 3D reconstruction application. Moreover, since the trihedral target used for calibration can be either orthogonal or not, it is very often present in structured environments, making the calibration convenient.},
author = {Gong, Xiaojin and Lin, Ying and Liu, Jilin},
doi = {10.3390/s130201902},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gong, Lin, Liu/3D LIDAR-camera extrinsic calibration using an arbitrary trihedron. Gong, Lin, Liu. 2013.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {3D LIDAR-camera system,Extrinsic calibration,Sensor fusion},
number = {2},
pages = {1902--1918},
title = {{3D LIDAR-camera extrinsic calibration using an arbitrary trihedron}},
volume = {13},
year = {2013}
}
@article{Liao2019,
abstract = {Fusion of heterogeneous exteroceptive sensors is the most efficient and effective path to the representation of the environment precisely, as it can compromise various drawbacks of each homogeneous sensor. The rigid transformation (aka. extrinsic parameters) of heterogeneous sensory systems is the prerequisite of fusing the multi-sensor information. Researchers have proposed several approaches to estimate the extrinsic parameters. However, these approaches neither rely on human interventions or specifically designed auxiliary object or do not provide the library which makes it hard to test or benchmark. In this paper, we propose a novel extrinsic calibration approach for the extrinsic calibration of a Lidar (Laser Range Finder) and a camera which only based on a polygon board and we offer the relevant tools. In this paper, we firstly track and extract the target polygon from both the image and point-cloud. Then we try to match the polygon between the 2D and 3D feature spaces. With the associated polygon, we are able to get multiple constraints to optimize the extrinsic parameters. At the end, we validate our approach by four configurations , including the simulation, 16/32-beam Lidar and 100-line MEMS-Lidar. The outcome indicates high-precision extrinsic calibration performance.},
author = {Liao, Qinghai and Chen, Zhenyong and Liu, Yang and Wang, Zhe and Liu, Ming},
doi = {10.1109/ROBIO.2018.8665256},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Liao et al/Extrinsic Calibration of Lidar and Camera with Polygon. Liao et al.. 2019.pdf:pdf},
isbn = {9781728103761},
journal = {2018 IEEE Int. Conf. Robot. Biomimetics, ROBIO 2018},
pages = {200--205},
publisher = {IEEE},
title = {{Extrinsic Calibration of Lidar and Camera with Polygon}},
year = {2019}
}
@article{Dhall2017,
abstract = {With the advent of autonomous vehicles, LiDAR and cameras have become an indispensable combination of sensors. They both provide rich and complementary data which can be used by various algorithms and machine learning to sense and make vital inferences about the surroundings. We propose a novel pipeline and experimental setup to find accurate rigid-body transformation for extrinsically calibrating a LiDAR and a camera. The pipeling uses 3D-3D point correspondences in LiDAR and camera frame and gives a closed form solution. We further show the accuracy of the estimate by fusing point clouds from two stereo cameras which align perfectly with the rotation and translation estimated by our method, confirming the accuracy of our method's estimates both mathematically and visually. Taking our idea of extrinsic LiDAR-camera calibration forward, we demonstrate how two cameras with no overlapping field-of-view can also be calibrated extrinsically using 3D point correspondences. The code has been made available as open-source software in the form of a ROS package, more information about which can be sought here: https://github.com/ankitdhall/lidar{\_}camera{\_}calibration .},
archivePrefix = {arXiv},
arxivId = {1705.09785},
author = {Dhall, Ankit and Chelani, Kunal and Radhakrishnan, Vishnu and Krishna, K. M.},
eprint = {1705.09785},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Dhall et al/LiDAR-Camera Calibration using 3D-3D Point correspondences. Dhall et al.. 2017.pdf:pdf},
pages = {1--19},
title = {{LiDAR-Camera Calibration using 3D-3D Point correspondences}},
url = {http://arxiv.org/abs/1705.09785},
year = {2017}
}
@article{Persic2019,
abstract = {Autonomous navigation of mobile robots is often based on information from a variety of heterogeneous sensors; hence, extrinsic sensor calibration is a fundamental step in the fusion of such information. In this paper, we address the problem of extrinsic calibration of a radar–LiDAR–camera sensor system. This problem is primarily challenging due to sparse informativeness of radar measurements. Namely, radars cannot extract rich structural information about the environment, while their lack of elevation resolution, that is nevertheless accompanied by substantial elevation field of view, introduces uncertainty in the origin of the measurements. We propose a novel calibration method which involves a special target design and two-step optimization procedure to solve the aforementioned challenges. First step of the optimization is minimization of a reprojection error based on an introduced point–circle geometric constraint. Since the first step is not able to provide reliable estimates of all the six extrinsic parameters, we introduce a second step to refine the subset of parameters with high uncertainty. We exploit a pattern discovered in the radar cross section estimation that is correlated to the missing elevation angle. Additionally, we carry out identifiability analysis based on the Fisher Information Matrix to show minimal requirements on the dataset and to verify the method through simulations. We test the calibration method on a variety of sensor configurations and address the problem of radar vertical misalignment. In the end, we show via extensive experiment analysis that the proposed method is able to reliably estimate all the six parameters of the extrinsic calibration.},
author = {Per{\v{s}}i{\'{c}}, Juraj and Markovi{\'{c}}, Ivan and Petrovi{\'{c}}, Ivan},
doi = {10.1016/j.robot.2018.11.023},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Per{\v{s}}i{\'{c}}, Markovi{\'{c}}, Petrovi{\'{c}}/Extrinsic 6DoF calibration of a radar–LiDAR–camera system enhanced by radar cross section estimates evaluation. Per{\v{s}}i{\'{c}}, Markovi{\'{c}}.pdf:pdf},
issn = {09218890},
journal = {Rob. Auton. Syst.},
keywords = {Camera,LiDAR,Radar,Radar cross section,Sensor calibration},
pages = {217--230},
publisher = {Elsevier B.V.},
title = {{Extrinsic 6DoF calibration of a radar–LiDAR–camera system enhanced by radar cross section estimates evaluation}},
url = {https://doi.org/10.1016/j.robot.2018.11.023},
volume = {114},
year = {2019}
}
@article{Lin2014,
abstract = {Recently, object-based image analysis (OBIA) has been proven to be more efficientin image classificationthan pixel-based methods. In order to raise the capability of automatic recognition of land features from LiDAR data, 2D OBIA method is extended for 3D point cloud classification of LiDAR data in this study. First, point cloud is segmented to independent 3D objects by various methods. Second, several object features designed by this study are calculated. At last, the point clouds are classified automatically according to the variations of object features. In this study, an airborne LiDAR dataset is used to test and verify the efficiency of the proposed method. The results showed that utilizing the object-based concept can give assistance to point cloud recognition by means of depicting the spatial characters of these objects. Furthermore, the proposed method improves not only the completeness, but also the quality of classification results. INTRODUCTION Traditional remote sensing techniques classified different kinds of land features mainly by characteristics of spectral responses. As the development of surveying methods, using LiDAR (Light Detection And Ranging) system to rapidly acquire a large number of 3D point clouds has gradually become a crucial manner of obtaining spatial data. In contrast with images, LiDAR point clouds, which possess distinctive 3Dgeometric properties, provide another kind of interpreting information for land feature classification. In the past the pixel-based concept was often used for image classification. However, as image resolution increases, spatial relations among adjacent pixels are more abundant and diversified, leading traditional pixel-based classification to fail to meet demand.In response to it, image classification methods have been extending from pixel-based consideration to the object-based concept, which additionally takes account of spatial featuressuch as shape or texture that are formed by neighbor pixels as classified standards. With augmenting informational dimensions, completeness and rationality of classification results can be increased (Oruc et al., 2004; Lang, 2008). On the other hand, owing to characteristics of directly recording 3D coordinates and multi-reflected echoes of LiDAR system, numerous researches progressively adopted the way that direct classifies point clouds to acquire land feature information. Nevertheless, since LiDAR system does not execute measuring by aiming at major feature points, it is difficult to confirm geometric relations among point, causing one must introduce other auxiliaryinformation or decrease complexity of extraction information so as to increaseavailability of point clouds. In terms of data characteristics about images and LiDAR data, images provide plentiful spectral information as classification bases, and establish subordinative relations among pixels to improve classification efficiency after adding the object-based concept. On the contrary, LiDAR data provide ample geometric information. Supposing one can further build up subordinative relations among these point clouds, utilizing spatial distributive properties that point clouds possess, application efficiency should be largely improved. Therefore, in this study2D object-based image classification method, which is so-called object-based image analysis (OBIA), is extended for 3D point cloudclassification of LiDAR data, namely object-based point cloud analysis (OBPCA).},
author = {Lin, KF and Hsu, CPWPH},
isbn = {9781629939100},
journal = {Aars.Org},
keywords = {classification,lidar,object-based,point cloud},
number = {1},
pages = {1134--1141},
title = {{Object-Based Classification for Lidar Point Cloud}},
url = {http://a-a-r-s.org/acrs/administrator/components/com{\_}jresearch/files/publications/SC02-0600.pdf},
year = {2014}
}
@article{Fischler1981a,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
archivePrefix = {arXiv},
arxivId = {3629719},
author = {Fischler, Martin a and Bolles, Robert C},
doi = {10.1145/358669.358692},
eprint = {3629719},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fischler, Bolles/Random Sample Paradigm for Model Consensus A Apphcatlons to Image Fitting with Analysis and Automated Cartography. Fischler, Bolles. 198.pdf:pdf},
isbn = {0934613338},
issn = {00010782},
journal = {Commun. ACM},
keywords = {0,1,2,3,5,60,61,71,8,analysis,and phrases,automated cartography,camera calibration,cr categories,determination,image matching,location,model fitting,scene},
number = {6},
pages = {381--395},
pmid = {4650729},
title = {{Random Sample Paradigm for Model Consensus: A Apphcatlons to Image Fitting with Analysis and Automated Cartography}},
volume = {24},
year = {1981}
}
@article{Strom2010,
abstract = {We present an efficient graph-theoretic algorithm for segmenting a colored laser point cloud derived from a laser scanner and camera. Segmentation of raw sensor data is a crucial first step for many high level tasks such as object recognition, obstacle avoidance and terrain classification. Our method enables combination of color information from a wide field of view camera with a 3D LIDAR point cloud from an actuated planar laser scanner. We extend previous work on robust camera-only graph-based segmentation to the case where spatial features, such as surface normals, are available. Our combined method produces segmentation results superior to those derived from either cameras or laser-scanners alone. We verify our approach on both indoor and outdoor scenes.},
author = {Strom, Johannes and Richardson, Andrew and Olson, Edwin},
doi = {10.1109/IROS.2010.5650459},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Strom, Richardson, Olson/Graph-based segmentation for colored 3D laser point clouds. Strom, Richardson, Olson. 2010.pdf:pdf},
isbn = {9781424466757},
journal = {IEEE/RSJ 2010 Int. Conf. Intell. Robot. Syst. IROS 2010 - Conf. Proc.},
pages = {2131--2136},
title = {{Graph-based segmentation for colored 3D laser point clouds}},
year = {2010}
}
@article{EvansT.C.GavrilovichE.MihaiR.C.andIsbasescuI.2014,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0403007},
author = {{Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isbasescu, I.}, Easyg Llc and Thelen, Darryl and Martin, J A and Allen, S M and SA, Slane},
doi = {10.1037/t24245-000},
eprint = {0403007},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isbasescu, I. et al/( 12 ) Patent Application Publication ( 10 ) Pub . No . US 2006 0222585 A1 Figure 1. Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isba.pdf:pdf},
isbn = {2009023471},
issn = {13871811},
number = {15},
pages = {354},
pmid = {23110556},
primaryClass = {arXiv:physics},
title = {{( 12 ) Patent Application Publication ( 10 ) Pub . No .: US 2006 / 0222585 A1 Figure 1}},
volume = {002},
year = {2014}
}
@article{SICK2017,
author = {SICK},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/SICK/Mrs1000. SICK. 2017.pdf:pdf},
title = {{Mrs1000}},
year = {2017}
}
@article{Menze2015,
abstract = {자동주행 application에서 사용될 3D scene flow estimation 방법 제안 Outdoor scene에서 moving object를 분해함 그래서 각각의 element들을 각각 super-pixel 3D plane에 표현(indexing 도 수행)},
author = {Menze, Moritz and Geiger, Andreas and Mpi, T},
doi = {10.1109/CVPR.2015.7298925},
file = {:media/martinspedro/DATA/UA/Thesis/Research/Organized/Menze, Geiger, Mpi/Supplementary Material Object Scene Flow for Autonomous Vehicles - 2015 - Menze, Geiger, Mpi.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Cvpr},
pages = {1--39},
title = {{Supplementary Material: Object Scene Flow for Autonomous Vehicles}},
volume = {07-12-June},
year = {2015}
}
@article{Wang2018,
abstract = {We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive deep learning framework for 3D object instance segmentation on point clouds. SGPN uses a single network to predict point grouping proposals and a corresponding semantic class for each proposal, from which we can directly extract instance segmentation results. Important to the effectiveness of SGPN is its novel representation of 3D instance segmentation results in the form of a similarity matrix that indicates the similarity between each pair of points in embedded feature space, thus producing an accurate grouping proposal for each point. To the best of our knowledge, SGPN is the first framework to learn 3D instance-aware semantic segmentation on point clouds. Experimental results on various 3D scenes show the effectiveness of our method on 3D instance segmentation, and we also evaluate the capability of SGPN to improve 3D object detection and semantic segmentation results. We also demonstrate its flexibility by seamlessly incorporating 2D CNN features into the framework to boost performance.},
archivePrefix = {arXiv},
arxivId = {arXiv:1711.08588v2},
author = {Wang, Weiyue and Yu, Ronald and Huang, Qiangui and Neumann, Ulrich},
doi = {10.1109/CVPR.2018.00272},
eprint = {arXiv:1711.08588v2},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wang et al/SGPN Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation. Wang et al.. 2018.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {2569--2578},
title = {{SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation}},
year = {2018}
}
@article{Geiger2013,
abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high- resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to inner- city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide},
author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
file = {:media/martinspedro/DATA/UA/Thesis/Research/Organized/Geiger et al/Vision meets robotics The KITTI dataset. The International Journal of Robotics Research - 2013 - Geiger et al.pdf:pdf},
journal = {Ijrr},
number = {October},
pages = {1--6},
title = {{Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research}},
year = {2013}
}
@article{Young1963,
abstract = {Li{\{}DAR{\}} {\{}F{\}}or {\{}D{\}}ummies, {\{}A{\}}utodesk and {\{}DLT{\}} {\{}S{\}}olutions {\{}S{\}}pecial$\backslash$n{\{}E{\}}dition, spells out the basics of {\{}L{\}}i{\{}DAR{\}}, including what it$\backslash$nis, how it works, and what those letters stand for. {\{}Y{\}}ou�ll learn$\backslash$nthe main capture techniques along with some details about how {\{}L{\}}i{\{}DAR{\}}$\backslash$nis used and how it will help you pinpoint different features in the$\backslash$nenvironment.},
author = {Young, James},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Young/James Young.. Young. 1963.pdf:pdf},
issn = {00223204},
journal = {J. Obstet. Gynaecol. Br. Commonw.},
pages = {705--707},
title = {{James Young.}},
volume = {70},
year = {1963}
}
@article{Park2019,
abstract = {We present a deep convolutional neural network (CNN) architecture for high-precision depth estimation by jointly utilizing sparse 3D LiDAR and dense stereo depth information. In this network, the complementary characteristics of sparse 3D LiDAR and dense stereo depth are simultaneously encoded in a boosting manner. Tailored to the LiDAR and stereo fusion problem, the proposed network differs from previous CNNs in the incorporation of a compact convolution module, which can be deployed with the constraints of mobile devices. As training data for the LiDAR and stereo fusion is rather limited, we introduce a simple yet effective approach for reproducing the raw KITTI dataset. The raw LIDAR scans are augmented by adapting an off-the-shelf stereo algorithm and a confidence measure. We evaluate the proposed network on the KITTI benchmark and data collected by our multi-sensor acquisition system. Experiments demonstrate that the proposed network generalizes across datasets and is significantly more accurate than various baseline approaches.},
author = {Park, Kihong and Kim, Seungryong and Sohn, Kwanghoon},
doi = {10.1109/tits.2019.2891788},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Park, Kim, Sohn/High-Precision Depth Estimation Using Uncalibrated LiDAR and Stereo Fusion. Park, Kim, Sohn. 2019.pdf:pdf},
issn = {1524-9050},
journal = {IEEE Trans. Intell. Transp. Syst.},
pages = {1--15},
publisher = {IEEE},
title = {{High-Precision Depth Estimation Using Uncalibrated LiDAR and Stereo Fusion}},
volume = {PP},
year = {2019}
}
@article{Verma2019,
abstract = {This paper proposes an automated method to obtain the extrinsic calibration parameters between a camera and a 3D lidar with as low as 16 beams. We use a checkerboard as a reference to obtain features of interest in both sensor frames. The calibration board centre point and normal vector are automatically extracted from the lidar point cloud by exploiting the geometry of the board. The corresponding features in the camera image are obtained from the camera's extrinsic matrix. We explain the reasons behind selecting these features, and why they are more robust compared to other possibilities. To obtain the optimal extrinsic parameters, we choose a genetic algorithm to address the highly non-linear state space. The process is automated after defining the bounds of the 3D experimental region relative to the lidar, and the true board dimensions. In addition, the camera is assumed to be intrinsically calibrated. Our method requires a minimum of 3 checkerboard poses, and the calibration accuracy is demonstrated by evaluating our algorithm using real world and simulated features.},
archivePrefix = {arXiv},
arxivId = {1904.12433},
author = {Verma, Surabhi and Berrio, Julie Stephany and Worrall, Stewart and Nebot, Eduardo},
eprint = {1904.12433},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Verma et al/Automatic extrinsic calibration between a camera and a 3D Lidar using 3D point and plane correspondences. Verma et al.. 2019.pdf:pdf},
pages = {1--7},
title = {{Automatic extrinsic calibration between a camera and a 3D Lidar using 3D point and plane correspondences}},
url = {http://arxiv.org/abs/1904.12433},
year = {2019}
}
@article{Silva2018,
author = {Silva, Varuna De and Roche, Jamie and Kondoz, Ahmet},
doi = {10.3390/s18082730},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Robust Fusion of LiDAR and Wide-Angle Camera. Silva, Roche, Kondoz.pdf:pdf},
issn = {1424-8220},
keywords = {assistive robots,autonomous vehicles,depth sensing,detection,free space,gaussian process regression,lidar,sensor data fusion},
title = {{Robust Fusion of LiDAR and Wide-Angle Camera}},
year = {2018}
}
@article{Mousavian,
abstract = {supplementary material for 3d deepbox},
author = {Mousavian, Arsalan and Anguelov, Dragomir and Flynn, John and Ko{\v{s}}eck{\'{a}}, Jana},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Mousavian et al/3D bounding box material. Mousavian et al.. Unknown.pdf:pdf},
pages = {2},
title = {{3D bounding box material}}
}
@article{Walch2014,
abstract = {In this paper we describe a 2D/3D vision sensor, which consists of a laser triangulation sensor and matrix colour camera. The outcome of this sensor is the fusion of the 3D data delivered from the laser triangulation sensor and the colour information of the matrix camera in the form of a coloured point cloud. For this reason a novel calibration method for the laser triangulation sensor was developed, which makes it possible to use one common calibration object for both cameras and provides their relative spatial position. A sensor system with a SICK Ranger E55 profile scanner and a DALSA Genie color camera was set up to test the calibration in terms of the quality of the match between the color information and the 3D point cloud.},
author = {Walch, Alexander and Eitzinger, Christian},
doi = {10.5220/0004682900890095},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Walch, Eitzinger/A Combined Calibration of 2D and 3D Sensors - A Novel Calibration for Laser Triangulation Sensors based on Point Correspondences. Walch,.pdf:pdf},
journal = {2014 Int. Conf. Comput. Vis. Theory Appl.},
pages = {89--95},
publisher = {SCITEPRESS},
title = {{A Combined Calibration of 2D and 3D Sensors - A Novel Calibration for Laser Triangulation Sensors based on Point Correspondences}},
volume = {1},
year = {2014}
}
@article{Shaukat2016,
abstract = {{\textcopyright} 2016 by the authors; licensee MDPI,Basel,Switzerland. In recent decades,terrain modelling and reconstruction techniques have increased research interest in precise short and long distance autonomous navigation,localisation and mapping within field robotics. One of the most challenging applications is in relation to autonomous planetary exploration using mobile robots. Rovers deployed to explore extraterrestrial surfaces are required to perceive and model the environment with little or no intervention from the ground station. Up to date,stereopsis represents the state-of-the art method and can achieve short-distance planetary surface modelling. However,future space missions will require scene reconstruction at greater distance,fidelity and feature complexity,potentially using other sensors like Light Detection And Ranging (LIDAR). LIDAR has been extensively exploited for target detection,identification,and depth estimation in terrestrial robotics,but is still under development to become a viable technology for space robotics. This paper will first review current methods for scene reconstruction and terrain modelling using cameras in planetary robotics and LIDARs in terrestrial robotics; then we will propose camera-LIDAR fusion as a feasible technique to overcome the limitations of either of these individual sensors for planetary exploration. A comprehensive analysis will be presented to demonstrate the advantages of camera-LIDAR fusion in terms of range,fidelity,accuracy and computation.},
author = {Shaukat, Affan and Blacker, Peter C. and Spiteri, Conrad and Gao, Yang},
doi = {10.3390/s16111952},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Shaukat et al/Towards camera-LIDAR fusion-based terrain modelling for planetary surfaces Review and analysis. Shaukat et al.. 2016.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {3-D reconstruction,Hybrid vision systems,LIDAR-camera fusion,Planetary surface perception,Terrain modelling},
number = {11},
title = {{Towards camera-LIDAR fusion-based terrain modelling for planetary surfaces: Review and analysis}},
volume = {16},
year = {2016}
}
@article{Hana2018,
abstract = {The introduction of inexpensive 3D data acquisition devices has promisingly facilitated the wide availability and popularity of 3D point cloud, which attracts more attention on the effective extraction of novel 3D point cloud descriptors for accurate and efficient of 3D computer vision tasks. However, how to de- velop discriminative and robust feature descriptors from various point clouds remains a challenging task. This paper comprehensively investigates the exist- ing approaches for extracting 3D point cloud descriptors which are categorized into three major classes: local-based descriptor, global-based descriptor and hybrid-based descriptor. Furthermore, experiments are carried out to present a thorough evaluation of performance of several state-of-the-art 3D point cloud descriptors used widely in practice in terms of descriptiveness, robustness and efficiency.},
archivePrefix = {arXiv},
arxivId = {1802.02297},
author = {Hana, Xian-Feng and Jin, Jesse S. and Xie, Juan and Wang, Ming-Jie and Jiang, Wei},
eprint = {1802.02297},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hana et al/A comprehensive review of 3D point cloud descriptors. Hana et al.. 2018.pdf:pdf},
keywords = {3d point cloud,global-based descriptor,hybrid-based descriptor,local-based descriptor,performance evaluation},
title = {{A comprehensive review of 3D point cloud descriptors}},
url = {http://arxiv.org/abs/1802.02297},
year = {2018}
}
@article{Hakim2018,
abstract = {For safe and reliable driving, it is essential that an autonomous vehicle can accurately perceive the surrounding environment. Modern sensor technologies used for perception, such as LiDAR and RADA ...},
author = {Hakim, Ezeddin AL},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hakim/3D YOLO End-to-End 3D Object Detection Using Point Clouds. Hakim. 2018.pdf:pdf},
journal = {Degree Proj. Comput. Sci. Eng.},
title = {{3D YOLO: End-to-End 3D Object Detection Using Point Clouds}},
url = {http://www.nada.kth.se/{~}ann/exjobb/ezeddin{\_}alhakim.pdf},
year = {2018}
}
@article{Course,
author = {Course, Summer and Framework, R O S and Console, Byobu and Basics, Bash and Directories, Navigating and Basics, Command and Variables, Environment and Editor, Nano and Processes, Managing and Tools, Shell},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/GNU Linux and ROS Introduction. Course et al.pdf:pdf},
pages = {1--16},
title = {{GNU / Linux and ROS Introduction}}
}
@article{Brooker2007,
abstract = {This paper examines the probability that any millimeter-wave radar systems will interfere mutually by considering spatial, temporal, and operational frequency-related overlaps. It examines the nature and magnitude of the interference under different conditions and for different sensor types before concluding that in an overlapping frequency band, the probability that interference will occur is high. It goes on to demonstrate that, though there are some forms of interference that can be identified and controlled, there are others which are impossible to isolate, resulting in degraded target detection performance and tracking},
author = {Brooker, Graham M.},
doi = {10.1109/TEMC.2006.890223},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Brooker/Mutual interference of millimeter-wave radar systems. Brooker. 2007.pdf:pdf},
issn = {00189375},
journal = {IEEE Trans. Electromagn. Compat.},
keywords = {77 GHz,94 GHz,Electromagnetic induction (EMI),Frequency-modulated continuous wave (FMCW),Interference,Millimeter-wave radar},
number = {1},
pages = {170--181},
title = {{Mutual interference of millimeter-wave radar systems}},
volume = {49},
year = {2007}
}
@article{Fitzgibbon2003,
abstract = {This paper introduces a new method of registering point sets. The registration error is directly minimized using general-purpose non-linear optimization (the Levenberg-Marquardt algorithm). The surprising conclusion of the paper is that this technique is comparable in speed to the special-purpose Iterated Closest Point algorithm, which is most commonly used for this task. Because the routine directly minimizes an energy function, it is easy to extend it to incorporate robust estimation via a Huber kernel, yielding a basin of convergence that is many times wider than existing techniques. Finally, we introduce a data structure for the minimization based on the chamfer distance transform, which yields an algorithm that is both faster and more robust than previously described methods. {\textcopyright} 2003 Published by Elsevier B.V.},
author = {Fitzgibbon, Andrew W.},
doi = {10.1016/j.imavis.2003.09.004},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fitzgibbon/Robust registration of 2D and 3D point sets. Fitzgibbon. 2003.pdf:pdf},
issn = {02628856},
journal = {Image Vis. Comput.},
keywords = {Iterated Closest Point,Levenberg-Marquardt,Range image registration},
number = {13-14},
pages = {1145--1153},
title = {{Robust registration of 2D and 3D point sets}},
volume = {21},
year = {2003}
}
@article{Kim2015a,
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
doi = {10.1109/IVS.2015.7225724},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/Investigation on the occurrence of mutual interference between pulsed terrestrial LIDAR scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {9781467372664},
journal = {IEEE Intell. Veh. Symp. Proc.},
number = {Iv},
pages = {437--442},
publisher = {IEEE},
title = {{Investigation on the occurrence of mutual interference between pulsed terrestrial LIDAR scanners}},
volume = {2015-Augus},
year = {2015}
}
@article{Bimbraw2015,
author = {Bimbraw, K},
doi = {10.5220/0005540501910198},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Bimbraw/Autonomous Cars Past , Present and Future - A Review of the Developments in the Last Century , the Present .... Bimbraw. 2015.pdf:pdf},
isbn = {9789897581229},
journal = {12th Int. Conf. Informatics Control. Autom. Robot.},
keywords = {accomplished in this,and much has been,automation,automation is of interest,autonomous cars,autonomous vehicles,cars,intelligent transportation,mechatronics systems,technologies and systems,the field of autonomous,to researchers},
number = {August},
pages = {191--198},
title = {{Autonomous Cars : Past , Present and Future - A Review of the Developments in the Last Century , the Present ...}},
year = {2015}
}
@article{Giuggioli2015,
abstract = {Animal coordinated movement interactions are commonly explained by assuming unspecified social forces of attraction, repulsion and alignment with parameters drawn from observed movement data. Here we propose and test a biologically realistic and quantifiable biosonar movement interaction mechanism for echolocating bats based on spatial perceptual bias, i.e. actual sound field, a reaction delay, and observed motor constraints in speed and acceleration. We found that foraging pairs of bats flying over a water surface swapped leader-follower roles and performed chases or coordinated manoeuvres by copying the heading a nearby individual has had up to 500 ms earlier. Our proposed mechanism based on the interplay between sensory-motor constraints and delayed alignment was able to recreate the observed spatial actor-reactor patterns. Remarkably, when we varied model parameters (response delay, hearing threshold and echolocation directionality) beyond those observed in nature, the spatio-temporal interaction patterns created by the model only recreated the observed interactions, i.e. chases, and best matched the observed spatial patterns for just those response delays, hearing thresholds and echolocation directionalities found to be used by bats. This supports the validity of our sensory ecology approach of movement coordination, where interacting bats localise each other by active echolocation rather than eavesdropping.},
author = {Giuggioli, Luca and McKetterick, Thomas J. and Holderied, Marc},
doi = {10.1371/journal.pcbi.1004089},
editor = {Ayers, Joseph},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Giuggioli, McKetterick, Holderied/Delayed Response and Biosonar Perception Explain Movement Coordination in Trawling Bats. Giuggioli, McKetterick, Holderied. 2015.pdf:pdf},
issn = {1553-7358},
journal = {PLOS Comput. Biol.},
month = {mar},
number = {3},
pages = {e1004089},
title = {{Delayed Response and Biosonar Perception Explain Movement Coordination in Trawling Bats}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1004089},
volume = {11},
year = {2015}
}
@article{Wang2019,
abstract = {High precision 3D LiDARs are still expensive and hard to acquire. This paper presents the characteristics of RS-LiDAR, a model of low-cost LiDAR with sufficient supplies, in comparison with VLP-16. The paper also provides a set of evaluations to analyze the characterizations and performances of LiDARs sensors. This work analyzes multiple properties, such as drift effects, distance effects, color effects and sensor orientation effects, in the context of 3D perception. By comparing with Velodyne LiDAR, we found RS-LiDAR as a cheaper and acquirable substitute of VLP-16 with similar efficiency.},
archivePrefix = {arXiv},
arxivId = {arXiv:1709.07641v1},
author = {Wang, Zhe and Liu, Yang and Liao, Qinghai and Ye, Haoyang and Liu, Ming and Wang, Lujia},
doi = {10.1109/CYBER.2018.8688235},
eprint = {arXiv:1709.07641v1},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wang et al/Characterization of a RS-LiDAR for 3D Perception. Wang et al.. 2019.pdf:pdf},
isbn = {9781538670569},
journal = {8th Annu. IEEE Int. Conf. Cyber Technol. Autom. Control Intell. Syst. CYBER 2018},
pages = {564--569},
title = {{Characterization of a RS-LiDAR for 3D Perception}},
year = {2019}
}
@article{BCLiquorStores2013a,
abstract = {This document describes relevant characteristics of the Provisional Landsat 8 Surface Reflectance Climate Data Record to facilitate its use in the land remote sensing community. This document describes Top of Atmosphere Reflectance and Brightness Temperature derived from Landsat 8 Operational Land Imager (OLI) and the Thermal Infrared Sensor (TIRS). Surface Reflectance can be derived only for OLI data. Other processing options, such as spectral indices, format conversion, spatial subset, and/or coordinate system reprojection are described in other product guides.},
author = {{BC Liquor Stores}},
doi = {10.1080/1073161X.1994.10467258},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/BC Liquor Stores/Product Guide. BC Liquor Stores. 2013.pdf:pdf},
isbn = {5079468811201},
issn = {1073-161X},
journal = {Provisional Landsat 8 Surf. reflectance Prod.},
number = {October},
pages = {96},
pmid = {24474255},
title = {{Product Guide}},
volume = {Version 2.},
year = {2013}
}
@article{Liu2019,
abstract = {The extrinsic and intrinsic calibration of light detection and ranging (LiDAR) and inertial measurement unit (IMU) system is an essential prerequisite for its using in robots navigation or localization tasks. However, the existing LiDAR-IMU calibration method usually based on either point or planar features and existed adjustment parameter correlation limitations, which great restrict the extrinsic–intrinsic calibration flexibility and versatility. For this reason, a novel calibration technique that incorporates cone and cylinder features is proposed to overcome these drawbacks. The basic principle of our proposed method is that, first of all, we establish the transformation relationship between LiDAR and IMU coordinate frame, the LiDAR-IMU system calibration parameters and cone-cylinder geometric constrained. Secondly, the LiDAR extrinsic parameters are calibrated by estimating the pose of each scanned point datasets lies on the cone-cylinder surface and then solving the cone-cylinder geometric constrained optimization problem. Thirdly, the restricted maximum likelihood estimation (RMLE) algorithm is used to solve the optimization of IMU intrinsic parameters calibration Finally, intensive experimental studies are conducted to check the validity of our proposed method, the experimental results are presented that validate the proposed method and demonstrate the overall performances of LiDAR-IMU system obviously improved compared to plane based calibration method.},
author = {Liu, W. I. and Li, Yunwang},
doi = {10.1016/j.robot.2019.01.010},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Liu, Li/Error modeling and extrinsic–intrinsic calibration for LiDAR-IMU system based on cone-cylinder features. Liu, Li. 2019.pdf:pdf},
issn = {09218890},
journal = {Rob. Auton. Syst.},
keywords = {Cone-cylinder feature,Error modeling,Extrinsic–intrinsic calibration,LiDAR-IMU},
pages = {124--133},
publisher = {Elsevier B.V.},
title = {{Error modeling and extrinsic–intrinsic calibration for LiDAR-IMU system based on cone-cylinder features}},
url = {https://doi.org/10.1016/j.robot.2019.01.010},
volume = {114},
year = {2019}
}
@article{Sui2017,
abstract = {Evidence of a link between recurrent affective disorders and the hypothalamic-pituitary-thyroid (HPT) axis has been frequently reported. The clinical course of 40 recurrent major depressive patients was monitored over 2 years with respect to several parameters of thyroid function. Patients entering a depressive recurrence manifested lower free versus bound thyroxine quotients and had higher TSH secretion. A dynamic HPT axis response may enhance the clinical outcome of acute depression during continuation pharmacotherapy. (J. Clin. Psychiatry 46:267-272, 1985).},
author = {Sui, Jingfeng and Wang, Shuo},
doi = {10.23919/ChiCC.2017.8028441},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Sui, Wang/Extrinsic calibration of camera and 3D laser sensor system. Sui, Wang. 2017.pdf:pdf},
isbn = {9789881563934},
issn = {21612927},
journal = {Chinese Control Conf. CCC},
keywords = {edges extraction,extrinsic calibration,sensor fusion},
pages = {6881--6886},
title = {{Extrinsic calibration of camera and 3D laser sensor system}},
year = {2017}
}
@article{Cheng2018,
abstract = {We develop an unprecedented 3D pulsed chaos lidar system for potential intelligent machinery applications. Benefited from the random nature of the chaos, conventional CW chaos lidars already possess excellent anti-jamming and anti-interference capabilities and have no range ambiguity. In our system, we further employ self-homodyning and time gating to generate a pulsed homodyned chaos to boost the energy-utilization efficiency. Compared to the original chaos, we show that the pulsed homodyned chaos improves the detection SNR by more than 20 dB. With a sampling rate of just 1.25 GS/s that has a native sampling spacing of 12 cm, we successfully achieve millimeter-level accuracy and precision in ranging. Compared with two commercial lidars tested side-by-side, namely the pulsed Spectroscan and the random-modulation continuous-wave Lidar-lite, the pulsed chaos lidar that is in compliance with the class-1 eye-safe regulation shows significantly better precision and a much longer detection range up to 100 m. Moreover, by employing a 2-axis MEMS mirror for active laser scanning, we also demonstrate real-time 3D imaging with errors of less than 4 mm in depth.},
author = {Cheng, Chih-Hao and Chen, Chih-Ying and Chen, Jun-Da and Pan, Da-Kung and Ting, Kai-Ting and Lin, Fan-Yi},
doi = {10.1364/oe.26.012230},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Cheng et al/3D pulsed chaos lidar system. Cheng et al.. 2018.pdf:pdf},
journal = {Opt. Express},
number = {9},
pages = {12230},
title = {{3D pulsed chaos lidar system}},
volume = {26},
year = {2018}
}
@misc{CameronOliver2017,
abstract = {Why did LIDAR take off with self-driving cars? In a word: mapping. LIDAR allows you to generate huge 3D maps (its original application!), which you can then navigate the car or robot predictably within. By using a LIDAR to map and navigate an environment, you can know ahead of time the bounds of a lane, or that there is a stop sign or traffic light 500m ahead. This kind of predictability is exactly what a technology like self-driving cars requires, and has been a big reason for the progress over the last 5 years.
},
author = {{Cameron Oliver}},
booktitle = {Voyage},
keywords = {LIDAR,Self-Driving Cars},
mendeley-tags = {LIDAR,Self-Driving Cars},
title = {{An Introduction to LIDAR: The Key Self-Driving Car Sensor}},
url = {https://news.voyage.auto/an-introduction-to-lidar-the-key-self-driving-car-sensor-a7e405590cff},
urldate = {2018-10-09},
year = {2017}
}
@article{Mccoy2015,
abstract = {In this critical multisite case study we examined the concept of colorblind mentoring. Using Bonilla-Silva's Colorblind Racism Frames, we sought to understand White faculty members' perspectives on their mentoring of Students of Color. The findings revealed that White faculty members often engage with students from a " colorblind perspective. " Their use of race-neutral, colorblind language (avoiding racial terms but implying them) allowed White faculty members to describe their students as academ-ically inferior, less prepared, and less interested in pursuing research and graduate studies while potentially ignoring structural causes. Faculty perceptions of students may influence the way Students of Color perceive their academic abilities and potential to achieve success in STEM disciplines and in graduate education.},
author = {Mccoy, Dorian L and Winkle-wagner, Rachelle and Luedke, Courtney L},
doi = {10.1037/a0038676},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Mccoy, Winkle-wagner, Luedke/D c 1 2. Mccoy, Winkle-wagner, Luedke. 2015.pdf:pdf},
isbn = {1938-8926},
issn = {19388926},
keywords = {2007,colorblind racism,critically important for students,evidenced to be,faculty mentoring has been,gardner,graduate programs,herzig,mentoring,students of color,success in},
number = {4},
pages = {2},
title = {{D c 1 2}},
volume = {8},
year = {2015}
}
@article{RodriguezF.2008,
abstract = {In this paper, we present a novel approach for solving the extrinsic calibration between a camera and a multi-layer laser range finder. Our approach is oriented for intelligent vehicle applications, where the separation distance between sensors frames are frequently very important. For this purpose, we use a circle-based calibration object because its geometry allows us to obtain not only an accurate estimation pose by taking advantage of the 3D multi-layer laser range finder perception but also a simultaneous estimation of the pose in the camera frame and the camera intrinsic parameters. These advantages simplify the calibration task in outdoor environments. The method determines the relative position of the sensors by estimating sets of corresponded features and by solving the classical absolute orientation problem. The proposed method is evaluated by using different synthetics environments and real data. An error propagation analysis is made in order to estimate the calibration accuracy and the confidence intervals. Finally, we present a laser data projection into images to validate the consistency of the results. {\textcopyright}2008 IEEE.},
author = {{Rodriguez F.}, Sergio A. and Fr{\'{e}}mont, Vincent and Bonnifait, Philippe},
doi = {10.1109/MFI.2008.4648067},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rodriguez F., Fr{\'{e}}mont, Bonnifait/Extrinsic calibration between a multi-layer lidar and a camera. Rodriguez F., Fr{\'{e}}mont, Bonnifait. 2008.pdf:pdf},
journal = {IEEE Int. Conf. Multisens. Fusion Integr. Intell. Syst.},
keywords = {Circle based calibration target,Extrinsic calibration,Multi-sensor system},
number = {September},
pages = {214--219},
title = {{Extrinsic calibration between a multi-layer lidar and a camera}},
year = {2008}
}
@book{Kane,
abstract = {In experimental psychology different experiments have been developed to assess goal{\{}$\backslash$textendash{\}}directed as compared to habitual control over instrumental decisions. Similar to animal studies selective devaluation procedures have been used. More recently sequential decision-making tasks have been designed to assess the degree of goal-directed versus habitual choice behavior in terms of an influential computational theory of model-based compared to model-free behavioral control. As recently suggested, different measurements are thought to reflect the same construct. Yet, there has been no attempt to directly assess the construct validity of these different measurements. In the present study, we used a devaluation paradigm and a sequential decision-making task to address this question of construct validity in a sample of 18 healthy male human participants. Correlational analysis revealed a positive association between model-based choices during sequential decisions and goal-directed behavior after devaluation suggesting a single framework underlying both operationalizations and speaking in favour of construct validity of both measurement approaches. Up to now, this has been merely assumed but never been directly tested in humans.},
author = {Kane, Jason M O},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kane/A Gentle Introduction to ROS. Kane. 2016.pdf:pdf},
isbn = {9781492143239},
keywords = {()},
pages = {166},
title = {{A Gentle Introduction to ROS}},
year = {2016}
}
@article{Elias2014,
author = {Elias, Rimon and Elias, Rimon},
doi = {10.1007/978-3-319-05137-6_5},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Elias, Elias/Transformations in 3D Space. Elias, Elias. 2014.pdf:pdf},
journal = {Digit. Media},
pages = {143--197},
title = {{Transformations in 3D Space}},
year = {2014}
}
@article{Kidd2016,
abstract = {Special collections resources are distinguishable by the label we affix to their spine as tvelJ as the{\"{a}}r "resource list" assignation in the OPAC. [...] students can readily access these collections whether online or physically browsing the shelves. [...] at the end of the quarter they review their selections, thus growing the inventory of reviewed resources in the collection.},
author = {Kidd, John and Pe, Shachak and Eren, Firat and Armstrong, Andrew},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kidd et al/Performance evaluation of the Velodyne VLP-16 system for surface feature surveying. Kidd et al.. 2016.pdf:pdf},
isbn = {14811782},
journal = {Can. Hydrogr. Conf.},
pages = {1--10},
title = {{Performance evaluation of the Velodyne VLP-16 system for surface feature surveying}},
year = {2016}
}
@article{Castorena2016,
abstract = {We present a new method for joint automatic extrinsic calibration and sensor fusion for a multimodal sensor system comprising a LIDAR and an optical camera. Our approach exploits the natural alignment of depth and intensity edges when the calibration parameters are correct. Thus, in contrast to a number of existing approaches, we do not require the presence or identification of known alignment targets. On the other hand, the characteristics of each sensor modality, such as sampling pattern and information measured, are significantly different, making direct edge alignment difficult. To overcome this difficulty, we jointly fuse the data and estimate the calibration parameters. In particular, the joint processing evaluates and optimizes both the quality of edge alignment and the performance of the fusion algorithm using a common cost function on the output. We demonstrate accurate calibration in practical configurations in which depth measurements are sparse and contain no reflectivity information. Experiments on synthetic and real data obtained with a three-dimensional LIDAR sensor demonstrate the effectiveness of our approach.},
author = {Castorena, Juan and Kamilov, Ulugbek S. and Boufounos, Petros T.},
doi = {10.1109/ICASSP.2016.7472200},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Autocalibration of lidar and optical cameras via edge alignment. Castorena, Kamilov, Boufounos.pdf:pdf},
isbn = {9781479999880},
issn = {15206149},
journal = {ICASSP, IEEE Int. Conf. Acoust. Speech Signal Process. - Proc.},
keywords = {Multimodal calibration,depth superresolution,intersensor registration,sensor fusion,total variation},
pages = {2862--2866},
publisher = {IEEE},
title = {{Autocalibration of lidar and optical cameras via edge alignment}},
volume = {2016-May},
year = {2016}
}
@inproceedings{Sulkowski2018,
annote = {Problema: para se obter carros que consigam atingir uma autonomia de n{\'{i}}vel 4 ou 5, {\'{e}} necess{\'{a}}rio que a Intelig{\^{e}}ncia Artificial do carro possua uma quantidade de dados para teste gigante. 

Em compara{\c{c}}{\~{a}}o, uma frota de 20 carros aut{\'{o}}nomos (demasiados) apenas consegue fazer 1 milh{\~{a}}o de milhas por ano. 

Solu{\c{c}}{\~{a}}o: Testar os algoritmos dentro do simulador que emula o mundo real. Se o simulador for bom o suficiente, {\'{e}} poss{\'{i}}vel criar dados que possam servir para treinar os algoritmos, sendo poss{\'{i}}vel gerar "mais horas de condu{\c{c}}{\~{a}}o" para os algoritmos},
author = {Sulkowski, Tomasz and Bugiel, Paulina and Izydorczyk, Jacek},
booktitle = {2018 Int. Conf. Signals Electron. Syst.},
doi = {10.1109/ICSES.2018.8507288},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/In Search of the Ultimate Autonomous Driving Simulator. Sulkowski, Bugiel, Izydorczyk.pdf:pdf},
isbn = {978-1-5386-6768-2},
keywords = {Autonomous driving,Closed loop,Sensor emulation,Simulator},
month = {sep},
pages = {252--256},
publisher = {IEEE},
title = {{In Search of the Ultimate Autonomous Driving Simulator}},
url = {https://ieeexplore.ieee.org/document/8507288/},
year = {2018}
}
@article{Visionc,
author = {Vision, Allied and Line, Camera},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Vision, Line/High-performance machine vision cameras Better vision for your application. Vision, Line. Unknown(2).pdf:pdf},
title = {{High-performance machine vision cameras Better vision for your application}}
}
@article{Steder2010a,
abstract = {We present our findings regarding a novel method for interest point detection and feature descriptor calculation in 3D range data called NARF (Normal Aligned Radial Feature). The method makes explicit use of object boundary information and tries to extract the features in areas where the surface is stable but has substantial change in the vicinity.},
author = {Steder, Bastian and Konolige, Kurt},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Steder, Konolige/NARF 3D Range Image Features for Object Recognition. Steder, Konolige. 2010.pdf:pdf},
journal = {October},
pages = {11},
pmid = {17116253},
title = {{NARF : 3D Range Image Features for Object Recognition}},
url = {http://ais.informatik.uni-freiburg.de/publications/papers/steder10irosws.pdf},
volume = {215},
year = {2010}
}
@article{Package2019,
author = {Package, R O S},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Package/1. Creating a ROS Package. Package. 2019.pdf:pdf},
pages = {4--7},
title = {{1. Creating a ROS Package}},
year = {2019}
}
@article{Scaramuzza,
author = {Scaramuzza, Davide and Harati, Ahad and Siegwart, Roland},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Scaramuzza, Harati, Siegwart/Extrinsic self calibration of a camera and a 3D Laser Range Finder. Scaramuzza, Harati, Siegwart. Unknown.pdf:pdf},
keywords = {camera calibration,computer vision,laser calibration,omndirectional vision,omnidirectional camera,p3p algorithm,pnp algorithm,reprojection error,robotics,self calibration},
title = {{Extrinsic self calibration of a camera and a 3D Laser Range Finder}}
}
@book{1385,
author = {غلامحسین, ثنایی},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Multiple View Geometry in computer vision. غلامحسین.pdf:pdf},
isbn = {9780521540513},
pages = {302},
title = {{Multiple View Geometry in computer vision}},
year = {1385}
}
@article{Fersch2017,
abstract = {{\textcopyright} 2017 IEEE. We present the application of optical code division multiple access (OCDMA) modulation based on optical orthogonal codes for automotive time-of-flight light detection and ranging (LiDAR) system sensors using avalanche photodiode (APD) detectors. The modulation opens the possibility to discriminate single laser transmissions. This allows the realization of additional features like enhancing the systems' interference robustness or accelerating their scanning rate. The requirements on automotive LiDAR OCDMA modulation differs from telecommunication's demands in several ways: The sensor must guarantee absolute laser safety; front facing devices must be able to provide reliable long range results up to a distance of 150m and beyond; and the signal is transmitted through free space. After outlining the basic functionality of the sort of sensors in consideration, the properties of the optical orthogonal codes (OOC) modulation are compared with the state of the art theoretically. The influence of OOC parameters with respect to scanning LiDAR systems is examined. The modulation technique is then demonstrated experimentally with two examples: The detection and separation of coded and traditional signals is shown using a matched filter detection algorithm. In the same way, differently coded signals can be separated from each other. Impediments of the interference suppression quality are discussed. Finally, an overview of possible applications of the proposed technique in automotive LiDAR systems is given, which are enabled by the OCDMA modulation in the first place.},
annote = {Solu{\c{c}}{\~{a}}o f{\'{i}}sica para o problema da interfer{\^{e}}ncia entre LIDARs que funcionam por TOF, usando Optical Code Division Multiple Access (OCDMA), atrav{\'{e}}s de Optical Orthogonal Codes (OOC), com On-Off Keying (OOK)},
author = {Fersch, Thomas and Weigel, Robert and Koelpin, Alexander},
doi = {10.1109/JSEN.2017.2688126},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems. Fersch, Weigel, Koelpin.pdf:pdf},
issn = {1530-437X},
journal = {IEEE Sens. J.},
keywords = {Laser radar,LiDAR,OCDMA modulation,time-of-flight},
month = {jun},
number = {11},
pages = {3507--3516},
title = {{A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems}},
url = {http://ieeexplore.ieee.org/document/7887680/},
volume = {17},
year = {2017}
}
@article{Taylor2013,
abstract = {This paper is about automatic calibration of a camera-lidar system. The method presented is designed to be as general as possible allowing it to be used in a large range of systems and applications. The approach uses normalized mutual information to compare camera images with lidar scans of the same area. A camera model that takes into account orientation, location and focal length is used to create a 2D lidar image, with the intensity of the pixels representing a feature of the lidar scan that is chosen depending on the application. Particle swarm optimization is used to find the optimal model parameters. The method presented is successfully validated on a variety of cameras, lidars and locations, including scans of both urban and natural environments},
author = {Taylor, Zachary and Nieto, Juan},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Automatic Calibration of Lidar and Camera Images using Normalized Mutual Information. Taylor, Nieto.pdf:pdf},
journal = {IEEE Int. Conf. Robot. Autom.},
title = {{Automatic Calibration of Lidar and Camera Images using Normalized Mutual Information}},
url = {https://pdfs.semanticscholar.org/fc08/d41be320a827c3f045c4429e13495da5a1cc.pdf{\%}0Ahttp://www-personal.acfr.usyd.edu.au/jnieto/Publications{\_}files/TaylorICRA2013.pdf},
year = {2013}
}
@book{Borenstein2012,
abstract = {This detailed, hands-on guide provides the technical and conceptual information you need to build cool applications with Microsoft's Kinect, the amazing motion-sensing device that enables computers to see. Through half a dozen meaty projects, you'll learn how to create gestural interfaces for software, use motion capture for easy 3D character animation, 3D scanning for custom fabrication, and many other applications. Perfect for hobbyists, makers, artists, and gamers, Making Things See shows you how to build every project with inexpensive off-the-shelf components, including the open source Processing programming language and the Arduino microcontroller. You'll learn basic skills that will enable you to pursue your own creative applications with Kinect.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Borenstein, Greg},
booktitle = {O'Reilly Media},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Borenstein/Making Things See 3D Vision with Kinect, Processing, Arduino, and MakerBot. Borenstein. 2012.pdf:pdf},
isbn = {978-1-44930-707-3},
keywords = {icle},
pages = {440},
title = {{Making Things See: 3D Vision with Kinect, Processing, Arduino, and MakerBot}},
year = {2012}
}
@article{Classes,
abstract = {Matrix inversion procedures in minivar},
author = {Classes, Key Opencv and Access, Part and Basics, Matrix},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Classes, Access, Basics/OpenCV 2.4 Cheat Sheet (C). Classes, Access, Basics. Unknown.pdf:pdf},
pages = {2--3},
title = {{OpenCV 2.4 Cheat Sheet (C++)}}
}
@article{Rusu2010,
abstract = {Environment models serve as important resources for an autonomous robot by providing it with the neces- sary task-relevant information about its habitat. Their use enables robots to perform their tasks more reliably, flexibly, and efficiently. As autonomous robotic platforms get more sophisticated manipulation capabilities, they also need more expressive and comprehensive environment models: forma- nipulation purposes their models have to include the objects present in the world, together with their position, form, and other aspects, as well as an interpretation of these objects with respect to the robot tasks. The dissertation presented in this article (Rusu, PhD the- sis, 2009) proposes Semantic 3D Object Models as a novel representation of the robot's operating environment that sat- isfies these requirements and shows how these models can be automatically acquired from dense 3D range data.},
author = {Rusu, Radu Bogdan},
doi = {10.1007/s13218-010-0059-6},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rusu/Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments. Rusu. 2010.pdf:pdf},
issn = {0933-1875},
journal = {KI - K{\"{u}}nstliche Intelligenz},
number = {4},
pages = {345--348},
title = {{Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments}},
volume = {24},
year = {2010}
}
@article{Redmon2016,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02640v5},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {arXiv:1506.02640v5},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Redmon et al/You only look once Unified, real-time object detection. Redmon et al.. 2016.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {779--788},
title = {{You only look once: Unified, real-time object detection}},
volume = {2016-Decem},
year = {2016}
}
@article{Alyamkin2019,
abstract = {Computer vision has achieved impressive progress in recent years. Meanwhile, mobile phones have become the primary computing platforms for millions of people. In addition to mobile phones, many autonomous systems rely on visual data for making decisions and some of these systems have limited energy (such as unmanned aerial vehicles also called drones and mobile robots). These systems rely on batteries and energy efficiency is critical. This article serves two main purposes: (1) Examine the state-of-the-art for low-power solutions to detect objects in images. Since 2015, the IEEE Annual International Low-Power Image Recognition Challenge (LPIRC) has been held to identify the most energy-efficient computer vision solutions. This article summarizes 2018 winners' solutions. (2) Suggest directions for research as well as opportunities for low-power computer vision.},
author = {Alyamkin, Sergei and Ardi, Matthew and Berg, Alexander C. and Brighton, Achille and Chen, Bo and Chen, Yiran and Cheng, Hsin Pai and Fan, Zichen and Feng, Chen and Fu, Bo and Gauen, Kent and Goel, Abhinav and Goncharenko, Alexander and Guo, Xuyang and Ha, Soonhoi and Howard, Andrew and Hu, Xiao and Huang, Yuanjun and Kang, Donghyun and Kim, Jaeyoun and Ko, Jong Gook and Kondratyev, Alexander and Lee, Junhyeok and Lee, Seungjae and Lee, Suwoong and Li, Zichao and Liang, Zhiyu and Liu, Juzheng and Liu, Xin and Lu, Yang and Lu, Yung Hsiang and Malik, Deeptanshu and Nguyen, Hong Hanh and Park, Eunbyung and Repin, Denis and Shen, Liang and Sheng, Tao and Sun, Fei and Svitov, David and Thiruvathukal, George K. and Zhang, Baiwu and Zhang, Jingchi and Zhang, Xiaopeng and Zhuo, Shaojie},
doi = {10.1109/JETCAS.2019.2911899},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Alyamkin et al/Low-Power Computer Vision Status, Challenges, and Opportunities. Alyamkin et al.. 2019.pdf:pdf},
issn = {21563357},
journal = {IEEE J. Emerg. Sel. Top. Circuits Syst.},
keywords = {Computer vision,low-power electronics,machine intelligence,object detection},
number = {2},
pages = {411--421},
title = {{Low-Power Computer Vision: Status, Challenges, and Opportunities}},
volume = {9},
year = {2019}
}
@article{Challis1995,
abstract = {For many biomechanical applications it is necessary to determine the parameters which describe the transformation of a rigid body from one reference frame to another. These parameters are a scaling factor, an attitude matrix, and a translation vector. The paper presents a new procedure for the determination of these parameters incorporating the work of Arun et al. [IEEE Trans. Pattern Anal. Machine Intell, 9, 698-700 (1987)] but expanding their analysis to allow for the determination of a scale factor, the scalar weighting of the least-squares problem, and the problem of obtaining the incorrect determinant when determining the attitude matrix. The procedure, which requires the coordinates of three or more noncollinear points, is based around the singular value decomposition, and provides a least-squares estimate of the rigid body transformation parameters. Examples are presented of the use of this procedure for determining the attitude of a rigid body, and for osteometric scaling. When used for osteometric scaling mirror transformations are possible, therefore a right-hand specimen can be scaled to the left-hand side of another specimen. {\textcopyright} 1995.},
author = {Challis, John H.},
doi = {10.1016/0021-9290(94)00116-L},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Challis/A procedure for determining rigid body transformation parameters. Challis. 1995.pdf:pdf},
issn = {00219290},
journal = {J. Biomech.},
number = {6},
pages = {733--737},
title = {{A procedure for determining rigid body transformation parameters}},
volume = {28},
year = {1995}
}
@article{Lisca2010,
abstract = {This paper describes a new extrinsic calibration method to estimate the extrinsic parameters of a multi layer laser scanner relative to a stereo camera. Unlike the existing methods, this method has two main advantages: it is automatic and single stepped. The user imprecise selections of calibration features are avoided by using the features of a special designed object as calibration features. The extraction and matching of calibration object features are accurate and automatic. Non coplanar distribution of calibration object features introduces strong constraints on extrinsic parameters. By using these constraints the computation of extrinsic parameters is simultaneously done in a single step. In addition the assumptions on extrinsic parameters and error minimization steps required by existing methods are avoided. The proposed method is tested and the obtained results prove its accuracy.},
author = {Lişca, Gheorghe and Jeong, Pangyu and Nedevschi, Sergiu},
doi = {10.1109/ICCP.2010.5606434},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lişca, Jeong, Nedevschi/Automatic one step extrinsic calibration of a multi layer laser scanner relative to a stereo camera. Lişca, Jeong, Nedevschi. 2010.pdf:pdf},
isbn = {9781424482306},
journal = {Proc. - 2010 IEEE 6th Int. Conf. Intell. Comput. Commun. Process. ICCP10},
keywords = {Extrinsic calibration,Multilayer laser scanner,Stereo camera},
pages = {223--230},
title = {{Automatic one step extrinsic calibration of a multi layer laser scanner relative to a stereo camera}},
year = {2010}
}
@article{Garage2018,
author = {Garage, Willow},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Garage/TF (transform) in ROS ECET 4990058100 Credit PhD comics and Willow Garage. Garage. 2018.pdf:pdf},
isbn = {49900/58100},
pages = {1--7},
title = {{TF (transform) in ROS ECET 49900/58100 Credit: PhD comics and Willow Garage}},
year = {2018}
}
@article{Kidd2016a,
abstract = {Special collections resources are distinguishable by the label we affix to their spine as tvelJ as the{\"{a}}r "resource list" assignation in the OPAC. [...] students can readily access these collections whether online or physically browsing the shelves. [...] at the end of the quarter they review their selections, thus growing the inventory of reviewed resources in the collection.},
author = {Kidd, John and Pe, Shachak and Eren, Firat and Armstrong, Andrew},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kidd et al/Performance evaluation of the Velodyne VLP-16 system for surface feature surveying. Kidd et al.. 2016(2).pdf:pdf},
isbn = {14811782},
journal = {Can. Hydrogr. Conf.},
pages = {1--10},
title = {{Performance evaluation of the Velodyne VLP-16 system for surface feature surveying}},
year = {2016}
}
@article{Iyer2018,
abstract = {3D LiDARs and 2D cameras are increasingly being used alongside each other in sensor rigs for perception tasks. Before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. A vast majority of existing calibration techniques require significant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. We address this gap with CalibNet: a self-supervised deep network capable of automatically estimating the 6-DoF rigid body transformation between a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need for calibration targets, thereby resulting in significant savings in calibration efforts. During training, the network only takes as input a LiDAR point cloud, the corresponding monocular image, and the camera calibration matrix K. At train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration parameters, for example). Instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. CalibNet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration parameters for a wide range of mis-calibrations, without requiring retraining or domain adaptation. The project page is hosted at https://epiception.github.io/CalibNet},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.08181v1},
author = {Iyer, Ganesh and Ram, R. Karnik and Murthy, J. Krishna and Krishna, K. Madhava},
doi = {10.1109/IROS.2018.8593693},
eprint = {arXiv:1803.08181v1},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Iyer et al/CalibNet Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks. Iyer et al.. 2018.pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
pages = {1110--1117},
title = {{CalibNet: Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks}},
year = {2018}
}
@book{MorganQuigleyBrianGerkey2015,
author = {{Morgan Quigley, Brian Gerkey}, William D. Smart},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Morgan Quigley, Brian Gerkey/Robots with ROS. Morgan Quigley, Brian Gerkey. 2015.pdf:pdf},
isbn = {9781449323899},
keywords = {www.it-ebooks.info},
pages = {447},
title = {{Robots with ROS}},
year = {2015}
}
@article{GmbH2016b,
author = {GmbH, Allied Vision Technologies},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/GmbH/Manta Technical Manual Manta at a glance. GmbH. 2016(2).pdf:pdf},
isbn = {3642867723},
title = {{Manta Technical Manual Manta at a glance}},
url = {http://dv.ujaen.es/goto{\_}docencia{\_}file{\_}769494{\_}download.html},
year = {2016}
}
@article{Wei2018,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@article{Hassanein2016,
abstract = {In the last few years, multi-cameras and LIDAR systems draw the attention of the mapping community. They have been deployed on different mobile mapping platforms. The different uses of these platforms, especially the UAVs, offered new applications and developments which require fast and accurate results. The successful calibration of such systems is a key factor to achieve accurate results and for the successful processing of the system measurements especially with the different types of measurements provided by the LIDAR and the cameras. The system calibration aims to estimate the geometric relationships between the different system components. A number of applications require the systems be ready for operation in a short time especially for disasters monitoring applications. Also, many of the present system calibration techniques are constrained with the need of special arrangements in labs for the calibration procedures. In this paper, a new technique for calibration of integrated LIDAR and multi-cameras systems is presented. The new proposed technique offers a calibration solution that overcomes the need for special labs for standard calibration procedures. In the proposed technique, 3D reconstruction of automatically detected and matched image points is used to generate a sparse images-driven point cloud then, a registration between the LIDAR generated 3D point cloud and the images-driven 3D point takes place to estimate the geometric relationships between the cameras and the LIDAR.. In the presented technique a simple 3D artificial target is used to simplify the lab requirements for the calibration procedure. The used target is composed of three intersected plates. The choice of such target geometry was to ensure enough conditions for the convergence of registration between the constructed 3D point clouds from the two systems. The achieved results of the proposed approach prove its ability to provide an adequate and fully automated calibration without sophisticated calibration arrangement requirements. The proposed technique introduces high potential for system calibration for many applications especially those with critical logistic and time constraints such as in disaster monitoring applications.},
author = {Hassanein, M. and Moussa, A. and El-Sheimy, N.},
doi = {10.5194/isprsarchives-XLI-B1-589-2016},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hassanein, Moussa, El-Sheimy/A new automatic system calibration of multi-cameras and lidar sensors. Hassanein, Moussa, El-Sheimy. 2016.pdf:pdf},
issn = {16821750},
journal = {Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. - ISPRS Arch.},
keywords = {LIDAR,Multi-camera,Point cloud,System calibration},
number = {July},
pages = {589--594},
title = {{A new automatic system calibration of multi-cameras and lidar sensors}},
volume = {2016-Janua},
year = {2016}
}
@article{Steder,
author = {Steder, Bastian},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/The Point Cloud Library PCL Thanks to Radu Rusu from Willow Garage for some of the slides!. Steder.pdf:pdf},
title = {{The Point Cloud Library PCL Thanks to Radu Rusu from Willow Garage for some of the slides!}},
url = {http://ais.informatik.uni-freiburg.de/teaching/ws10/robotics2/pdfs/rob2-12-ros-pcl.pdf}
}
@article{DeSilva2018,
abstract = {Autonomous robots that assist humans in day to day living tasks are becoming increasingly popular. Autonomous mobile robots operate by sensing and perceiving their surrounding environment to make accurate driving decisions. A combination of several different sensors such as LiDAR, radar, ultrasound sensors and cameras are utilized to sense the surrounding environment of autonomous vehicles. These heterogeneous sensors simultaneously capture various physical attributes of the environment. Such multimodality and redundancy of sensing need to be positively utilized for reliable and consistent perception of the environment through sensor data fusion. However, these multimodal sensor data streams are different from each other in many ways, such as temporal and spatial resolution, data format, and geometric alignment. For the subsequent perception algorithms to utilize the diversity offered by multimodal sensing, the data streams need to be spatially, geometrically and temporally aligned with each other. In this paper, we address the problem of fusing the outputs of a Light Detection and Ranging (LiDAR) scanner and a wide-angle monocular image sensor for free space detection. The outputs of LiDAR scanner and the image sensor are of different spatial resolutions and need to be aligned with each other. A geometrical model is used to spatially align the two sensor outputs, followed by a Gaussian Process (GP) regression-based resolution matching algorithm to interpolate the missing data with quantifiable uncertainty. The results indicate that the proposed sensor data fusion framework significantly aids the subsequent perception steps, as illustrated by the performance improvement of a uncertainty aware free space detection algorithm},
archivePrefix = {arXiv},
arxivId = {1710.06230},
author = {{De Silva}, Varuna and Roche, Jamie and Kondoz, Ahmet},
doi = {10.3390/s18082730},
eprint = {1710.06230},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Robust fusion of LiDAR and wide-angle camera data for autonomous mobile robots. De Silva, Roche, Kondoz.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Assistive robots,Autonomous vehicles,Depth sensing,Free space detection,Gaussian process regression,LiDAR,Sensor data fusion},
number = {8},
title = {{Robust fusion of LiDAR and wide-angle camera data for autonomous mobile robots}},
volume = {18},
year = {2018}
}
@article{Geiger2013a,
abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high- resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to inner- city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide},
author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger et al/Vision meets robotics The KITTI dataset. The International Journal of Robotics Research. Geiger et al.. 2013.pdf:pdf},
journal = {Ijrr},
number = {October},
pages = {1--6},
title = {{Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research}},
year = {2013}
}
@article{GmbH2016,
author = {GmbH, Allied Vision Technologies},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/GmbH/Manta Technical Manual Manta at a glance. GmbH. 2016.pdf:pdf},
isbn = {3642867723},
title = {{Manta Technical Manual Manta at a glance}},
url = {http://dv.ujaen.es/goto{\_}docencia{\_}file{\_}769494{\_}download.html},
year = {2016}
}
@article{Wang,
abstract = {In this paper, we present a technique that places 3D bounding boxes around objects in an RGB-D scene. Our approach makes best use of the 2D information to quickly reduce the search space in 3D, benefiting from state-of-the-art 2D object detection techniques. We then use the 3D information to orient, place, and score bounding boxes around objects. We independently estimate the orienta-tion for every object, using previous techniques that utilize normal information. Object locations and sizes in 3D are learned using a multilayer perceptron (MLP). In the final step, we refine our detections based on object class relations within a scene. When compared to state-of-the-art detection methods that operate almost entirely in the sparse 3D do-main, extensive experiments on the well-known SUN RGB-D dataset [29] show that our proposed method is much faster (4.1s per image) in detecting 3D objects in RGB-D images and performs better (3 mAP higher) than the state-of-the-art method that is 4.7 times slower and comparably to the method that is two orders of magnitude slower. This work hints at the idea that 2D-driven object detection in 3D should be further explored, especially in cases where the 3D input is sparse.},
author = {Wang, Ye},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wang/3D Object Detection in RGB-­D Images. Wang. Unknown.pdf:pdf},
title = {{3D Object Detection in RGB-­D Images}},
url = {http://openaccess.thecvf.com/content{\_}ICCV{\_}2017/papers/Lahoud{\_}2D-Driven{\_}3D{\_}Object{\_}ICCV{\_}2017{\_}paper.pdf}
}
@article{Visionb,
author = {Vision, Allied},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Vision/Installing Vimba under Linux Changing the IP configuration in a foreign subnet Compiling the programming examples and the C API. Vision.pdf:pdf},
pages = {1--2},
title = {{Installing Vimba under Linux Changing the IP configuration in a foreign subnet Compiling the programming examples and the C ++ API}}
}
@article{Leaflet2018,
author = {Leaflet, Instruction},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Leaflet/I O cables. Leaflet. 2018.pdf:pdf},
pages = {12--13},
title = {{I / O cables}},
year = {2018}
}
@article{Gokhale2013,
author = {Gokhale, Rashmi and Kumar, R},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gokhale, Kumar/Analysis of Three Dimensional Object Reconstruction Method for Underwater Images. Gokhale, Kumar. 2013.pdf:pdf},
journal = {Ijstr.Org},
number = {6},
pages = {85--88},
title = {{Analysis of Three Dimensional Object Reconstruction Method for Underwater Images}},
url = {http://www.ijstr.org/final-print/june2013/Analysis-Of-Three-Dimensional-Object-Reconstruction-Method-For-Underwater-Images.pdf},
volume = {2},
year = {2013}
}
@techreport{Rev,
address = {San Jose},
author = {Rev, E},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rev/VLP-16 User Manual. Rev. 2019.pdf:pdf},
institution = {Velodyne LiDAR, Inc.},
pages = {71},
title = {{VLP-16 User Manual}},
year = {2019}
}
@article{Kim2017,
author = {Kim, Gunzung and Eom, Jeongsook and Choi, Jeonghee and Park, Yongwan},
doi = {10.14372/iemek.2017.12.1.43},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Mutual Interference on Mobile Pulsed Scanning LIDAR. Kim et al.. 2017.pdf:pdf},
issn = {1975-5066},
journal = {IEMEK J. Embed. Syst. Appl.},
month = {feb},
number = {1},
pages = {43--62},
title = {{Mutual Interference on Mobile Pulsed Scanning LIDAR}},
url = {http://koreascience.or.kr/journal/view.jsp?kj=OBDDBE{\&}py=2017{\&}vnc=v12n1{\&}sp=43},
volume = {12},
year = {2017}
}
@article{Lepetit2009,
abstract = {We propose a non-iterative solution to the PnP problem—the estimation of the pose of a calibrated camera from n 3D-to-2D point correspondences—whose computational complexity grows linearly with n. This is in contrast to state-of-the-art methods that are O(n 5) or even O(n 8), without being more accurate. Our method is applicable for all n≥4 and handles properly both planar and non-planar configurations. Our central idea is to express the n 3D points as a weighted sum of four virtual control points. The problem then reduces to estimating the coordinates of these control points in the camera referential, which can be done in O(n) time by expressing these coordinates as weighted sum of the eigenvectors of a 12×12 matrix and solving a small constant number of quadratic equations to pick the right weights. Furthermore, if maximal precision is required, the output of the closed-form solution can be used to initialize a Gauss-Newton scheme, which improves accuracy with negligible amount of additional time. The advantages of our method are demonstrated by thorough testing on both synthetic and real-data.},
author = {Lepetit, Vincent and Moreno-Noguer, Francesc and Fua, Pascal},
doi = {10.1007/s11263-008-0152-6},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lepetit, Moreno-Noguer, Fua/EPnP An accurate O(n) solution to the PnP problem. Lepetit, Moreno-Noguer, Fua. 2009.pdf:pdf},
issn = {09205691},
journal = {Int. J. Comput. Vis.},
keywords = {Absolute orientation,Perspective-n-Point,Pose estimation},
number = {2},
pages = {155--166},
title = {{EPnP: An accurate O(n) solution to the PnP problem}},
volume = {81},
year = {2009}
}
@article{Sadakuni2019,
abstract = {In this research, we propose automatic construction of highly accurate data set for depth estimation by sensor fusion with high density 3D LiDAR and stereo camera. It is difficult to assign depth information all pixels with LiDAR stationary, due to the shortness of the LiDAR'S ranging distance to measure all of the objects reflected on the camera and point cloud is not so dense enough to obtain depth information corresponding to each pixel of the RGB image. We solved these issues by integrating point cloud based on relative position calculated with high accuracy by localization. In order to show the usefulness of this research, we have conducted a running experiment at Meiji University Ikuta Campus and compared the depth image of the stereo camera with the depth image of the proposed method.},
author = {Sadakuni, Yudai and Kusakari, Ryosuke and Onda, Kazuya and Kuroda, Yoji},
doi = {10.1109/SII.2019.8700333},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Sadakuni et al/Construction of Highly Accurate Depth Estimation Dataset Using High Density 3D LiDAR and Stereo Camera. Sadakuni et al.. 2019.pdf:pdf},
isbn = {9781538636152},
journal = {Proc. 2019 IEEE/SICE Int. Symp. Syst. Integr. SII 2019},
keywords = {Automation systems,Robotics technology,Software sy},
pages = {554--559},
title = {{Construction of Highly Accurate Depth Estimation Dataset Using High Density 3D LiDAR and Stereo Camera}},
year = {2019}
}
@article{Glennie2010,
abstract = {The static calibration and analysis of the Velodyne HDL-64E S2 scanning LiDAR system is presented and analyzed. The mathematical model for measurements for the HDL-64E S2 scanner is derived and discussed. A planar feature based least squares adjustment approach is presented and utilized in a minimally constrained network in order to derive an optimal solution for the laser's internal calibration parameters. Finally, the results of the adjustment along with a detailed examination of the adjustment residuals are given. A three-fold improvement in the planar misclosure residual RMSE over the standard factory calibration model was achieved by the proposed calibration. Results also suggest that there may still be some unmodelled distortions in the range measurements from the scanner. However, despite this, the overall precision of the adjusted laser scanner data appears to make it a viable choice for high accuracy mobile scanning applications.},
author = {Glennie, Craig and Lichti, Derek D.},
doi = {10.3390/rs2061610},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Glennie, Lichti/Static calibration and analysis of the velodyne HDL-64E S2 for high accuracy mobile scanning. Glennie, Lichti. 2010.pdf:pdf},
issn = {20724292},
journal = {Remote Sens.},
keywords = {Accuracy,Error analysis,Lidar,System calibration},
number = {6},
pages = {1610--1624},
title = {{Static calibration and analysis of the velodyne HDL-64E S2 for high accuracy mobile scanning}},
volume = {2},
year = {2010}
}
@article{Zhao2018,
abstract = {Three-dimensional object detection aims to produce a three-dimensional bounding box of an object at its full extent. Nowadays, three-dimensional object detection is mainly based on red green blue-depth (RGB-D) images. However, it remains an open problem because of the difficulty in labeling for three-dimensional training data. In this article, we present a novel three-dimensional object detection method based on two-dimensional object detection, which only takes a set of RGB images as input. First, aiming at the requirement of three-dimensional object detection and the low location accuracy of You Only Look Once, a modified two-dimensional object detection method based on You Only Look Once is proposed. Then, using a set of images from different visual angles, three-dimensional geometric data are reconstructed. In addition, making use of the modified You Only Look Once method, the two-dimensional object bounding boxes of the forward and side views are obtained. Finally, according to the transformation bet...},
author = {Zhao, Xia and Jia, Haihang and Ni, Yingting},
doi = {10.1177/1729881418765507},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Zhao, Jia, Ni/A novel three-dimensional object detection with the modified you only look once method. Zhao, Jia, Ni. 2018.pdf:pdf},
issn = {17298814},
journal = {Int. J. Adv. Robot. Syst.},
keywords = {3D object bounding box,Cluster box,Convolutional neural network,Coordinate transformation,Object detection},
number = {2},
pages = {1--13},
title = {{A novel three-dimensional object detection with the modified you only look once method}},
volume = {15},
year = {2018}
}
@article{Tools,
author = {Tools, Filesystem Command-line},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Tools/ROS Cheatsheet. Tools. Unknown.pdf:pdf},
pages = {2--3},
title = {{ROS Cheatsheet}},
url = {https://robotics.shanghaitech.edu.cn/static/cheatSheets/ROScheatsheet.pdf}
}
@article{Szeliski2011,
abstract = {As humans, we perceive the three-dimensional structure of the world around us with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem, and what is the current state of the art?Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging and fun consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos.More than just a source of "recipes", this text/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting this process to produce the best possible descriptions of a scene. Exercises are presented throughout the book, with a heavy emphasis on testing algorithms.Suitable for either an undergraduate or a graduate-level course in computer vision, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries.Dr. Richard Szeliski has over twenty years' experience in computer vision research, most notably at Digital Equipment Corporation and Microsoft.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Szeliski, Richard},
doi = {10.1007/978-1-84882-935-0},
eprint = {arXiv:1011.1669v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Szeliski/Computer Vision Algorithms {\&} Applications. Szeliski. 2011.pdf:pdf},
isbn = {978-1-84882-934-3},
issn = {10636919},
journal = {Computer (Long. Beach. Calif).},
pages = {832},
pmid = {16259003},
title = {{Computer Vision: Algorithms {\&} Applications}},
url = {http://research.microsoft.com/en-us/um/people/szeliski/book/drafts/szelski{\_}20080330am{\_}draft.pdf{\%}0Ahttp://link.springer.com/10.1007/978-1-84882-935-0},
volume = {5},
year = {2011}
}
@article{Velodyne2016,
author = {Velodyne},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Velodyne/VLP-16 Interface Box. Velodyne. 2016.pdf:pdf},
title = {{VLP-16 Interface Box}},
year = {2016}
}
@article{Pandey2012,
abstract = {This paper reports on a mutual information (MI) based algo- rithm for automatic extrinsic calibration of a 3D laser scan- ner and optical camera system. By using MI as the regis- tration criterion, our method is able to work in situ without the need for any specific calibration targets, which makes it practical for in-field calibration. The calibration parameters are estimated by maximizing the mutual information obtained between the sensor-measured surface intensities. We calcu- late the Cramer-Rao-Lower-Bound (CRLB) and show that the sample variance of the estimated parameters empirically ap- proaches the CRLB for a sufficient number of views. Fur- thermore, we compare the calibration results to independent ground-truth and observe that the mean error also empirically approaches to zero as the number of views are increased. This indicates that the proposed algorithm, in the limiting case, calculates a minimum variance unbiased (MVUB) estimate of the calibration parameters. Experimental results are pre- sented for data collected by a vehicle mounted with a 3D laser scanner and an omnidirectional camera system.},
author = {Pandey, Gaurav and McBride, James R and Savarese, Silvio and Eustice, Ryan M},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pandey et al/Automatic targetless extrinsic calibration of a 3d lidar and camera by maximizing mutual information. Pandey et al.. 2012.pdf:pdf},
journal = {Proc. {\{}AAAI{\}} Natl. Conf. Artif. Intell.},
keywords = {calibration,ladybug,omnidirectional,velodyne},
pages = {2053--2059},
title = {{Automatic targetless extrinsic calibration of a 3d lidar and camera by maximizing mutual information}},
year = {2012}
}
@article{Mar,
archivePrefix = {arXiv},
arxivId = {arXiv:1803.08181v1},
author = {Mar, R O},
eprint = {arXiv:1803.08181v1},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Mar/CalibNet Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks. Mar. Unknown.pdf:pdf},
pages = {1--8},
title = {{CalibNet: Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks}}
}
@article{Alberto2010a,
author = {Alberto, Sergio and Florez, Rodriguez and Fremont, Vincent and Bonnifait, Philippe and Alberto, Sergio and Florez, Rodriguez and Fremont, Vincent and Bonnifait, Philippe and F, Sergio A Rodriguez and Fr, Vincent and Bonnifait, Philippe},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Alberto et al/Influence of Intrinsic Parameters over Extrinsic Calibration between a Multi-Layer Lidar and a Camera To cite this version Influence (2).pdf:pdf},
title = {{Influence of Intrinsic Parameters over Extrinsic Calibration between a Multi-Layer Lidar and a Camera To cite this version : Influence of Intrinsic Parameters over Extrinsic Calibration between a Multi-Layer Lidar and a Camera}},
year = {2010}
}
@article{Hast2013,
abstract = {Optimal RANSAC - Towards a Repeatable Algorithm for Finding the Optimal Set},
author = {Hast, Anders and Nysj{\"{o}}, Johan and Marchetti, Andrea},
issn = {12136972},
journal = {J. WSCG},
keywords = {3D planes,Feature matching,Image stitching,Local optimisation,Optimal set,RANSAC,Repeatable},
number = {1},
pages = {21--30},
title = {{Optimal RANSAC - Towards a repeatable algorithm for finding the optimal set}},
url = {http://www.cb.uu.se/{~}aht/articles/A53-full.pdf},
volume = {21},
year = {2013}
}
@article{Visiona,
author = {Vision, Allied},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Vision/Terms and Conditions for Using the SF-36. Vision. Unknown.pdf:pdf},
pages = {2--4},
title = {{Terms and Conditions for Using the SF-36}},
url = {https://www.rand.org/health-care/surveys{\_}tools/mos/36-item-short-form.html}
}
@article{Ishikawa2018,
abstract = {In this paper, we propose a method of targetless and automatic Camera-LiDAR calibration. Our approach is an extension of hand-eye calibration framework to 2D-3D calibration. By using the sensor fusion odometry method, the scaled camera motions are calculated with high accuracy. In addition to this, we clarify the suitable motion for this calibration method. The proposed method only requires the three-dimensional point cloud and the camera image and does not need other information such as reflectance of LiDAR and to give initial extrinsic parameter. In the experiments, we demonstrate our method using several sensor configurations in indoor and outdoor scenes to verify the effectiveness. The accuracy of our method achieves more than other comparable state-of-the-art methods.},
author = {Ishikawa, Ryoichi and Oishi, Takeshi and Ikeuchi, Katsushi},
doi = {10.1109/IROS.2018.8593360},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ishikawa, Oishi, Ikeuchi/LiDAR and Camera Calibration Using Motions Estimated by Sensor Fusion Odometry. Ishikawa, Oishi, Ikeuchi. 2018(2).pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
number = {October 2018},
pages = {7342--7349},
title = {{LiDAR and Camera Calibration Using Motions Estimated by Sensor Fusion Odometry}},
year = {2018}
}
@article{Goppelt2011,
abstract = {Radar sensors are key components of modern driver assistance systems. Mutual interference was identified as a problem of increased importance because of the appearance of safety functions and the increasing rate of vehicles equipped with radar sensors. This paper describes mutual interference between automotive FMCW radar sensors. Analytical formulas were derived to be able to calculate the probability for ghost targets and the interference power per frequency bin. The results of the analytical calculations are compared with simulation results on the basis of a simple interference scenario with interference from an oncoming vehicle.},
author = {Goppelt, M. and Blocher, H.-L. and Menzel, W.},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Analytical investigation of mutual interference between automotive FMCW radar sensors. Goppelt, Blocher, Menzel.pdf:pdf},
isbn = {9783981266825},
journal = {2011 Ger. Microw. Conf.},
keywords = {Automotive components,Millimeter wave radar,Mutual coupling,Radiofrequency interference,Sensor systems},
number = {March},
pages = {1--4},
title = {{Analytical investigation of mutual interference between automotive FMCW radar sensors}},
year = {2011}
}
@article{Zhang2012,
abstract = {A method has been developed for registering two dense 3-D maps obtained by using a correlation-based stereo system. Geometric matching in general is a difficult unsolved problem in computer vision. Fortunately, in many practical applications, some a priori knowledge exists which considerably simplifies the problem. In visual navigation, for example, the motion between successive positions is usually either small or approximately known. From this initial estimate, our algorithm allows to compute the motion with very good precision, which is required for environment modeling. Objects are represented by a set of 3-D points, which are considered as the samples of a surface. No constraint is imposed on the form of the objects. The proposed algorithm is based on iteratively matching points of one view to the closest points of the another view. A statistical method based on the distance distribution is used to discard the outliers. A least-squares technique is used to estimate 3-D motion from the point correspondences, which reduces the average distance between curves in the two sets. Real data have been used to test the algorithm. The results show that it is efficient and robust, and yields an accurate motion estimate.},
author = {Zhang, Zhengyou},
doi = {10.1007/3-540-57233-3_60},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Zhang/Point matching for registration of free-form surfaces. Zhang. 2012.pdf:pdf},
pages = {460--467},
title = {{Point matching for registration of free-form surfaces}},
year = {2012}
}
@article{Shamshiri2018,
abstract = {Research efforts for development of agricultural robots that can effectively perform tedious field tasks have grown significantly in the past decade. Agricultural robots are complex systems that require interdisciplinary collaborations between different research groups for effective task delivery in unstructured crops and plants environments. With the exception of milking robots, the extensive research works that have been carried out in the past two decades for adaptation of robotics in agriculture have not yielded a commercial product to date. To accelerate this pace, simulation approach and evaluation methods in virtual environments can provide an affordable and reliable framework for experimenting with different sensing and acting mechanisms in order to verify the performance functionality of the robot in dynamic scenarios. This paper reviews several professional simulators and custom-built virtual environments that have been used for agricultural robotic applications. The key features and performance efficiency of three selected simulators were also compared. A simulation case study was demonstrated to highlight some of the powerful functionalities of the Virtual Robot Experimentation Platform. Details of the objects and scenes were presented as the proof-of-concept for using a completely simulated robotic platform and sensing systems in a virtual citrus orchard. It was shown that the simulated workspace can provide a configurable and modular prototype robotic system that is capable of adapting to several field conditions and tasks through easy testing and debugging of control algorithms with zero damage risk to the real robot and to the actual equipment. This review suggests that an open-source software platform for agricultural robotics will significantly accelerate effective collaborations between different research groups for sharing existing workspaces, algorithms, and reusing the materials.},
author = {Shamshiri, Redmond Ramin and Hameed, Ibrahim A. and Pitonakova, Lenka and Weltzien, Cornelia and Balasundram, Siva K. and Yule, Ian J. and Grift, Tony E. and Chowdhary, Girish},
doi = {10.25165/ijabe.v11i4.4032},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Shamshiri et al/Simulation software and virtual environments for acceleration of agricultural robotics Features highlights and performance comparison. S.pdf:pdf},
issn = {19346352},
journal = {Int. J. Agric. Biol. Eng.},
keywords = {Agricultural robotics,Digital agriculture,Multi-robots,Precision agriculture,Simulation software,Virtual orchards},
number = {4},
pages = {15--31},
title = {{Simulation software and virtual environments for acceleration of agricultural robotics: Features highlights and performance comparison}},
volume = {11},
year = {2018}
}
@article{Mathworks2018,
abstract = {Introdution to deep learning with matlab and basic concepts},
archivePrefix = {arXiv},
arxivId = {arXiv:1710.05006v3},
author = {Mathworks},
doi = {10.1109/ISBI.2018.8363547},
eprint = {arXiv:1710.05006v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Mathworks/Introducing Deep Learning with MATLAB. Mathworks. 2018.pdf:pdf},
isbn = {9781538636367},
issn = {19458452},
journal = {Introd. Deep Learn. with MATLAB},
pages = {15},
title = {{Introducing Deep Learning with MATLAB}},
url = {https://www.mathworks.com/campaigns/offers/deep-learning-with-matlab.html},
year = {2018}
}
@article{Kiran2019,
abstract = {Lidar has become an essential sensor for autonomous driving as it provides reliable depth estimation. Lidar is also the primary sensor used in building 3D maps which can be used even in the case of low-cost systems which do not use Lidar. Computation on Lidar point clouds is intensive as it requires processing of millions of points per second. Additionally there are many subsequent tasks such as clustering, detection, tracking and classification which makes real-time execution challenging. In this paper, we discuss real-time dynamic object detection algorithms which leverages previously mapped Lidar point clouds to reduce processing. The prior 3D maps provide a static background model and we formulate dynamic object detection as a background subtraction problem. Computation and modeling challenges in the mapping and online execution pipeline are described. We propose a rejection cascade architecture to subtract road regions and other 3D regions separately. We implemented an initial version of our proposed algorithm and evaluated the accuracy on CARLA simulator.},
archivePrefix = {arXiv},
arxivId = {arXiv:1809.11036v1},
author = {Kiran, B. Ravi and Rold{\~{a}}o, Luis and Irastorza, Be{\~{n}}at and Verastegui, Renzo and S{\"{u}}ss, Sebastian and Yogamani, Senthil and Talpaert, Victor and Lepoutre, Alexandre and Trehard, Guillaume},
doi = {10.1007/978-3-030-11021-5_35},
eprint = {arXiv:1809.11036v1},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kiran et al/Real-time dynamic object detection for autonomous driving using prior 3D-maps. Kiran et al.. 2019.pdf:pdf},
isbn = {9783030110208},
issn = {16113349},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
keywords = {3D obstacles,Inlier rejection,Prior maps},
pages = {567--582},
title = {{Real-time dynamic object detection for autonomous driving using prior 3D-maps}},
volume = {11133 LNCS},
year = {2019}
}
@article{Christopher2015,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0403007},
author = {Christopher, Inventors and Ramsey, Paul and Chant, Garry Richard and Lockley, Andrew Robert and Gb, Wantage and Fields, Brian and Watson, Martin John and Rachel, Eleanor and Hyde, Ann and Gb, Wantage and Jasper, A},
doi = {10.1016/j.(73)},
eprint = {0403007},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Christopher et al/( 12 ) United States Patent ( 10 ) Patent No .. Christopher et al.. 2015.pdf:pdf},
isbn = {2010512510},
issn = {2470-0010},
number = {12},
pmid = {1000182772},
primaryClass = {arXiv:physics},
title = {{( 12 ) United States Patent ( 10 ) Patent No .:}},
volume = {2},
year = {2015}
}
@article{Dupont2005,
abstract = {Toward a successful 3D and textural reconstruction of urban scenes, the use of both single-row based telemetric and photographic data in a same framework has proved to be a powerful technique. A necessary condition to ob-tain good results is to accurately calibrate the telemetric and photographic sensors together. We present a study of this calibration process and propose an improved extrin-sic calibration technique. It is based on an existing tech-nique which consists in scanning a planar pattern in sev-eral poses, giving a set of relative position and orientation constraints. The innovation is the use of a more appropri-ate laser beam distance between telemetric points and the planar target. Moreover, we use robust methods to manage outliers at several steps of the algorithm. Improved results on both theoretical and experimental data are given.},
author = {Dupont, Romain and Keriven, Renaud and Fuchs, Philippe},
doi = {10.1109/3DIM.2005.19},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Dupont, Keriven, Fuchs/An improved calibration technique for coupled single-row telemeter and CCD camera. Dupont, Keriven, Fuchs. 2005.pdf:pdf},
isbn = {0769523277},
issn = {15506185},
journal = {Proc. Int. Conf. 3-D Digit. Imaging Model. 3DIM},
pages = {89--94},
title = {{An improved calibration technique for coupled single-row telemeter and CCD camera}},
year = {2005}
}
@article{Li2019,
abstract = {We propose a 3D object detection method for autonomous driving by fully exploiting the sparse and dense, semantic and geometry information in stereo imagery. Our method, called Stereo R-CNN, extends Faster R-CNN for stereo inputs to simultaneously detect and associate object in left and right images. We add extra branches after stereo Region Proposal Network (RPN) to predict sparse keypoints, viewpoints, and object dimensions, which are combined with 2D left-right boxes to calculate a coarse 3D object bounding box. We then recover the accurate 3D bounding box by a region-based photometric alignment using left and right RoIs. Our method does not require depth input and 3D position supervision, however, outperforms all existing fully supervised image-based methods. Experiments on the challenging KITTI dataset show that our method outperforms the state-of-the-art stereo-based method by around 30{\%} AP on both 3D detection and 3D localization tasks. Code has been released at https://github.com/HKUST-Aerial-Robotics/Stereo-RCNN.},
archivePrefix = {arXiv},
arxivId = {1902.09738},
author = {Li, Peiliang and Chen, Xiaozhi and Shen, Shaojie},
eprint = {1902.09738},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Li, Chen, Shen/Stereo R-CNN based 3D Object Detection for Autonomous Driving. Li, Chen, Shen. 2019.pdf:pdf},
title = {{Stereo R-CNN based 3D Object Detection for Autonomous Driving}},
url = {http://arxiv.org/abs/1902.09738},
year = {2019}
}
@article{Gmbh2018,
author = {Gmbh, Allied Vision},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gmbh/Modular Concept V10.0.0. Gmbh. 2018(2).pdf:pdf},
keywords = {Additional options for camera mounts, housing desi},
pages = {1--31},
title = {{Modular Concept V10.0.0}},
year = {2018}
}
@article{Bingham2010,
author = {Bingham, Brian and Foley, Brendan and Singh, Hanumant and Camilli, Richard and Delaporta, Katerina and Eustice, Ryan and Mallios, Angelos and Mindell, David and Roman, Christopher and Sakellariou, Dimitris},
doi = {10.1002/rob},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Bingham et al/Brian Bingham. Bingham et al.. 2010.pdf:pdf},
number = {5},
pages = {1--16},
title = {{Brian Bingham}},
volume = {32},
year = {2010}
}
@article{Yu2018a,
abstract = {—In recent years, Convolutional Neural Network (CNN) has been widely applied in computer vision tasks and has achieved significant improvement in image object detection. The CNN methods consume more computation as well as storage, so GPU is introduced for real-time object detection. However, due to the high power consumption of GPU, it is difficult to adopt GPU in mobile applications like automatic driving. The previous work proposes some optimizing techniques to lower the power consumption of object detection on mobile GPU or FPGA. In the first Low-Power Image Recognition Challenge (LPIRC), our system achieved the best result with mAP/Energy on mobile GPU platforms. We further research the acceleration of detection algorithms and implement two more systems for real-time detec-tion on FPGA with higher energy efficiency. In this paper, we will introduce the object detection algorithms and summarize the optimizing techniques in three of our previous energy efficient detection systems on different hardware platforms for object detection.},
author = {Yu, Jincheng and Guo, Kaiyuan and Hu, Yiming and Ning, Xuefei and Qiu, Jiantao and Mao, Huizi and Yao, Song and Tang, Tianqi and Li, Boxun and Wang, Yu and Yang, Huazhong},
doi = {10.23919/DATE.2018.8342100},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yu et al/Real-time object detection towards high power efficiency. Yu et al.. 2018.pdf:pdf},
isbn = {9783981926316},
journal = {Proc. 2018 Des. Autom. Test Eur. Conf. Exhib. DATE 2018},
pages = {704--708},
publisher = {EDAA},
title = {{Real-time object detection towards high power efficiency}},
volume = {2018-Janua},
year = {2018}
}
@article{March2014,
author = {March, Due and April, Due and April, Due},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/March, April, April/CSCE574 – Robotics Spring 2014 – Guidelines for Graduate Student Papers. March, April, April. 2014.pdf:pdf},
pages = {1--2},
title = {{CSCE574 – Robotics Spring 2014 – Guidelines for Graduate Student Papers}},
year = {2014}
}
@article{Hurtado-ramos2013,
author = {Hurtado-ramos, Juan B},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hurtado-ramos/LIDAR and Panoramic Camera Extrinsic. Hurtado-ramos. 2013.pdf:pdf},
keywords = {extrinsic,lidar,panoramic camera,sensor calibration},
pages = {104--113},
title = {{LIDAR and Panoramic Camera Extrinsic}},
year = {2013}
}
@article{Mirzaei2012,
abstract = {In this paper we address the problem of estimating the intrinsic parameters of a 3D LIDAR while at the same time computing its extrinsic calibration with respect to a rigidly connected camera. Existing approaches to solve this nonlinear estimation problem are based on iterative minimization of nonlinear cost functions. In such cases, the accuracy of the resulting solution hinges on the availability of a precise initial estimate, which is often not available. In order to address this issue, we divide the problem into two least-squares sub-problems, and analytically solve each one to determine a precise initial estimate for the unknown parameters. We further increase the accuracy of these initial estimates by iteratively minimizing a batch nonlinear least-squares cost function. In addition, we provide the minimal identifiability conditions, under which it is possible to accurately estimate the unknown parameters. Experimental results consisting of photorealistic 3D reconstruction of indoor and outdoor scenes, as well as standard metrics of the calibration errors, are used to assess the validity of our approach.},
author = {Mirzaei, Faraz M. and Kottas, Dimitrios G. and Roumeliotis, Stergios I.},
doi = {10.1177/0278364911435689},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Mirzaei, Kottas, Roumeliotis/3D LIDAR-camera intrinsic and extrinsic calibration Identifiability and analytical least-squares-based initialization. Mirzaei, Kottas,.pdf:pdf},
issn = {02783649},
journal = {Int. J. Rob. Res.},
keywords = {Sensing and perception,calibration and identification,computer vision,range sensing},
number = {4},
pages = {452--467},
title = {{3D LIDAR-camera intrinsic and extrinsic calibration: Identifiability and analytical least-squares-based initialization}},
volume = {31},
year = {2012}
}
@article{Sullivan2016,
abstract = {Analysis of LiDAR technology for Advanced Safety Preface Safety and its improvement is a concern paramount to all passenger vehicle manufacturers. The value proposition for driver, passenger and pedestrian safety has become equally important as engine performance and fuel economy. This paper addresses how light detection and range (LiDAR) technology will impact Advanced Driver Assistance Systems and explore the term, " autonomous driving intelligence. " The paper will consider Lidar technology today and how it stands to capture a large market share of automotive sensor technology in the future. Lidar is poised to penetrate the market in 2016 as the lead technology in automotive safety and autonomous systems. Three primary approaches to Lidar development are considered, including Hybrid Solid-State Lidar, MEMS Lidar, and Mechanical Mechanism Lidar. Mechanical mechanism Lidar is the oldest and most traditional technology. MEMS Lidar technology is in the beginning stage of development as a low cost Lidar solution for low level automotive safety. Solid-state Hybrid Lidar (SH Lidar) was introduced in 2005 as a result of the Darpa Robotic Car Races. The technology has been tested for autonomous safety over the years and the cost for SH Lidar dropped dramatically in 2015. With planned mass production to meet the growing demand for autonomous navigation and advanced safety, further dramatic cost reduction is expected in 2016 – 2017. The development of Solid-State Hybrid Lidar (SH Lidar) was a break away from the traditional mechanical mechanism of single Lidar technology, and it is described in detail in this paper. The technology simplified what was previously a complex mechanical system of parts into one robust solid-state part. The solid-state developmental enabled faster data capture in 3D, capturing pictures instantaneously while moving in real-time at speeds of 30-40MPH. SH Lidar technology has matured from an extremely costly technology and large system to being affordable, smaller in size, and headed toward mass production. SH Lidar technology is poised to be commercialized in 2015-16 and radically change the way we move about in the world.},
author = {Sullivan, Frost {\&}},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Sullivan/LiDAR Driving the Future of Autonomous Navigation. Sullivan. 2016.pdf:pdf},
pages = {1--30},
title = {{LiDAR: Driving the Future of Autonomous Navigation}},
year = {2016}
}
@article{Chen2017a,
abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25{\%} and 30{\%} AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3{\%} higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.07759v3},
author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
doi = {10.1109/CVPR.2017.691},
eprint = {arXiv:1611.07759v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Chen et al/Multi-view 3D object detection network for autonomous driving. Chen et al.. 2017.pdf:pdf},
isbn = {9781538604571},
journal = {Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017},
pages = {6526--6534},
title = {{Multi-view 3D object detection network for autonomous driving}},
volume = {2017-Janua},
year = {2017}
}
@article{Costa2018a,
abstract = {The accuracy of photogrammetric and Lidar dataset integration is dependent on the quality of a group of parameters that models accurately the conditions of the system at the moment of the survey. In this sense, this paper aims to study the effect of the sub-block position in the entire image block to estimate the interior orientation parameters (IOP) in flight conditions to be used in integrated sensor orientation (ISO). For this purpose, five sub-blocks were extracted in different regions of the entire block. Then, in situ camera calibrations were performed using sub-blocks and sets of Lidar control points (LCPs), computed by a three planes' intersection extracted from the Lidar point cloud on building roofs. The ISO experiments were performed using IOPs from in situ calibrations, the entire image block, and the exterior orientation parameters (EOP) from the direct sensor orientation (DSO). Analysis of the results obtained from the ISO experiments performed show that the IOP from the sub-block positioned at the center of the entire image block can be recommended.},
author = {Costa, Felipe A.L. and Mitishita, Edson A. and Martins, Marlo},
doi = {10.3390/rs10020260},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Costa, Mitishita, Martins/The influence of sub-block position on performing integrated sensor orientation using in situ camera calibration and Lidar control point.pdf:pdf},
issn = {20724292},
journal = {Remote Sens.},
keywords = {Direct sensor orientation (DSO),In situ camera calibration,Integrated sensor orientation,Lidar control points},
number = {2},
title = {{The influence of sub-block position on performing integrated sensor orientation using in situ camera calibration and Lidar control points}},
volume = {10},
year = {2018}
}
@article{Godbaz2012,
abstract = {We present two new closed-form methods for mixed pixel/multipath interference separation in AMCW lidar systems. The mixed pixel/multipath interference problem arises from the violation of a standard range-imaging assumption that each pixel integrates over only a single, discrete backscattering source. While a numerical inversion method has previously been proposed, no close-form inverses have previously been posited. The first new method models reflectivity as a Cauchy distribution over range and uses four measurements at different modulation frequencies to determine the amplitude, phase and reflectivity distribution of up to two component returns within each pixel. The second new method uses attenuation ratios to determine the amplitude and phase of up to two component returns within each pixel. The methods are tested on both simulated and real data and shown to produce a significant improvement in overall error. While this paper focusses on the AMCW mixed pixel/multipath interference problem, the algorithms contained herein have applicability to the reconstruction of a sparse one dimensional signal from an extremely limited number of discrete samples of its Fourier transform.},
author = {Godbaz, John P. and Cree, Michael J. and Dorrington, Adrian A.},
doi = {10.1117/12.909778},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Godbaz, Cree, Dorrington/Closed-form inverses for the mixed pixelmultipath interference problem in AMCW lidar. Godbaz, Cree, Dorrington. 2012.pdf:pdf},
journal = {Comput. Imaging X},
keywords = {amcw,atic error,attenuation ratio,lidar,mixed pixels,multipath interference,multiple return separation,range imaging,system-},
pages = {829618},
title = {{Closed-form inverses for the mixed pixel/multipath interference problem in AMCW lidar}},
volume = {8296},
year = {2012}
}
@article{Unnikrishnan2008,
author = {Unnikrishnan, Ranjith},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Unnikrishnan/Statistical Approaches to Multi-scale Point Cloud Processing. Unnikrishnan. 2008.pdf:pdf},
journal = {Wwwoldricmuedu},
number = {May},
pages = {1----146},
title = {{Statistical Approaches to Multi-scale Point Cloud Processing}},
url = {http://www-old.ri.cmu.edu/pubs/pub{\_}6112{\_}text.html},
year = {2008}
}
@article{VelodyneLiDAR2019,
abstract = {Velodyne's Puck (VLP-16) is the smallest, cost-optimized product in Velodyne's 3D LiDAR product range. Developed with mass production in mind, the Puck is far more cost-effective than comparable sensors, and it retains the key features of Velodyne's breakthroughs in LiDAR: Real-time, 360°, 3D distance and calibrated reflectivity measurements. The VLP-16 has a range of 100 m, and the sensor's low power consumption, light weight, compact footprint and dual return capability make it ideal not only for autonomous vehicles but also for robotics, terrestrial 3D mapping and many other applications. Velodyne's LiDAR Puck supports 16 channels, {\~{}}300,000 points/second, 360° horizontal field of view and a 30° vertical field of view, with ±15° up and down. The Puck does not have visible rotating parts, and is highly resilient in challenging environments while operating over a wide temperature range. VLP-16 DIMENSIONS M12 CONNECTOR OPTION 2X .16 FEATURES FOR 5/32in. PINS 7/32in. 5.5mm 88.9mm 3.50in. 0° 90° 1/4-20 MOUNT 9/32in. 7.1mm 12.7mm MAX 0.50in. MAX 12.7mm MAX 0.50in. MAX 103.3mm 4.07in. ∅ ∅ OPTICAL CENTER 37.8mm 1.49in. 18.8mm 0.74in. 12.7mm MAX 0.50in. MAX 71.7mm 2.82in.},
author = {{Velodyne LiDAR}, Inc.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Velodyne LiDAR/Velodyne Lidar Puck Data Sheet. Velodyne LiDAR. 2019.pdf:pdf},
pages = {2},
title = {{Velodyne Lidar Puck Data Sheet}},
year = {2019}
}
@article{Pusztai2018,
abstract = {As autonomous driving attracts more and more attention these days, the algorithms and sensors used for machine perception become popular in research, as well. This paper investigates the extrinsic calibration of two frequently-applied sensors: the camera and Light Detection and Ranging (LiDAR). The calibration can be done with the help of ordinary boxes. It contains an iterative refinement step, which is proven to converge to the box in the LiDAR point cloud, and can be used for system calibration containing multiple LiDARs and cameras. For that purpose, a bundle adjustment-like minimization is also presented. The accuracy of the method is evaluated on both synthetic and real-world data, outperforming the state-of-the-art techniques. The method is general in the sense that it is both LiDAR and camera-type independent, and only the intrinsic camera parameters have to be known. Finally, a method for determining the 2D bounding box of the car chassis from LiDAR point clouds is also presented in order to determine the car body border with respect to the calibrated sensors.},
author = {Pusztai, Zolt{\'{a}}n and Eichhardt, Iv{\'{a}}n and Hajder, Levente},
doi = {10.3390/s18072139},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pusztai, Eichhardt, Hajder/Accurate calibration of multi-lidar-multi-camera systems. Pusztai, Eichhardt, Hajder. 2018.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Autonomous driving,Camera,Extrinsic calibration,LiDAR,LiDAR camera system,Machine perception},
number = {7},
pages = {1--22},
title = {{Accurate calibration of multi-lidar-multi-camera systems}},
volume = {18},
year = {2018}
}
@article{Alberto2010,
author = {Alberto, Sergio and Florez, Rodriguez and Fremont, Vincent and Bonnifait, Philippe and Alberto, Sergio and Florez, Rodriguez and Fremont, Vincent and Bonnifait, Philippe and F, Sergio A Rodriguez and Fr, Vincent and Bonnifait, Philippe},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Alberto et al/Influence of Intrinsic Parameters over Extrinsic Calibration between a Multi-Layer Lidar and a Camera To cite this version Influence of.pdf:pdf},
title = {{Influence of Intrinsic Parameters over Extrinsic Calibration between a Multi-Layer Lidar and a Camera To cite this version : Influence of Intrinsic Parameters over Extrinsic Calibration between a Multi-Layer Lidar and a Camera}},
year = {2010}
}
@phdthesis{Vali2018,
abstract = {The main goal of this thesis was to evaluate different lidars placements on a self- driving vehicle in Autoware software. Evaluation requires running multiple tests in the same environment to be sure of the results. Since real-life testing is time consuming and may be more inaccurate, a simulation environment called Gazebo was used throughout the thesis. Gazebo allows simulating the same test with different lidar configurations multiple times. In addition to the main goal, a lidar simulator plugin had to be integrated with Autoware and multple lidar setup had to be configured in Autoware. When evaluating lidar configuration, the goal was to improve object reprojection with a constraint that the localization process does not get worse. At first testing different lidar configurations in Gazebo lead the author to belive that a two lidar setup with wanted configuration makes localization process worse. After analyzing and carrying out further tests it turned out that the initial results were a cause of bad map. The localization process had gotten worse because the configuration was trying to localize on parts of map that were not mapped very well. As a final result it turns out that adding a second lidar improves object reprojection while the localization process remains the same. The thesis is in English and contains 43 pages of text, 6 chapters, 7 code examples, 14 figures, 1 table.},
author = {V{\"{a}}li, Mihkel},
doi = {10.1068/d160163},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/V{\"{a}}li/Evaluation of multiple lidar placement on a self-driving car in Autoware. V{\"{a}}li. 2018.pdf:pdf},
issn = {02637758},
month = {apr},
title = {{Evaluation of multiple lidar placement on a self-driving car in Autoware}},
url = {http://journals.sagepub.com/doi/10.1068/d160163},
year = {2018}
}
@article{Kirmani2013,
abstract = {We propose a framework for simultaneous phase unwrapping and multipath interference cancellation (SPUMIC) in homodyne time-of-flight (ToF) cameras. Our multi-frequency acquisition framework is based on parametric modeling of the multipath interference phenomena. We use robust spectral estimation methods with low computational complexity to detect and estimate multipath parameters. Using simulations and analysis we demonstrate that our proposed solution is implementable in real-time on existing ToF cameras without requiring any hardware modifications.},
author = {Kirmani, Ahmed and Benedetti, Arrigo and Chou, Philip A.},
doi = {10.1109/ICME.2013.6607553},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/SPUMIC Simultaneous phase unwrapping and multipath interference cancellation in time-of-flight cameras using spectral methods. Kirmani,.pdf:pdf},
isbn = {9781479900152},
issn = {19457871},
journal = {Proc. - IEEE Int. Conf. Multimed. Expo},
keywords = {3D camera,Time-of-flight range imaging,depth sensing,flying pixel,mixed,multipath cancellation,phase unwrapping,spectral estimation},
number = {2},
title = {{SPUMIC: Simultaneous phase unwrapping and multipath interference cancellation in time-of-flight cameras using spectral methods}},
volume = {0},
year = {2013}
}
@article{HesaiPhotonicsTechnologyCoLtd2018,
author = {{Hesai Photonics Technology Co Ltd}},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hesai Photonics Technology Co Ltd/Pandar40 40-Channel Mechanical LiDAR User's Manual. Hesai Photonics Technology Co Ltd. 2018.pdf:pdf},
pages = {41},
title = {{Pandar40 40-Channel Mechanical LiDAR User's Manual}},
year = {2018}
}
@misc{Simonite2017,
author = {Simonite, Tom},
booktitle = {MT Technol. Rev.},
title = {{Self-Driving Cars' Spinning-Laser Problem - MIT Technology Review}},
url = {https://www.technologyreview.com/s/603885/autonomous-cars-lidar-sensors/},
urldate = {2019-01-23},
year = {2017}
}
@article{KresimirKusevicOttawaCA;PaulMrstikOttawaCA;CraigLenGlennieSpring2017,
author = {{Kresimir Kusevic, Ottawa (CA); Paul Mrstik, Ottawa (CA); Craig Len Glennie, Spring}, TX (US)},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Method and System for Aligninga Line Scan Camerawith a Lidar Scanner for Real Time Data Fusion in Three Dimiensions. Kresimir Kusevic, O.pdf:pdf},
isbn = {2222222222},
number = {19},
title = {{Method and System for Aligninga Line Scan Camerawith a Lidar Scanner for Real Time Data Fusion in Three Dimiensions}},
volume = {1},
year = {2017}
}
@article{Lepetit2009a,
author = {Lepetit, Vincent and Moreno-Noguer, Francesc and Fua, Pascal},
doi = {10.1109/ICCV.2007.4409116},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lepetit, Moreno-Noguer, Fua/Accurate Non-Iterative O(n) Solution to the PnP Problem. Lepetit, Moreno-Noguer, Fua. 2009.pdf:pdf},
isbn = {978-1-4244-1630-1},
issn = {1550-5499},
journal = {Int. J. Comput. Vis.},
keywords = {absolute orientation,perspective- n -point,pose estimation},
number = {2},
pages = {155--166},
title = {{Accurate Non-Iterative O(n) Solution to the PnP Problem}},
url = {http://www.springerlink.com/index/W1P227W846535772.pdf},
volume = {81},
year = {2009}
}
@article{Cohen1978a,
author = {Cohen, A. and Kleiman, M. and Cooney, J.},
doi = {10.1364/ao.17.001905},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Cohen, Kleiman, Cooney/Lidar measurements of rotational Raman and double scattering. Cohen, Kleiman, Cooney. 1978.pdf:pdf;:media/martinspedro/DATA/UA/Thesis/Organized Research/Cohen, Kleiman, Cooney/Lidar measurements of rotational Raman and double scattering. Cohen, Kleiman, Cooney. 1978(2).pdf:pdf},
issn = {0003-6935},
journal = {Appl. Opt.},
number = {12},
pages = {1905},
title = {{Lidar measurements of rotational Raman and double scattering}},
volume = {17},
year = {1978}
}
@article{Cutter2016,
abstract = {Stereotactic neurosurgical robots allow quick, accurate location of small targets within the brain, relying on accurate registration of pre-operative MRI/CT images with patient and robot coordinate systems during surgery. Fiducial markers or a stereotactic frame are used as registration landmarks; the patient's head is fixed in position throughout surgery. An image-based system could be quicker and less invasive, allowing the head to be moved during surgery to give greater ease of access, but would be required to retain a surgical precision of ∼1 mm at the target point. We compare two registration algorithms, iterative closest point (ICP) and coherent point drift (CPD), by registering ideal point clouds taken from MRI data with re-meshed, noisy and smoothed versions. We find that ICP generally gives better and more consistent registration accuracy for the region of interest than CPD, with a best RMS distance of 0.884±0.050 mm between aligned point clouds, as compared to 0.995±0.170 mm or worse for CPD.},
author = {Cutter, Jennifer R. and Styles, Iain B. and Leonardis, Ale{\v{s}} and Dehghani, Hamid},
doi = {10.1016/j.procs.2016.07.006},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Cutter et al/Image-based Registration for a Neurosurgical Robot Comparison Using Iterative Closest Point and Coherent Point Drift Algorithms. Cutter.pdf:pdf},
issn = {18770509},
journal = {Procedia Comput. Sci.},
keywords = {CPD,ICP,Registration,neurosurgery,robot},
number = {July},
pages = {28--34},
publisher = {The Author(s)},
title = {{Image-based Registration for a Neurosurgical Robot: Comparison Using Iterative Closest Point and Coherent Point Drift Algorithms}},
url = {http://dx.doi.org/10.1016/j.procs.2016.07.006},
volume = {90},
year = {2016}
}
@article{Matiukas2011,
abstract = {This article addresses the problem of reconstructing 3D surfaces from unorganized point sets (also known as point clouds). Thisissue is common to many different areas of science and engineering, including computer graphics and computer vision. Unorganizedpoint sets can be acquired in different ways, e.g., using laser scanners, computer tomography, magnetic resonance imaging, multicameravision systems, etc. This paper presents a method for point cloud merging that allows calculating the initial rotation andtranslation between point clouds of different viewing angles. By tracking the marker's position, scanning and point cloud registrationcan be done in real time and with high accuracy. Ill. 7, bibl. 11 (in English; abstracts in English and Lithuanian). http://dx.doi.org/10.5755/j01.eee.113.7.616},
author = {Matiukas, V. and Miniotas, D.},
doi = {10.5755/j01.eee.113.7.616},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Matiukas, Miniotas/Point cloud merging for complete 3D surface reconstruction. Matiukas, Miniotas. 2011.pdf:pdf},
issn = {13921215},
journal = {Elektron. ir Elektrotechnika},
number = {7},
pages = {73--76},
title = {{Point cloud merging for complete 3D surface reconstruction}},
volume = {7},
year = {2011}
}
@article{Glennie2016,
abstract = {We report on a calibration and stability analysis of the Velodyne VLP-16 LiDAR scanner. The sensor is evaluated for long-term stability, geometric calibration and the effect of temperature variations. To generalize the results, three separate VLP-16 sensors were examined. The results and conclusions from the analysis of each of the individual sensors was similar. We found that the VLP-16 showed a consistent level of performance, in terms of range bias and noise level over the tested temperature range from 0{\&}ndash;40 °C. A geometric calibration was able to marginally improve the accuracy of the VLP-16 point cloud (by approximately 20{\%}) for a single collection, however the temporal stability of the geometric calibration negated this accuracy improvement. Overall, it was found that there is some long-term walk in the ranging observations from individual lasers within the VLP-16, which likely causes the instability in the determination of geometric calibration parameters. However, despite this range walk, the point cloud delivered from the VLP-16 sensors tested showed an accuracy level within the manufacturer specifications of 3 cm RMSE, with an overall estimated RMSE of range residuals between 22 mm and 27 mm.},
author = {Glennie, C. L. and Kusari, A. and Facchin, A.},
doi = {10.5194/isprsarchives-XL-3-W4-55-2016},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Glennie, Kusari, Facchin/Calibration and stability analysis of the VLP-16 laser scanner. Glennie, Kusari, Facchin. 2016.pdf:pdf},
issn = {16821750},
journal = {Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. - ISPRS Arch.},
keywords = {Calibration,LiDAR,Stability,Velodyne},
number = {3W4},
pages = {55--60},
title = {{Calibration and stability analysis of the VLP-16 laser scanner}},
volume = {40},
year = {2016}
}
@article{Scholler2019,
abstract = {Most intelligent transportation systems use a combination of radar sensors and cameras for robust vehicle perception. The calibration of these heterogeneous sensor types in an automatic fashion during system operation is challenging due to differing physical measurement principles and the high sparsity of traffic radars. We propose - to the best of our knowledge - the first data-driven method for automatic rotational radar-camera calibration without dedicated calibration targets. Our approach is based on a coarse and a fine convolutional neural network. We employ a boosting-inspired training algorithm, where we train the fine network on the residual error of the coarse network. Due to the unavailability of public datasets combining radar and camera measurements, we recorded our own real-world data. We demonstrate that our method is able to reach precise and robust sensor registration and show its generalization capabilities to different sensor alignments and perspectives.},
archivePrefix = {arXiv},
arxivId = {1904.08743},
author = {Sch{\"{o}}ller, Christoph and Schnettler, Maximilian and Kr{\"{a}}mmer, Annkathrin and Hinz, Gereon and Bakovic, Maida and G{\"{u}}zet, M{\"{u}}ge and Knoll, Alois},
eprint = {1904.08743},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Sch{\"{o}}ller et al/Targetless Rotational Auto-Calibration of Radar and Camera for Intelligent Transportation Systems. Sch{\"{o}}ller et al.. 2019.pdf:pdf},
title = {{Targetless Rotational Auto-Calibration of Radar and Camera for Intelligent Transportation Systems}},
url = {http://arxiv.org/abs/1904.08743},
year = {2019}
}
@article{Li2016,
abstract = {Detailed 3D modeling of indoor scene has become an important topic in many research fields. It can provide extensive information about the environment and boost various location based services, such as interactive gaming and indoor navigation. This paper presents an indoor scene construction approach using 2D line-scan LiDAR and entry-level digital camera. Both devices are mounted rigidly on a robotic servo, which sweeps vertically to cover the third dimension. Fiducial target based extrinsic calibration is applied to acquire transformation matrices between LiDAR and camera. Based on the transformation matrix, we perform registration to fuse the color images from the camera with the 3D points cloud from the LiDAR. The whole system setup has much lower cost as compared to systems using 3D LiDAR and omnidirectional camera. Using pre-calculated transformation matrices instead of feature extraction techniques such as SIFT or SURF in registration gives better fusion result and lower computational complexity. The experiments carried out in office building environment show promising results of our approach.},
author = {Li, Juan and He, Xiang and Li, Jia},
doi = {10.1109/NAECON.2015.7443100},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Li, He, Li/2D LiDAR and camera fusion in 3D modeling of indoor environment. Li, He, Li. 2016.pdf:pdf},
isbn = {9781467375658},
issn = {23792027},
journal = {Proc. IEEE Natl. Aerosp. Electron. Conf. NAECON},
keywords = {2D line-scan LiDAR,3D indoor modeling,digital camera,extrinsic calibration,sensor fusion},
number = {June 2015},
pages = {379--383},
title = {{2D LiDAR and camera fusion in 3D modeling of indoor environment}},
volume = {2016-March},
year = {2016}
}
@book{Vision2013,
author = {Vision, Allied and Gmbh, Technologies},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Vision, Gmbh/AVT GigE Cameras Camera and Driver Features Legal notice. Vision, Gmbh. 2013.pdf:pdf},
isbn = {6048758855},
number = {September},
pages = {1--57},
title = {{AVT GigE Cameras Camera and Driver Features Legal notice}},
year = {2013}
}
@article{Fritsch2013,
abstract = {Detecting the road area and ego-lane ahead of a vehicle is central to modern driver assistance systems. While lane-detection on well-marked roads is already available in modern vehicles, finding the boundaries of unmarked or weakly marked roads and lanes as they appear in inner-city and rural environments remains an unsolved problem due to the high variability in scene layout and illumination conditions, amongst others. While recent years have witnessed great interest in this subject, to date no commonly agreed upon benchmark exists, rendering a fair comparison amongst methods difficult. In this paper, we introduce a novel open-access dataset and benchmark for road area and ego-lane detection. Our dataset comprises 600 annotated training and test images of high variability from the KITTI autonomous driving project, capturing a broad spectrum of urban road scenes. For evaluation, we propose to use the 2D Bird's Eye View (BEV) space as vehicle control usually happens in this 2D world, requiring detection results to be represented in this very same space. Furthermore, we propose a novel, behavior-based metric which judges the utility of the extracted ego-lane area for driver assistance applications by fitting a driving corridor to the road detection results in the BEV. We believe this to be important for a meaningful evaluation as pixel-level performance is of limited value for vehicle control. State-of-the-art road detection algorithms are used to demonstrate results using classical pixel-level metrics in perspective and BEV space as well as the novel behavior-based performance measure. All data and annotations are made publicly available on the KITTI online evaluation website in order to serve as a common benchmark for road terrain detection algorithms.},
author = {Fritsch, Jannik and Kuhnl, Tobias and Geiger, Andreas},
doi = {10.1109/ITSC.2013.6728473},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fritsch, Kuhnl, Geiger/A new performance measure and evaluation benchmark for road detection algorithms. Fritsch, Kuhnl, Geiger. 2013.pdf:pdf},
isbn = {9781479929146},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
pages = {1693--1700},
title = {{A new performance measure and evaluation benchmark for road detection algorithms}},
year = {2013}
}
@article{Moreno2016,
abstract = {Sensors for collecting 3D spatial data from the real world are becoming more important. They are a prime research area topic and have applications in consumer markets, such as medical, entertainment, and robotics. However, a primary concern with collecting this data is the vast amount of information being generated, and thus, needing to be processed before transmitted. To address the issue, several filtering methods have been proposed to remove unimportant data. In this paper, a comparative study is conducted to find which methods work best and under what situation. To collect the 3D spatial data, called point clouds, we used the Microsoft Kinect sensor. In addition, we utilized the Point Cloud Library (PCL) to process and filter the data being generated by the Kinect. Two different computers were setup: a client which collects, filters, and transmits the point clouds; and a server that receives and visualizes the point clouds. In order to compare the filtering methods, quality of service (QoS) metrics such as frame rate, filtering time, and percentage of filter were introduced. These metrics indicate how well a certain filtering method accomplishes the goal of transmitting point clouds from one location to another in real-time. Results show that, regardless of the filtering approach chosen, there is still too much data for a satisfactory QoS. For a real-time system to provide reasonable end-to-end quality, further compression and other techniques need to be developed.},
author = {Moreno, Carlos and Li, Ming},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Moreno, Li/A comparative study of filtering methods for point clouds in real-time video streaming. Moreno, Li. 2016.pdf:pdf},
isbn = {9789881404718},
issn = {20780958},
journal = {Lect. Notes Eng. Comput. Sci.},
keywords = {Filtering,Kinect,Point clouds,Real-time,Video streaming},
pages = {389--393},
title = {{A comparative study of filtering methods for point clouds in real-time video streaming}},
volume = {2225},
year = {2016}
}
@inproceedings{Kim2015,
abstract = {LIDAR scanners are essential components of intelligent vehicles capable of autonomous travel. Mutual interference between LIDAR scanners has not been regarded as a problem yet. Mutual interference was identified as a problem of increased importance because of the appearance of safety functions and the increasing rate of vehicles equipped with LIDAR scanner. This paper will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some generic interference scenarios and report on the current status of the analysis of interference mechanisms.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
booktitle = {2015 12th Int. Conf. Inf. Technol. - New Gener.},
doi = {10.1109/ITNG.2015.113},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/An Experiment of Mutual Interference between Automotive LIDAR Scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {978-1-4799-8828-0},
keywords = {LIDAR scanner,autonomous vehicle,ghost target,mutual interference,obstacle detection},
month = {apr},
pages = {680--685},
publisher = {IEEE},
title = {{An Experiment of Mutual Interference between Automotive LIDAR Scanners}},
url = {http://ieeexplore.ieee.org/document/7113553/},
year = {2015}
}
@book{OpenSourceRoboticFoundation2014,
abstract = {ROS (Robot Operating System) provides libraries and tools to help software developers create robot applications. It provides hardware abstraction, device drivers, libraries, visualizers, message-passing, package management, and more. ROS is licensed under an open source, BSD license.},
author = {OpenSourceRoboticFoundation},
doi = {10.1007/978-3-319-91590-6},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/OpenSourceRoboticFoundation/Robot Operating System. OpenSourceRoboticFoundation. 2014.pdf:pdf},
isbn = {978-3-319-26052-5},
issn = {17 Februar 2014},
number = {Volume 1},
title = {{Robot Operating System}},
url = {http://www.ros.org/},
volume = {1},
year = {2014}
}
@article{Jiao2019,
abstract = {Multiple lidars are prevalently used on mobile vehicles for rendering a broad view to enhance the performance of localization and perception systems. However, precise calibration of multiple lidars is challenging since the feature correspondences in scan points cannot always provide enough constraints. To address this problem, the existing methods require fixed calibration targets in scenes or rely exclusively on additional sensors. In this paper, we present a novel method that enables automatic lidar calibration without these restrictions. Three linearly independent planar surfaces appearing in surroundings is utilized to find correspondences. Two components are developed to ensure the extrinsic parameters to be found: a closed-form solver for initialization and an optimizer for refinement by minimizing a nonlinear cost function. Simulation and experimental results demonstrate the high accuracy of our calibration approach with the rotation and translation errors smaller than 0.05rad and 0.1m respectively.},
archivePrefix = {arXiv},
arxivId = {1904.12116},
author = {Jiao, Jianhao and Liao, Qinghai and Zhu, Yilong and Liu, Tianyu and Yu, Yang and Fan, Rui and Wang, Lujia and Liu, Ming},
eprint = {1904.12116},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Jiao et al/A Novel Dual-Lidar Calibration Algorithm Using Planar Surfaces. Jiao et al.. 2019.pdf:pdf},
title = {{A Novel Dual-Lidar Calibration Algorithm Using Planar Surfaces}},
url = {http://arxiv.org/abs/1904.12116},
year = {2019}
}
@book{Alajlan2016,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2016.In this tutorial chapter, we demonstrate how to integrate a new planner into ROS and present their benefits. Extensive experimentations are performed to show the effectiveness of the newly integrated planners as compared to Robot Operating System (ROS) default planners. The navigation stack of the ROS opensource middleware incorporates both global and local path planners to support ROS-enabled robot navigation. Only basic algorithms are defined for the global path planner including Dijkstra, A*, and carrot planners. However, more intelligent global planners have been defined in the literature but were not integrated in ROS distributions. This tutorial was developed under Ubuntu 12.4 and for ROS Hydro version. However, it is expected to also work with Groovy (not tested). A repository of the new path planner is available at https://github.com/coins-lab/relaxed{\_}astar. A video tutorial also available at ttps://www.youtube.com/playlist? list=PL8UbFU8tzwRjkxccq2zLkmTkOOYela5fu.},
author = {Alajlan, Maram and Koub{\^{a}}a, Anis},
booktitle = {Stud. Comput. Intell.},
doi = {10.1007/978-3-319-26054-9_4},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Alajlan, Koub{\^{a}}a/Writing global path planners plugins in ROS A tutorial. Alajlan, Koub{\^{a}}a. 2016.pdf:pdf},
isbn = {9783319260525},
issn = {1860949X},
keywords = {Global path planner,Navigation stack,ROS},
number = {Volume 1},
pages = {73--97},
title = {{Writing global path planners plugins in ROS: A tutorial}},
volume = {625},
year = {2016}
}
@article{Fremont2013,
author = {Fremont, Vincent and Alberto, Sergio and Florez, Rodriguez and Bonnifait, Philippe and Targets, Circular and Video, Alignment and Sensors, Lidar and Robotics, Advanced and Stm, Francis},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fremont et al/Circular Targets for 3D Alignment of Video and Lidar Sensors To cite this version Circular Targets for 3D Alignment of Video and Lidar.pdf:pdf},
title = {{Circular Targets for 3D Alignment of Video and Lidar Sensors To cite this version : Circular Targets for 3D Alignment of Video and Lidar Sensors}},
year = {2013}
}
@article{Zhang,
author = {Zhang, Qilong and Pless, Robert},
doi = {10.1109/IROS.2004.1389752},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Zhang, Pless/Tendencias de TI. Zhang, Pless. Unknown.pdf:pdf},
title = {{Tendencias de TI}}
}
@article{Workshop,
author = {Workshop, Training},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Workshop/Processing Camera Data. Workshop. Unknown.pdf:pdf},
title = {{Processing Camera Data}}
}
@inproceedings{Yoneda2014,
abstract = {In recent years, automated vehicle researches move on to the next stage, that is auto-driving experiments on public roads. Major challenge is how to robustly drive at complicated situations such as narrow or non-featured road. In order to realize practical performance, some static information should be kept on memory such as road topology, building shape, white line, curb, traffic light and so on. Currently, some measurement companies have already begun to prepare map database for automated vehicles. They are able to provide highly-precise 3-D map for robust automated driving. This study focuses on what kind of data should be observed during automated driving with such precise database. In particular, we focus on the accurate localization based on the use of lidar data and precise 3-D map, and propose a feature quantity for scan data based on distribution of clusters. Localization experiment shows that our method can measure surrounding uncertainty and guarantee accurate localization.},
author = {Yoneda, Keisuke and Tehrani, Hossein and Ogawa, Takashi and Hukuyama, Naohisa and Mita, Seiichi},
booktitle = {IEEE Intell. Veh. Symp. Proc.},
doi = {10.1109/IVS.2014.6856596},
isbn = {9781479936380},
issn = {10901981},
pages = {1345--1350},
pmid = {12693519},
title = {{Lidar scan feature for localization with highly precise 3-D map}},
year = {2014}
}
@article{Velas,
author = {Veľas, Martin and {\v{S}}paa{\v{e}}l, Mi},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sensor Fusion and Calibration of Velodyne LiDAR and RGB Camera. Veľas, {\v{S}}paa{\v{e}}l.pdf:pdf},
title = {{Sensor Fusion and Calibration of Velodyne LiDAR and RGB Camera}}
}
@article{Joseph,
author = {Joseph, Lentin},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Joseph/Learning Robots Using Python. Joseph. Unknown.pdf:pdf},
title = {{Learning Robots Using Python}}
}
@article{Do2013,
abstract = {Multi-modal data processing based on visual and depth/range images has become relevant in computer vision for 3D reconstruction applications such as city modeling, robot navigation etc. In this paper, we generate high-accuracy dense range images from sparse point clouds to facilitate such applications. Our proposal addresses the problem of sparse data, mixed-pixels at the discontinuities and occlusions by combining multi-scale range images. The visual results show that our algorithm can create high-resolution dense range images with sharp discontinuities, while preserving the topology of objects even for environments that contain occlusions. To demonstrate the effectiveness of our approach, we propose an iterative perspective-to-point algorithm that aligns the edges between the color image and the range image from various viewpoints. The experimental results from 46 viewpoints show that the camera pose can be corrected when using high-accuracy dense range images, so that 3D reconstruction or 3D rendering can obtain a clearly higher quality. {\textcopyright} 2013 IEEE.},
author = {Do, Luat and Ma, Lingni and {De With}, Peter H.N.},
doi = {10.1109/WORV.2013.6521928},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Do, Ma, De With/Dense range images from sparse point clouds using multi-scale processing. Do, Ma, De With. 2013.pdf:pdf},
isbn = {9781467356466},
journal = {2013 IEEE Work. Robot Vision, WORV 2013},
pages = {138--143},
title = {{Dense range images from sparse point clouds using multi-scale processing}},
year = {2013}
}
@book{Cameras2015a,
author = {Cameras, Gige Vision},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Cameras/Allied Vision Manta Technical Manual. Cameras. 2015.pdf:pdf},
isbn = {6048758855},
number = {March},
pages = {1--74},
title = {{Allied Vision Manta Technical Manual}},
year = {2015}
}
@article{El-Ashmawy2014,
abstract = {Airborne Laser Scanning systems with light detection and ranging (LiDAR) technology is one of the fast and accurate 3D point data acquisition techniques. Generating accurate digital terrain and/or surface models (DTM/DSM) is the main application of collecting LiDAR range data. Recently, LiDAR range and intensity data have been used for land cover classification applications. Data range and Intensity, (strength of the backscattered signals measured by the LiDAR systems), are affected by the flying height, the ground elevation, scanning angle and the physical characteristics of the objects surface. These effects may lead to uneven distribution of point cloud or some gaps that may affect the classification process. Researchers have investigated the conversion of LiDAR range point data to raster image for terrain modelling. Interpolation techniques have been used to achieve the best representation of surfaces, and to fill the gaps between the LiDAR footprints. Interpolation methods are also investigated to generate LiDAR range and intensity image data for land cover classification applications. In this paper, different approach has been followed to classifying the LiDAR data (range and intensity) for land cover mapping. The methodology relies on the classification of the point cloud data based on their range and intensity and then converted the classified points into raster image. The gaps in the data are filled based on the classes of the nearest neighbour. Land cover maps are produced using two approaches using: (a) the conventional raster image data based on point interpolation; and (b) the proposed point data classification. A study area covering an urban district in Burnaby, British Colombia, Canada, is selected to compare the results of the two approaches. Five different land cover classes can be distinguished in that area: buildings, roads and parking areas, trees, low vegetation (grass), and bare soil. The results show that an improvement of around 10 {\%} in the classification results can be achieved by using the proposed approach.},
author = {El-Ashmawy, N. and Shaker, A.},
doi = {10.5194/isprsarchives-XL-7-79-2014},
issn = {2194-9034},
journal = {ISPRS - Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.},
keywords = {Intensity data,Land Cover classification,LiDAR,Point Clouds},
month = {sep},
number = {7},
pages = {79--83},
title = {{Raster Vs. Point Cloud LiDAR Data Classification}},
url = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-7/79/2014/},
volume = {XL-7},
year = {2014}
}
@article{Class2017,
author = {Class, Pixel and Size, Optical and Model, Sensor},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Class, Size, Model/Pin assignment I O connector. Class, Size, Model. 2017.pdf:pdf},
pages = {2--4},
title = {{Pin assignment I / O connector}},
year = {2017}
}
@article{Chien2017,
abstract = {{\textcopyright} 2016 IEEE. Recently LiDAR-camera systems have rapidly emerged in many applications. The integration of laser range-finding technologies into existing vision systems enables a more comprehensive understanding of 3D structure of the environment. The advantage, however, relies on a good geometrical calibration between the LiDAR and the image sensors. In this paper we consider visual odometry, a discipline in computer vision and robotics, in the context of recently emerging online sensory calibration studies. By embedding the online calibration problem into a LiDAR-monocular visual odometry technique, the temporal change of extrinsic parameters can be tracked and compensated effectively.},
author = {Chien, Hsiang Jen and Klette, Reinhard and Schneider, Nick and Franke, Uwe},
doi = {10.1109/ICPR.2016.7900068},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Chien et al/Visual odometry driven online calibration for monocular LiDAR-camera systems. Chien et al.. 2017.pdf:pdf},
isbn = {9781509048472},
issn = {10514651},
journal = {Proc. - Int. Conf. Pattern Recognit.},
pages = {2848--2853},
publisher = {IEEE},
title = {{Visual odometry driven online calibration for monocular LiDAR-camera systems}},
year = {2017}
}
@article{Wei2018,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@article{2019,
author = {Самарин, В. В. and Науменко, М. А.},
doi = {10.1134/s0367676519040239},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Самарин, Науменко/Изучение основных состояний ядер 6, 7, 9, 11 Li м.pdf:pdf},
issn = {0367-6765},
journal = {Известия Российской Академии Наук. Серия Физическая},
number = {4},
pages = {460--468},
title = {{ Изучение основных состояний ядер 6, 7, 9, 11 Li методом фейнмановских континуальных интегралов }},
volume = {83},
year = {2019}
}
@article{Yu2018,
abstract = {Datasets drive vision progress and autonomous driving is a critical vision application, yet existing driving datasets are impoverished in terms of visual content. Driving imagery is becoming plentiful, but annotation is slow and expensive, as annotation tools have not kept pace with the flood of data. Our first contribution is the design and implementation of a scalable annotation system that can provide a comprehensive set of image labels for large-scale driving datasets. Our second contribution is a new driving dataset, facilitated by our tooling, which is an order of magnitude larger than previous efforts, and is comprised of over 100K videos with diverse kinds of annotations including image level tagging, object bounding boxes, drivable areas, lane markings, and full-frame instance segmentation. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models so that they are less likely to be surprised by new conditions. The dataset can be requested at http://bdd-data.berkeley.edu.},
archivePrefix = {arXiv},
arxivId = {1805.04687},
author = {Yu, Fisher and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Liao, Mike and Madhavan, Vashisht and Darrell, Trevor},
eprint = {1805.04687},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yu et al/BDD100K A Diverse Driving Video Database with Scalable Annotation Tooling. Yu et al.. 2018.pdf:pdf},
pages = {1--16},
title = {{BDD100K: A Diverse Driving Video Database with Scalable Annotation Tooling}},
url = {http://arxiv.org/abs/1805.04687},
year = {2018}
}
@article{Girardeau-Montaut2005,
abstract = {{\textcopyright} 2005 International Society for Photogrammetry and Remote Sensing. All rights reserved. Ground based laser scanning is now well settled in the fields of cultural heritage, as-built modelling and monitoring applications. We intend to use laser scanning to detect changes on building sites or inside facilities, mainly for monitoring purposes but also to be able to provide synthetic information in case of an emergency. In both cases, the main constraint is time. A minimum precision on the measure of movements is also required, depending on the type of application. Laser scanner seems to be the perfect tool for such applications as it quickly acquires a large amount of accurate 3D data. But the comparison of so huge datasets implies the use of appropriate structures and ad-hoc algorithms. A specific octree structure is described, and then several simple cloud-to-cloud comparison techniques are presented. The best one, based on the Hausdorff distance computation is improved on various points. Also, as a full automatic process seems still unachievable, a software framework has been developed. It intends to minimize human intervention and therefore prevents from wasting the LiDAR speed in time-consuming post-processing operations.},
author = {Girardeau-Montaut, D. and Roux, M. and Marc, R. and Thibault, G.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Girardeau-Montaut et al/Change detection on points cloud data acquired with a ground laser scanner. Girardeau-Montaut et al.. 2005.pdf:pdf},
issn = {16821750},
journal = {Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. - ISPRS Arch.},
keywords = {Change detection,Ground LiDAR,Laser scanning,Monitoring,Points cloud comparison},
title = {{Change detection on points cloud data acquired with a ground laser scanner}},
volume = {36},
year = {2005}
}
@article{Slabaugh,
author = {Slabaugh, Gregory G},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Slabaugh/Rotation{\_}Matrix{\_}To{\_}Euler.Pdf. Slabaugh. Unknown.pdf:pdf},
pages = {1--7},
title = {{{\textless}Rotation{\_}Matrix{\_}To{\_}Euler.Pdf{\textgreater}}}
}
@article{Geiger2012,
abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
doi = {10.1109/CVPR.2012.6248074},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger, Lenz, Urtasun/Are we ready for autonomous driving the KITTI vision benchmark suite. Geiger, Lenz, Urtasun. 2012.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {3354--3361},
title = {{Are we ready for autonomous driving? the KITTI vision benchmark suite}},
year = {2012}
}
@article{Kammerl2011,
author = {Kammerl, Julius},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kammerl/Octrees Point Cloud Compression and Change Detection using Octrees. Kammerl. 2011.pdf:pdf},
journal = {Icra},
title = {{Octrees: Point Cloud Compression and Change Detection using Octrees}},
year = {2011}
}
@article{Velodyne2015a,
author = {Velodyne},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Velodyne/Webserver User Guide VLP-­‐16 {\&} HDL-­‐32E. Velodyne. 2015.pdf:pdf},
title = {{Webserver User Guide VLP-­‐16 {\&} HDL-­‐32E}},
year = {2015}
}
@article{Chum2005,
abstract = {The problem of model parameters estimation from data with a presence of outlier measurements often arises in computer vision and methods of robust estimation have to be used. The RANSAC algorithm introduced by Fishler and Bolles in 1981 is the a widely used robust estimator in the field of computer vision. The algorithm is capable of providing good estimates from data contaminated by large (even significantly more than 50{\%}) fraction of outliers. RANSAC is an optimization method that uses a data-driven random sampling of the parameter space to find the extremum of the cost function. Samples of data define points of the parameter space in which the cost function is evaluated and model parameters with the best score are output. This thesis provides a detailed analysis of RANSAC, which is recast as time-constrained op- timization – a solution that is optimal with certain confidence is sought in the shortest possible time. Next, the concept of randomized cost function evaluation in RANSAC is introduced and its superiority over the deterministic evaluation is shown. A provably optimal strategy for the ran- domized cost function evaluation is derived. A known discrepancy, caused by noise on inliers, between theoretical prediction of the time required to find the solution and practically observed running times is traced to a tacit assump- tions of RANSAC. The proposed LO-RANSAC algorithm reaches almost perfect agreement with theoretical predictions without any negative impact on the time complexity. A unified method of estimation of model and its degenerate configuration (epipolar geome- try and homography of a dominant plane) at the same time without a priori knowledge of the presence of the degenerate configuration (dominant plane) is derived. Next, it is shown that using oriented geometric constraints that arise from a realistic model of physical camera devices, saves non-negligible fraction of computational time. No negative side effect are related to the application of the oriented constraints. An algorithm exploiting (possibly noisy) match quality to modify the sampling strategy is introduced. The quality of a match is an often freely available quantity in the matching prob- lem. The approach increases the efficiency of the algorithm while keeping the same robustness as RANSAC in the worst-case situation (when the match quality is unrelated to whether a corre- spondence is a mismatch or not). Most of the algorithms in the thesis are motivated by (and presented on) estimation of a multi- view geometry. The algorithms are, however, general robust estimation techniques and can be easily used in other application areas too.},
author = {Chum, Ondrej},
doi = {10.1016/S0921-8777(99)00013-0},
isbn = {3015947974},
issn = {1213-2365},
journal = {Phd Thesis},
pmid = {10422537},
title = {{Two-View Geometry Estimation by Random Sample and Consensus}},
url = {http://cmp.felk.cvut.cz/{~}chum/papers/Chum-PhD.pdf},
year = {2005}
}
@article{Larusso2013a,
abstract = {Abstract: "A common need in machine vision is to compute the 3-D rigid transformation that exists between two sets of points in a coordinate system for which corresponding pairs have been determined. Several solutions to this problem have been devised. In this paper a comparative analysis of four popular algorithms is given. Each of them computes the translation and rotation of the transform in closed-form, as the solution to a least squares formulation of the problem. They differ in terms of the representation of the transform and the method of solution, using respectively: singular value decomposition of a matrix, orthonormal matrices, unit quaternions and dual quaternions. This comparison presents results of several experiments designed to determine (1) the accuracy of each algorithm in the presence of different levels of noise, (2) the stability of each technique with respect to degenerate data sets, and (3) the relative computation time of each approach for different sizes and forms of data."},
author = {Larusso, A and Eggert, DW and Fisher, RB},
doi = {10.5244/c.9.24},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Larusso, Eggert, Fisher/A Comparison of Four Algorithms for Estimating 3-D Rigid Transformations.. Larusso, Eggert, Fisher. 2013(2).pdf:pdf},
pages = {24.1--24.10},
title = {{A Comparison of Four Algorithms for Estimating 3-D Rigid Transformations.}},
year = {2013}
}
@article{Chen2018,
abstract = {{\textless}p{\textgreater}{\textless}strong{\textgreater}Abstract.{\textless}/strong{\textgreater} In recent years, growing public interest in three-dimensional technology has led to the emergence of affordable platforms that can capture 3D scenes for use in a wide range of consumer applications. These platforms are often widely available, inexpensive, and can potentially find dual use in taking measurements of indoor spaces for creating indoor maps. Their affordability, however, usually comes at the cost of reduced accuracy and precision, which becomes more apparent when these instruments are pushed to their limits to scan an entire room. The point cloud measurements they produce often exhibit systematic drift and random noise that can make performing comparisons with accurate data difficult, akin to trying to compare a fuzzy trapezoid to a perfect square with sharp edges. This paper outlines a process for assessing the accuracy and precision of these imperfect point clouds in the context of indoor mapping by integrating techniques such as the extended Gaussian image, iterative closest point registration, and histogram thresholding. A case study is provided at the end to demonstrate use of this process for evaluating the performance of the Scanse Sweep 3D, an ultra-low cost panoramic laser scanner.{\textless}/p{\textgreater}},
author = {Chen, J. and Mora, O. E. and Clarke, K. C.},
doi = {10.5194/isprs-annals-IV-4-W6-3-2018},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Chen, Mora, Clarke/Assessing the accuracy and precision of imperfect point clouds for 3D indoor mapping and modeling. Chen, Mora, Clarke. 2018.pdf:pdf},
issn = {21949050},
journal = {ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci.},
keywords = {Point clouds,indoor mapping,remote sensing},
number = {4/W6},
pages = {3--10},
title = {{Assessing the accuracy and precision of imperfect point clouds for 3D indoor mapping and modeling}},
volume = {4},
year = {2018}
}
@article{Zhang2014,
abstract = {Vehicle detection is important for advanced driver assistance systems (ADAS). Both LiDAR and cameras are often used. LiDAR provides excellent range information but with limits to object identification; on the other hand, the camera allows for better recognition but with limits to the high reso- lution range information. This paper presents a sensor fusion based vehicle detection approach by fusing information from both LiDAR and cameras. The proposed approach is based on two components: a hypothesis generation phase to generate positions that potential represent vehicles and a hypothesis ver- ification phase to classify the corresponding objects. Hypothesis generation is achieved using the stereo camera while verification is achieved using the LiDAR. The main contribution is that the complementary advantages of two sensors are utilized, with the goal of vehicle detection. The proposed approach leads to an enhanced detection performance; in addition, maintains tolerable false alarm rates compared to vision based classifiers. Experimental results suggest a performance which is broadly comparable to the current state of the art, albeit with reduced false alarm rate.},
author = {Zhang, Feihu and Clarke, Daniel and Knoll, Alois},
doi = {10.1109/ITSC.2014.6957925},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vehicle detection based on LiDAR and camera fusion. Zhang, Clarke, Knoll.pdf:pdf},
isbn = {9781479960781},
journal = {2014 17th IEEE Int. Conf. Intell. Transp. Syst. ITSC 2014},
pages = {1620--1625},
publisher = {IEEE},
title = {{Vehicle detection based on LiDAR and camera fusion}},
year = {2014}
}
@article{Ali2019,
abstract = {Object detection and classification in 3D is a key task in Automated Driving (AD). LiDAR sensors are employed to provide the 3D point cloud reconstruction of the surrounding environment, while the task of 3D object bounding box detection in real time remains a strong algorithmic challenge. In this paper, we build on the success of the one-shot regression meta-architecture in the 2D perspective image space and extend it to generate oriented 3D object bounding boxes from LiDAR point cloud. Our main contribution is in extending the loss function of YOLO v2 to include the yaw angle, the 3D box center in Cartesian coordinates and the height of the box as a direct regression problem. This formulation enables real-time performance, which is essential for automated driving. Our results are showing promising figures on KITTI benchmark, achieving real-time performance (40 fps) on Titan X GPU.},
archivePrefix = {arXiv},
arxivId = {arXiv:1808.02350v1},
author = {Ali, Waleed and Abdelkarim, Sherif and Zidan, Mahmoud and Zahran, Mohamed and Sallab, Ahmad El},
doi = {10.1007/978-3-030-11015-4_54},
eprint = {arXiv:1808.02350v1},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ali et al/YOLO3D End-to-end real-time 3D oriented object bounding box detection from LiDAR point cloud. Ali et al.. 2019.pdf:pdf},
isbn = {9783030110147},
issn = {16113349},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
keywords = {3D object detection,LiDAR,Real-time},
number = {August},
pages = {716--728},
title = {{YOLO3D: End-to-end real-time 3D oriented object bounding box detection from LiDAR point cloud}},
volume = {11131 LNCS},
year = {2019}
}
@article{Balance2016,
author = {Balance, White},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Balance/White Balance for Allied Vision GigE Cameras Introduction White Balance and Auto White Balance explained. Balance. 2016.pdf:pdf},
pages = {1--8},
title = {{White Balance for Allied Vision GigE Cameras Introduction White Balance and Auto White Balance explained}},
year = {2016}
}
@article{ElSallab2018,
abstract = {In this paper, YOLO4D is presented for Spatio-temporal Real-time 3D Multi-object detection and classification from LiDAR point clouds. Automated Driving dynamic scenarios are rich in temporal information. Most of the current 3D Object Detection approaches are focused on processing the spatial sensory features, either in 2D or 3D spaces, while the temporal factor is not fully exploited yet, especially from 3D LiDAR point clouds. In YOLO4D approach, the 3D LiDAR point clouds are aggregated over time as a 4D tensor; 3D space dimensions in addition to the time dimension, which is fed to a one-shot fully convolutional detector, based on YOLO v2 architecture. The outputs are the oriented 3D Object Bounding Box information, together with the object class. Two different techniques are evaluated to incorporate the temporal dimension; recurrence and frame stacking. The experiments conducted on KITTI dataset, show the advantages of incorporating the temporal dimension.},
author = {{El Sallab}, Ahmad and Sobh, Ibrahim},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/El Sallab, Sobh/YOLO4D A Spatio-temporal Approach for Real-time Multi-object Detection and Classification from LiDAR Point Clouds. El Sallab, Sobh. 201.pdf:pdf},
journal = {NIPS '18 Work.},
number = {Nips},
pages = {8},
title = {{YOLO4D : A Spatio-temporal Approach for Real-time Multi-object Detection and Classification from LiDAR Point Clouds}},
year = {2018}
}
@article{Brown1971,
abstract = {For highest accuracies it is necessary in close range photogrammetry to account for the variation of lens distortion within the photographic field. A theory to accomplish this is developed along with practical method for calibrating radial and decentering distortion of close-range cameras. This method, the analytical plumb line method, is applied in an experimental investigation leading to confirmation of the validity of the theoretical development accounting for variation of distortion with object distance.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Brown, Duane C},
doi = {10.1.1.14.6358},
eprint = {arXiv:1011.1669v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Brown/Close-range camera calibration. Brown. 1971.pdf:pdf},
isbn = {2006267816819},
issn = {03331024},
journal = {Photogramm. Eng.},
keywords = {PHOTOGRAMMETRIC ENGINEERING AND REMOTE SENSING,},
number = {8},
pages = {855--866},
pmid = {16776696},
title = {{Close-range camera calibration}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.6358},
volume = {37},
year = {1971}
}
@article{Vision,
author = {Vision, Allied and Line, Camera},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Vision, Line/High-performance machine vision cameras Better vision for your application. Vision, Line. Unknown.pdf:pdf},
title = {{High-performance machine vision cameras Better vision for your application}}
}
@article{Zhou2018,
author = {Zhou, Lipu and Li, Zimo and Kaess, Michael},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Zhou, Li, Kaess/Automatic Extrinsic Calibration of a Camera and a 3D LiDAR using Line and Plane Correspondences. Zhou, Li, Kaess. Unknown.pdf:pdf},
title = {{Automatic Extrinsic Calibration of a Camera and a 3D LiDAR using Line and Plane Correspondences}}
}
@article{Damm2016,
abstract = {With the ongoing spread of autonomous vehicles, challenges like obstacle
avoidance get more important. To realize obstacle avoidance, a reliable obstacle
detection is one of the preconditions. While common autonomous vehicles
mainly use camera and radar sensors for this purpose, currently laser
range sensors are enforcing as alternatives. Due to its high accuracy, this
kind of sensor establishes in different industries. In general, the sensor data
is used as point clouds. Within this master's thesis, an approach for obstacle
detection based on these point clouds is presented. Therefore, several
subtasks, e.g. downsampling and plane segmentation, of a reliable obstacle
detection are carried out. Finally, an algorithm for obstacle tracking, based
on a linear Kalman filter, is implemented. The received results are evaluated
within several test drives of the autonomous vehicle MadeInGermany (MIG).},
author = {Damm, Christian},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Damm/Object detection in 3D point clouds. Damm. 2016.pdf:pdf},
title = {{Object detection in 3D point clouds}},
url = {http://www.mi.fu-berlin.de/inf/groups/ag-ki/Theses/Completed-theses/Master{\_}Diploma-theses/2016/Damm/Master-Damm.pdf},
year = {2016}
}
@article{Sprickerhof2012,
author = {Sprickerhof, Jochen},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Sprickerhof/Registration PCL. Sprickerhof. 2012.pdf:pdf},
title = {{Registration PCL}},
url = {http://www.pointclouds.org/assets/icra2013/registration.pdf},
year = {2012}
}
@article{Functionalities2012a,
author = {Functionalities, New},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Functionalities/HDL32E Version 2 . 0 Software Overview. Functionalities. 2012.pdf:pdf},
title = {{HDL32E Version 2 . 0 Software Overview}},
volume = {2012},
year = {2012}
}
@article{Kim2015c,
abstract = {The LIDAR scanner is at the heart of object detection of the self-driving car. Mutual interference between LIDAR scanners has not been regarded as a problem because the percentage of vehicles equipped with LIDAR scanners was very rare. With the growing number of autonomous vehicle equipped with LIDAR scanner operated close to each other at the same time, the LIDAR scanner may receive laser pulses from other LIDAR scanners. In this paper, three types of experiments and their results are shown, according to the arrangement of two LIDAR scanners. We will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some typical mutual interference scenario and report an analysis of the interference mechanism. {\textcopyright} 2015 COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Seonghyeon and Park, Yongwan},
doi = {10.1117/12.2178502},
editor = {Prochazka, Ivan and Sobolewski, Roman and James, Ralph B.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Occurrence and characteristics of mutual interference between LIDAR scanners. Kim et al.. 2015.pdf:pdf},
journal = {Phot. Count. Appl. 2015},
keywords = {LIDAR scanner,ghost target,mutual interference,self-driving car,time of flight},
month = {may},
number = {September},
pages = {95040K},
publisher = {International Society for Optics and Photonics},
title = {{Occurrence and characteristics of mutual interference between LIDAR scanners}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2178502},
volume = {9504},
year = {2015}
}
@article{PCLRegistration2011,
author = {PCLRegistration},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/PCLRegistration/PCL Registration. PCLRegistration. 2011.pdf:pdf},
journal = {PCLRegistration},
title = {{PCL Registration}},
url = {http://www.pointclouds.org/assets/iros2011/registration.pdf},
year = {2011}
}
@article{Lin2014a,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.0312v3},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {arXiv:1405.0312v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lin et al/Microsoft COCO Common objects in context. Lin et al.. 2014.pdf:pdf},
issn = {16113349},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@article{Wurm2010,
abstract = {In this paper, we present an approach for modeling 3D environments based on octrees using a probabilistic occupancy estimation. Our technique is able to represent full 3D models including free and unknown areas. It is available as an open-source library to facilitate the development of 3D mapping systems. We also provide a detailed review of existing approaches to 3D modeling. Our approach was thoroughly evaluated using different real-world and simulated datasets. The results demonstrate that our approach is able to model the data probabilistically while, at the same time, keeping the memory requirement at a minimum.},
author = {Wurm, K M and Hornung, a and Bennewitz, M and Stachniss, C and Burgard, W},
doi = {10.1.1.176.7242},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wurm et al/OctoMap A probabilistic, flexible, and compact 3D map representation for robotic systems. Wurm et al.. 2010.pdf:pdf},
issn = {0929-5593},
journal = {Proc ICRA 2010 Work. best Pract. 3D Percept. Model. Mob. Manip.},
keywords = {3d environments,probabilistic occupancy estimation},
number = {3},
pages = {403--412},
title = {{OctoMap: A probabilistic, flexible, and compact 3D map representation for robotic systems}},
url = {http://ais.informatik.uni-freiburg.de/publications/papers/wurm10octomap.pdf},
volume = {16},
year = {2010}
}
@book{Joseph2015,
abstract = {Mastering ROS for Robotics Programming is an advanced guide of ROS that is very suitable for readers who already have a basic knowledge in ROS. ROS is widely used in robotics companies, universities, and robotics research institutes for designing, building, and simulating a robot model and interfacing it into real hardware. ROS is now an essential requirement for Robotic engineers; this guide can help you acquire knowledge of ROS and can also help you polish your skills in ROS using interactive examples. Even though it is an advanced guide, you can see the basics of ROS in the first chapter to refresh the concepts. It also helps ROS beginners. The book mainly focuses on the advanced concepts of ROS, such as ROS Navigation stack, ROS MoveIt!, ROS plugins, nodelets, controllers, ROS Industrial, and so on. You can work with the examples in the book without any special hardware; however, in some sections you can see the interfacing of I/O boards, vision sensors, and actuators to ROS. To work with this hardware, you will need to buy it. The book starts with an introduction to ROS and then discusses how to build a robot model in ROS for simulating and visualizing. After the simulation of robots using Gazebo, we can see how to connect the robot to Navigation stack and MoveIt!. In addition to this, we can see ROS plugins, controllers, nodelets, and interfacing of I/O boards and vision sensors. Finally, we can see more about ROS Industrial and troubleshooting and best practices in ROS.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Joseph, Lentin},
booktitle = {Publ. by Packt Publ. Ltd},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Joseph/Mastering ROS for Robotics Programming. Joseph. 2015.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
keywords = {Bott},
number = {1},
pages = {1--5},
pmid = {15003161},
title = {{Mastering ROS for Robotics Programming}},
volume = {22},
year = {2015}
}
@book{..1386a,
author = {{مختاري، پونه. شجاعي، معصومه. دانا، امير}},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/مختاري، پونه. شجاعي، معصومه. دانا، امير/No Titleتأثير تم.pdf:pdf},
isbn = {9781312392663},
pages = {32 ، ص 117},
title = {{No Titleتأثير تمرين مشاهدهاي بر يادگيري مهارت سرويس . بلند بدمينتون با تأكيد بر نقش ميانجيگري خودكارآمدي". مجلة حركت}},
year = {1386}
}
