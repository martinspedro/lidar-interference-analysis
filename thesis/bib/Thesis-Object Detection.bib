Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Redmon2016,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02640v5},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {arXiv:1506.02640v5},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Redmon et al/You only look once Unified, real-time object detection. Redmon et al.. 2016.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {779--788},
title = {{You only look once: Unified, real-time object detection}},
volume = {2016-Decem},
year = {2016}
}
@article{Surasak2018,
abstract = {Currently, Computer Vision (CV) is one of the most popular research topics in the world. This is because it can support the human daily life. Moreover, CV can also apply to various theories and researches. Human Detection is one of the most popular research topics in Computer Vision. In this paper, we present a study of technique for human detection from video, which is the Histograms of Oriented Gradients or HOG by developing a piece of application to import and detect the human from the video. We use the HOG Algorithm to analyze every frame from the video to find and count people. After analyzing video from starting to the end, the program generate histogram to show the number of detected people versus playing period of the video. As a result, the expected results are obtained, including the detection of people in the video and the histogram generation to show the appearance of human detected in the video file.},
author = {Surasak, Thattapon and Takahiro, Ito and Cheng, Cheng Hsuan and Wang, Chi En and Sheng, Pao You},
doi = {10.1109/ICBIR.2018.8391187},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Surasak et al/Histogram of oriented gradients for human detection in video. Surasak et al.. 2018.pdf:pdf},
isbn = {9781538652541},
journal = {Proc. 2018 5th Int. Conf. Bus. Ind. Res. Smart Technol. Next Gener. Information, Eng. Bus. Soc. Sci. ICBIR 2018},
keywords = {Histogram of Oriented Gradients,Human Detection},
pages = {172--176},
title = {{Histogram of oriented gradients for human detection in video}},
year = {2018}
}
@article{He2015,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224$\backslash$times 224) input image. This requirement is 'artificial' and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, 'spatial pyramid pooling', to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-The-Art classification results using a single full-image representation and no fine-Tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 $\backslash$times faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank {\#}2 in object detection and {\#}3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/TPAMI.2015.2389824},
eprint = {1406.4729},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/He et al/Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. He et al.. 2015.pdf:pdf},
isbn = {9783319105772},
issn = {01628828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
keywords = {Convolutional Neural Networks,Image Classification,Object Detection,Spatial Pyramid Pooling},
number = {9},
pages = {1904--1916},
pmid = {26353135},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
volume = {37},
year = {2015}
}
@misc{jsk_visualization,
author = {Ueda, Ryohei and Okada, Kei and Kakiuchi, Youhei},
title = {jsk{\_}visualization package},
url = {https://wiki.ros.org/jsk{\_}visualization https://github.com/jsk-ros-pkg/jsk{\_}visualization}
}
@article{Cleeremans1989,
abstract = {We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t-1, together with element t, to predict element t + 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to correspond to the nodes of the grammar, although this correspondence is not necessary for the network to act as a perfect finite-state recognizer. We explore the conditions under which the network can carry information about distant sequential contingencies across intervening elements. Such information is maintained with relative ease if it is relevant at each intermediate step; it tends to be lost when intervening elements do not depend on it. At first glance this may suggest that such networks are not relevant to natural language, in which dependencies may span indefinite distances. However, embeddings in natural language are not completely independent of earlier information. The final simulation shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information.},
author = {Cleeremans, Axel and Servan-Schreiber, David and McClelland, James L.},
doi = {10.1162/neco.1989.1.3.372},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Cleeremans, Servan-Schreiber, McClelland/Finite State Automata and Simple Recurrent Networks. Cleeremans, Servan-Schreiber, McClelland. 1989.pdf:pdf},
issn = {0899-7667},
journal = {Neural Comput.},
number = {3},
pages = {372--381},
title = {{Finite State Automata and Simple Recurrent Networks}},
volume = {1},
year = {1989}
}
@article{Huang2017,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L2+1) direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
doi = {10.1109/CVPR.2017.243},
eprint = {1608.06993},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Huang et al/Densely connected convolutional networks. Huang et al.. 2017.pdf:pdf},
isbn = {9781538604571},
journal = {Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017},
pages = {2261--2269},
title = {{Densely connected convolutional networks}},
volume = {2017-Janua},
year = {2017}
}
@article{Bay2008,
abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1016/j.cviu.2007.09.014},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Bay et al/Speeded-Up Robust Features (SURF). Bay et al.. 2008.pdf:pdf},
issn = {10773142},
journal = {Comput. Vis. Image Underst.},
keywords = {Camera calibration,Feature description,Interest points,Local features,Object recognition},
number = {3},
pages = {346--359},
title = {{Speeded-Up Robust Features (SURF)}},
volume = {110},
year = {2008}
}
@article{Girshick2014,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra and Berkeley, U C and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Girshick et al/1043.0690. Girshick et al.. 2014.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {5000},
pmid = {26656583},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://arxiv.},
volume = {1},
year = {2014}
}
@article{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Girshick/Fast R-CNN. Girshick. 2015.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
pages = {1440--1448},
title = {{Fast R-CNN}},
volume = {2015 Inter},
year = {2015}
}
@article{Szeliski2011a,
abstract = {As humans, we perceive the three-dimensional structure of the world around us with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem, and what is the current state of the art?Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging and fun consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos.More than just a source of "recipes", this text/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting this process to produce the best possible descriptions of a scene. Exercises are presented throughout the book, with a heavy emphasis on testing algorithms.Suitable for either an undergraduate or a graduate-level course in computer vision, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries.Dr. Richard Szeliski has over twenty years' experience in computer vision research, most notably at Digital Equipment Corporation and Microsoft.},
author = {Szeliski, Richard},
doi = {10.5860/choice.48-5140},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Szeliski/Computer vision algorithms and applications. Szeliski. 2011.pdf:pdf},
issn = {0009-4978},
journal = {Choice Rev. Online},
number = {09},
pages = {48--5140--48--5140},
title = {{Computer vision: algorithms and applications}},
volume = {48},
year = {2011}
}
@techreport{Redmon2018,
author = {Redmon, Joseph and Farhadi, Ali},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Redmon, Farhadi/YOLOv3 An Incremental Improvement. Redmon, Farhadi. 2018.pdf:pdf},
pages = {1--6},
title = {{YOLO v.3}},
url = {https://pjreddie.com/media/files/papers/YOLOv3.pdf},
year = {2018}
}
@article{Krizhevsky2007,
abstract = {Delineating the tremendous growth in this area, the Handbook of Approximation Algorithms and Metaheuristics covers fundamental, theoretical topics as well as advanced, practical applications. It is the first book to comprehensively study both approximation algorithms and metaheuristics. Starting with basic approaches, the handbook presents the methodologies to design and analyze efficient approximation algorithms for a large class of problems, and to establish inapproximability results for another class of problems. It also discusses local search, neural networks, and metaheuristics, as well as multiobjective problems, sensitivity analysis, and stability. After laying this foundation, the book applies the methodologies to classical problems in combinatorial optimization, computational geometry, and graph problems. In addition, it explores large-scale and emerging applications in networks, bioinformatics, VLSI, game theory, and data analysis. Undoubtedly sparking further developments in the field, this handbook provides the essential techniques to apply approximation algorithms and metaheuristics to a wide range of problems in computer science, operations research, computer engineering, and economics. Armed with this information, researchers can design and analyze efficient algorithms to generate near-optimal solutions for a wide range of computational intractable problems.},
author = {Krizhevsky, Alex and {Ilya Sutskever} and {Geoffrey E. Hinton}},
doi = {10.1201/9781420010749},
editor = {Gonzalez, Teofilo F.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton/ImageNet Classification with Deep Convolutional Neural Networks. Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton. 2007.pdf:pdf},
isbn = {9780429143793},
journal = {Handb. Approx. Algorithms Metaheuristics},
month = {may},
pages = {1--1432},
publisher = {Chapman and Hall/CRC},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://www.taylorfrancis.com/books/9781420010749},
year = {2007}
}
@article{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/He et al/Deep residual learning for image recognition. He et al.. 2016.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
volume = {2016-Decem},
year = {2016}
}
@article{Dalal2010,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
author = {Dalal, Navneet and Triggs, Bill and Dalal, Navneet and Triggs, Bill and Gradients, Oriented and International, Detection and Diego, San and Dalal, Navneet and Triggs, Bill},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Dalal et al/Histograms of Oriented Gradients for Human Detection To cite this version HAL Id inria-00548512 Histograms of Oriented Gradients for H.pdf:pdf},
journal = {IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
keywords = {feature extraction,gradient methods,object detec},
pages = {886--893},
title = {{Histograms of Oriented Gradients for Human Detection To cite this version : HAL Id : inria-00548512 Histograms of Oriented Gradients for Human Detection}},
year = {2010}
}
@inproceedings{Hughes2011a,
abstract = {An object recognition system has been developed that uses a new class oflocal image features. The features are invariant to image scaling, translation,and rotation, and partially in- variant to illumination changes and affine or 3D projection. These features share similar properties with neurons in in- ferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local ge- ometric deformations by representing blurred image gradi- ents in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final veri- fication ofeach match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time ofunder 2 seconds},
author = {Hughes, R.A.},
booktitle = {Int. Comput. Corfu},
doi = {10.1130/2011.2482(04)},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hughes/Geoscience data and derived spatial information Societal impacts and benefits, and relevance to geological surveys and agencies. Hughes..pdf:pdf},
isbn = {9780813724829},
issn = {00721077},
month = {sep},
title = {{Object Recognition from Local Scale-Invariant Features}},
url = {https://pubs.geoscienceworld.org/books/book/641/chapter/3806367/},
year = {1999}
}
@article{ViolaP2004,
author = {{Viola P.} and {Jones M.}},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Viola P., Jones M/Rapid Object Detection Using a Boosted Cascade of Simple Features.pdf (applicationpdf オブジェクト). Viola P., Jones M.. 2004.pdf:pdf},
title = {{Rapid Object Detection Using a Boosted Cascade of Simple Features}},
url = {file:///C:/Documents and Settings/owner/デスクトップ/Rapid Object Detection Using a Boosted.pdf},
year = {2004}
}
@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ren et al/Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks. Ren et al.. 2017.pdf:pdf},
issn = {01628828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
keywords = {Object detection,convolutional neural network,region proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
volume = {39},
year = {2017}
}
@article{Redmon2017,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster R-CNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
doi = {10.1109/CVPR.2017.690},
eprint = {1612.08242},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Redmon, Farhadi/YOLO9000 Better, faster, stronger. Redmon, Farhadi. 2017.pdf:pdf},
isbn = {9781538604571},
journal = {Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017},
pages = {6517--6525},
title = {{YOLO9000: Better, faster, stronger}},
volume = {2017-Janua},
year = {2017}
}
@article{Viola2001,
abstract = {This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction ofa new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number ofcritical visual features and yields extremely efficient classifiers [6]. The third contribution is a method for combining classifiers in a “cascade” which allows background regions ofthe image to be quickly discarded while spending more computation on promising object-like regions. A set ofexperiments in the domain offace detection are presented. The system yields face detection performace comparable to the best previous systems [18, 13, 16, 12, 1]. Implemented on a conventional desktop, face detection proceeds at 15 frames per second},
author = {Viola, Paul and Jones, Michael},
doi = {10.3917/ving.094.0121},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fitzpatrick/Une histoire trs catholique Rvisionnisme et orthodoxie dans l'historiographie irlandaise. Fitzpatrick. 2007.pdf:pdf},
issn = {0294-1759},
journal = {Second Int. Work. Stat. Comput. Theor. Vis. –MODELING,LEARNING,COMPUTING, Sampl.},
title = {{Robust Real-time Object Detection Paul}},
year = {2001}
}
@misc{Redmon2013,
author = {Redmon, Joseph},
title = {{Darknet: Open Source Neural Networks in C}},
url = {https://pjreddie.com/darknet/ http://pjreddie.com/darknet/},
urldate = {2019-10-24},
year = {2013}
}
@article{Messom2006,
abstract = {This paper introduces an extended set of Haarlike features beyond the standard vertically and horizontally aligned Haar-like features [Viola and Jones, 2001a; 2001b] and the 45o twisted Haar-like features [Lienhart and Maydt, 2002; Lienhart et al., 2003a; 2003b]. The extended rotated Haar-like features are based on the standard Haar-like features that have been rotated based on whole integer pixel based rotations. These rotated feature values can also be calculated using rotated integral images which means that they can be fast and efficiently calculated with just 8 operations irrespective of the feature size. In general each feature requires another 8 operations based on an identity integral image so that appropriate scaling corrections can be applied. These scaling corrections are needed due to the rounding errors associated with scaling the features. The errors introduced by these rotated features on natural images are small enough to allow rotated classifiers to be implemented using a classifier trained on only vertically aligned images. This is a significant improvement in training time for a classifier that is invariant to the rotations represented in the parallel classifier.},
author = {Messom, Chris and Barczak, Andre},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Messom, Barczak/Fast and efficient rotated haar-like features using rotated integral images. Messom, Barczak. 2006.pdf:pdf},
isbn = {9780958758383},
journal = {Proc. 2006 Australas. Conf. Robot. Autom. ACRA 2006},
pages = {4--9},
title = {{Fast and efficient rotated haar-like features using rotated integral images}},
year = {2006}
}
