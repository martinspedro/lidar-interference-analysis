Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Waymo,
author = {Waymo},
title = {{Open Dataset â€“ Waymo}},
url = {https://waymo.com/open/},
urldate = {2019-10-20}
}
@article{Huang2010,
abstract = {We describe the Lightweight Communications and Marshalling (LCM) library for message passing and data marshalling. The primary goal of LCM is to simplify the development of low-latency message passing systems, especially for real-time robotics research applications. Messages can be transmitted between different processes using LCM's publish/subscribe message-passing system. A platform- and language-independent type specification language separates message description from implementation. Message specifications are automatically compiled into language-specific bindings, eliminating the need for users to implement marshalling code while guaranteeing run-time type safety. LCM is notable in providing a real-time deep traffic inspection tool that can decode and display message traffic with minimal user effort and no impact on overall system performance. This and other features emphasize LCM's focus on simplifying both the development and debugging of message passing systems. In this paper, we explain the design of LCM, evaluate its performance, and describe its application to a number of autonomous land, underwater, and aerial robots. {\textcopyright}2010 IEEE.},
author = {Huang, Albert S. and Olson, Edwin and Moore, David C.},
doi = {10.1109/IROS.2010.5649358},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Huang, Olson, Moore/LCM Lightweight Communications and Marshalling. Huang, Olson, Moore. 2010.pdf:pdf},
isbn = {9781424466757},
journal = {IEEE/RSJ 2010 Int. Conf. Intell. Robot. Syst. IROS 2010 - Conf. Proc.},
number = {Lcm},
pages = {4057--4062},
title = {{LCM: Lightweight Communications and Marshalling}},
year = {2010}
}
@misc{TomasKrejci,
author = {{Tomas Krejci}},
title = {{GitHub - tomas789/kitti2bag: Convert KITTI dataset to ROS bag file the easy way!}},
url = {https://github.com/tomas789/kitti2bag}
}
@techreport{vlp16,
institution = {Velodyne LiDAR, Inc.},
title = {{VLP-16 User Manual}},
year = {2019}
}
@article{Geiger2013a,
abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high- resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to inner- city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide},
author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger et al/Vision meets robotics The KITTI dataset. The International Journal of Robotics Research. Geiger et al.. 2013.pdf:pdf},
journal = {Int. J. Robot. Res. - IJRR},
number = {October},
pages = {1--6},
title = {{Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research}},
year = {2013}
}
@article{nuScenes2019,
archivePrefix = {arXiv},
arxivId = {1903.11027},
author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
eprint = {1903.11027},
journal = {arXiv Prepr.},
title = {{nuScenes: A multimodal dataset for autonomous driving}},
url = {https://www.nuscenes.org/},
year = {2019}
}
@misc{udacity,
title = {{self-driving-car/datasets at master {\textperiodcentered} udacity/self-driving-car {\textperiodcentered} GitHub}},
url = {https://github.com/udacity/self-driving-car/tree/master/datasets},
urldate = {2019-10-20}
}
@article{Lin2014a,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.0312v3},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {arXiv:1405.0312v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lin et al/Microsoft COCO Common objects in context. Lin et al.. 2014.pdf:pdf},
issn = {16113349},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@article{Pandey2011,
abstract = {In this paper we describe a data set collected by an autonomous ground vehicle testbed, based upon a modified Ford F-250 pickup truck. The vehicle is outfitted with a professional (Applanix POS-LV) and consumer (Xsens MTi-G) inertial measurement unit, a Velodyne three-dimensional lidar scanner, two push-broom forward-looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system. Here we present the time-registered data from these sensors mounted on the vehicle, collected while driving the vehicle around the Ford Research Campus and downtown Dearborn, MI, during November-December 2009. The vehicle path trajectory in these data sets contains several large- and small-scale loop closures, which should be useful for testing various state-of-the-art computer vision and simultaneous localization and mapping algorithms. {\textcopyright} SAGE Publications 2011.},
author = {Pandey, Gaurav and McBride, James R. and Eustice, Ryan M.},
doi = {10.1177/0278364911400640},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pandey, McBride, Eustice/Ford Campus vision and lidar data set. Pandey, McBride, Eustice. 2011.pdf:pdf},
issn = {17413176},
journal = {Int. J. Rob. Res.},
keywords = {Mobile and distributed robotics SLAM,field and service robotics,sensing and perception computer vision},
number = {13},
pages = {1543--1552},
title = {{Ford Campus vision and lidar data set}},
url = {http://robots.engin.umich.edu/SoftwareData/Ford},
volume = {30},
year = {2011}
}
