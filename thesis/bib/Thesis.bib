Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Dai2015,
abstract = {This paper reviews the Euler-Rodrigues formula in the axis-angle representation of rotations, studies its variations and derivations in different mathematical forms as vectors, quaternions and Lie groups and investigates their intrinsic connections. The Euler-Rodrigues formula in the Taylor series expansion is presented and its use as an exponential map of Lie algebras is discussed particularly with a non-normalized vector. The connection between Euler-Rodrigues parameters and the Euler-Rodrigues formula is then demonstrated through quaternion conjugation and the equivalence between quaternion conjugation and an adjoint action of the Lie group is subsequently presented. The paper provides a rich reference for the Euler-Rodrigues formula, the variations and their connections and for their use in rigid body kinematics, dynamics and computer graphics.},
author = {Dai, Jian S.},
doi = {10.1016/j.mechmachtheory.2015.03.004},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Dai/Euler-Rodrigues formula variations, quaternion conjugation and intrinsic connections. Dai. 2015.pdf:pdf},
issn = {0094114X},
journal = {Mech. Mach. Theory},
keywords = {Euler-Rodrigues formula,Exponential map,Kinematics,Lie algebras,Lie groups,Quaternions},
pages = {144--152},
publisher = {The Author},
title = {{Euler-Rodrigues formula variations, quaternion conjugation and intrinsic connections}},
url = {http://dx.doi.org/10.1016/j.mechmachtheory.2015.03.004},
volume = {92},
year = {2015}
}
@article{Kim2015c,
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
doi = {10.1109/IVS.2015.7225724},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/Investigation on the occurrence of mutual interference between pulsed terrestrial LIDAR scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {9781467372664},
journal = {IEEE Intell. Veh. Symp. Proc.},
number = {Iv},
pages = {437--442},
publisher = {IEEE},
title = {{Investigation on the occurrence of mutual interference between pulsed terrestrial LIDAR scanners}},
volume = {2015-Augus},
year = {2015}
}
@misc{Photopillers,
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Cladera/The Ultimate Photography Guide to Depth of Field (DoF) PhotoPills. Cladera. Unknown.pdf:pdf},
title = {{The Ultimate Photography Guide to Depth of Field (DoF) | PhotoPills}},
url = {https://www.photopills.com/articles/ultimate-guide-depth-field},
urldate = {2019-10-21}
}
@misc{Lilumination2011,
author = {Lilumination, Field O F},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lilumination/(12) United States Patent. Lilumination. 2011.pdf:pdf},
number = {12},
title = {{(12) United States Patent}},
volume = {1},
year = {2011}
}
@article{Carlson2010,
abstract = {An Introduction to Signals and Noise in Electrical Communication},
author = {Carlson, A Bruce and Crilly, Paul B},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Carlson, Crilly/Communcation Systems. Carlson, Crilly. 2010.pdf:pdf},
isbn = {978-0073380407},
title = {{Communcation Systems}},
year = {2010}
}
@inproceedings{PCL,
abstract = {With the advent of new, low-cost 3D sensing hardware such as the Kinect, and continued efforts in advanced point cloud processing, 3D perception gains more and more importance in robotics, as well as other fields. In this paper we present one of our most recent initiatives in the areas of point cloud perception: PCL (Point Cloud Library - http://pointclouds.org). PCL presents an advanced and extensive approach to the subject of 3D perception, and it's meant to provide support for all the common 3D building blocks that applications need. The library contains state-of-the art algorithms for: filtering, feature estimation, surface reconstruction, registration, model fitting and segmentation. PCL is supported by an international community of robotics and perception researchers. We provide a brief walkthrough of PCL including its algorithmic capabilities and implementation strategies. {\textcopyright} 2011 IEEE.},
author = {Rusu, Radu Bogdan and Cousins, Steve},
booktitle = {Proc. - IEEE Int. Conf. Robot. Autom.},
doi = {10.1109/ICRA.2011.5980567},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rusu, Cousins/3D is here Point Cloud Library (PCL). Rusu, Cousins. 2011.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
title = {{3D is here: Point Cloud Library (PCL)}},
year = {2011}
}
@article{Ishikawa2018,
abstract = {In this paper, we propose a method of targetless and automatic Camera-LiDAR calibration. Our approach is an extension of hand-eye calibration framework to 2D-3D calibration. By using the sensor fusion odometry method, the scaled camera motions are calculated with high accuracy. In addition to this, we clarify the suitable motion for this calibration method. The proposed method only requires the three-dimensional point cloud and the camera image and does not need other information such as reflectance of LiDAR and to give initial extrinsic parameter. In the experiments, we demonstrate our method using several sensor configurations in indoor and outdoor scenes to verify the effectiveness. The accuracy of our method achieves more than other comparable state-of-the-art methods.},
author = {Ishikawa, Ryoichi and Oishi, Takeshi and Ikeuchi, Katsushi},
doi = {10.1109/IROS.2018.8593360},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ishikawa, Oishi, Ikeuchi/LiDAR and Camera Calibration Using Motions Estimated by Sensor Fusion Odometry. Ishikawa, Oishi, Ikeuchi. 2018(2).pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
number = {October 2018},
pages = {7342--7349},
title = {{LiDAR and Camera Calibration Using Motions Estimated by Sensor Fusion Odometry}},
year = {2018}
}
@article{Lepetit2009,
abstract = {We propose a non-iterative solution to the PnP problem—the estimation of the pose of a calibrated camera from n 3D-to-2D point correspondences—whose computational complexity grows linearly with n. This is in contrast to state-of-the-art methods that are O(n 5) or even O(n 8), without being more accurate. Our method is applicable for all n≥4 and handles properly both planar and non-planar configurations. Our central idea is to express the n 3D points as a weighted sum of four virtual control points. The problem then reduces to estimating the coordinates of these control points in the camera referential, which can be done in O(n) time by expressing these coordinates as weighted sum of the eigenvectors of a 12×12 matrix and solving a small constant number of quadratic equations to pick the right weights. Furthermore, if maximal precision is required, the output of the closed-form solution can be used to initialize a Gauss-Newton scheme, which improves accuracy with negligible amount of additional time. The advantages of our method are demonstrated by thorough testing on both synthetic and real-data.},
author = {Lepetit, Vincent and Moreno-Noguer, Francesc and Fua, Pascal},
doi = {10.1007/s11263-008-0152-6},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lepetit, Moreno-Noguer, Fua/EPnP An accurate O(n) solution to the PnP problem. Lepetit, Moreno-Noguer, Fua. 2009.pdf:pdf},
issn = {09205691},
journal = {Int. J. Comput. Vis.},
keywords = {Absolute orientation,Perspective-n-Point,Pose estimation},
number = {2},
pages = {155--166},
title = {{EPnP: An accurate O(n) solution to the PnP problem}},
volume = {81},
year = {2009}
}
@misc{simplecv,
title = {{SimpleCV Tutorial — Tutorial}},
url = {http://tutorial.simplecv.org/en/latest/},
urldate = {2019-10-22}
}
@article{Wei2018,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@techreport{Rev,
address = {San Jose},
author = {Rev, E},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rev/VLP-16 User Manual. Rev. 2019.pdf:pdf},
institution = {Velodyne LiDAR, Inc.},
pages = {71},
title = {{VLP-16 User Manual}},
year = {2019}
}
@article{Sullivan2016,
abstract = {Analysis of LiDAR technology for Advanced Safety Preface Safety and its improvement is a concern paramount to all passenger vehicle manufacturers. The value proposition for driver, passenger and pedestrian safety has become equally important as engine performance and fuel economy. This paper addresses how light detection and range (LiDAR) technology will impact Advanced Driver Assistance Systems and explore the term, " autonomous driving intelligence. " The paper will consider Lidar technology today and how it stands to capture a large market share of automotive sensor technology in the future. Lidar is poised to penetrate the market in 2016 as the lead technology in automotive safety and autonomous systems. Three primary approaches to Lidar development are considered, including Hybrid Solid-State Lidar, MEMS Lidar, and Mechanical Mechanism Lidar. Mechanical mechanism Lidar is the oldest and most traditional technology. MEMS Lidar technology is in the beginning stage of development as a low cost Lidar solution for low level automotive safety. Solid-state Hybrid Lidar (SH Lidar) was introduced in 2005 as a result of the Darpa Robotic Car Races. The technology has been tested for autonomous safety over the years and the cost for SH Lidar dropped dramatically in 2015. With planned mass production to meet the growing demand for autonomous navigation and advanced safety, further dramatic cost reduction is expected in 2016 – 2017. The development of Solid-State Hybrid Lidar (SH Lidar) was a break away from the traditional mechanical mechanism of single Lidar technology, and it is described in detail in this paper. The technology simplified what was previously a complex mechanical system of parts into one robust solid-state part. The solid-state developmental enabled faster data capture in 3D, capturing pictures instantaneously while moving in real-time at speeds of 30-40MPH. SH Lidar technology has matured from an extremely costly technology and large system to being affordable, smaller in size, and headed toward mass production. SH Lidar technology is poised to be commercialized in 2015-16 and radically change the way we move about in the world.},
author = {Sullivan, Frost {\&}},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Sullivan/LiDAR Driving the Future of Autonomous Navigation. Sullivan. 2016.pdf:pdf},
pages = {1--30},
title = {{LiDAR: Driving the Future of Autonomous Navigation}},
year = {2016}
}
@misc{Camerona,
abstract = {Retrofit a Car for self-driving using LIDAR, camera and Radar},
author = {Cameron, Oliver and Voyage},
keywords = {LIDAR,Self-Driving Cars,Sensor Fusion},
mendeley-tags = {LIDAR,Self-Driving Cars,Sensor Fusion},
title = {{The Story of Homer: Voyage's First Self-Driving Taxi}},
url = {https://news.voyage.auto/the-story-of-homer-voyages-first-self-driving-taxi-f0a6466718af},
urldate = {2018-10-09}
}
@misc{watson,
title = {{Watson Visual Recognition}},
url = {https://www.ibm.com/watson/services/visual-recognition/},
urldate = {2019-10-22}
}
@misc{PDAL,
author = {Contributors, PDAL},
doi = {10.5281/zenodo.2556738},
title = {{PDAL Point Data Abstraction Library}},
url = {https://doi.org/10.5281/zenodo.2556738},
year = {2018}
}
@article{Menze2015a,
abstract = {This paper proposes a novel model and dataset for 3D scene flow estimation with an application to autonomous driving. Taking advantage of the fact that outdoor scenes often decompose into a small number of independently mov- ing objects, we represent each element in the scene by its rigid motion parameters and each superpixel by a 3D plane as well as an index to the corresponding object. This minimal representation increases robustness and leads to a discrete-continuous CRF where the data term decomposes into pairwise potentials between superpixels and objects. Moreover, our model intrinsically segments the scene into its constituting dynamic components. We demonstrate the performance of our model on existing benchmarks as well as a novel realistic dataset with scene flow ground truth. We obtain this dataset by annotating 400 dynamic scenes from the KITTI raw data collection using detailed 3D CAD models for all vehicles in motion. Our experiments also re- veal novel challenges which cannot be handled by existing methods. 1.},
author = {Menze, Moritz and Geiger, Andreas},
doi = {10.1109/CVPR.2015.7298925},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Menze, Geiger/Object scene flow for autonomous vehicles. Menze, Geiger. 2015.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {3061--3070},
title = {{Object scene flow for autonomous vehicles}},
volume = {07-12-June},
year = {2015}
}
@article{Rate,
author = {Rate, High Frame},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rate/HDL-64E. Rate. Unknown.pdf:pdf},
pages = {3--4},
title = {{HDL-64E}}
}
@misc{cameraCalibrationRos,
author = {{James Bowman} and {Patrick Mihelich} and {Vincent Rabaud}},
title = {{camera{\_}calibration - ROS Wiki}},
url = {https://wiki.ros.org/camera{\_}calibration},
urldate = {2019-10-27}
}
@article{Sturm2010,
abstract = {This survey is mainly motivated by the increased availability and use of panoramic image acquisition devices, in computer vision and various of its applications. Different technologies and different computational models thereof exist and algorithms and theoretical studies for geometric computer vision ("structure-from-motion") are often re-developed without highlighting common underlying principles. One of the goals of this survey is to give an overview of image acquisition methods used in computer vision and especially, of the vast number of camera models that have been proposed and investigated over the years, where we try to point out similarities between different models. Results on epipolar and multi-view geometry for different camera models are reviewed as well as various calibration and self-calibration approaches, with an emphasis on non-perspective cameras.We finally describe what we consider are fundamental building blocks for geometric computer vision or structure-from-motion: epipolar geometry, pose and motion estimation, 3D scene modeling, and bundle adjustment. The main goal here is to highlight the main principles of these, which are independent of specific camera models.},
author = {Sturm, Peter and Ramalingam, Srikumar and Tardif, Jean Philippe and Gasparini, Simone and Barreto, Jo{\~{a}}o},
doi = {10.1561/0600000023},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Sturm et al/Camera models and fundamental concepts used in geometric computer vision. Sturm et al.. 2010.pdf:pdf},
issn = {15722740},
journal = {Found. Trends Comput. Graph. Vis.},
number = {1-2},
pages = {1--183},
title = {{Camera models and fundamental concepts used in geometric computer vision}},
volume = {6},
year = {2010}
}
@misc{Thorlabs,
author = {Thorlabs},
title = {{MVL16M1 Online Datasheet}},
url = {https://www.thorlabs.com/newgrouppage9.cfm?objectgroup{\_}id=1822},
urldate = {2019-10-26}
}
@incollection{AP,
address = {Boston, MA},
author = {Zhang, Ethan and Zhang, Yi},
booktitle = {Encycl. Database Syst.},
doi = {10.1007/978-0-387-39940-9_482},
editor = {LIU, LING and {\"{O}}ZSU, M TAMER},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Zhang, Zhang/Average Precision. Zhang, Zhang. 2009.pdf:pdf},
isbn = {978-0-387-39940-9},
pages = {192--193},
publisher = {Springer US},
title = {{Average Precision}},
url = {https://doi.org/10.1007/978-0-387-39940-9{\_}482},
year = {2009}
}
@misc{Unnikrishnan,
author = {Unnikrishnan, Ranjith and Hebert, Martial},
title = {{Laser-Camera Calibration Toolbox}},
url = {https://www.cs.cmu.edu/{~}ranjith/lcct.html}
}
@inproceedings{Yue2018,
abstract = {3D LiDAR scanners are playing an increasingly important role in autonomous driving as they can generate depth information of the environment. However, creating large 3D LiDAR point cloud datasets with point-level labels requires a significant amount of manual annotation. This jeopardizes the efficient development of supervised deep learning algorithms which are often data-hungry. We present a framework to rapidly create point clouds with accurate point-level labels from a computer game. The framework supports data collection from both auto-driving scenes and user-configured scenes. Point clouds from auto-driving scenes can be used as training data for deep learning algorithms, while point clouds from user-configured scenes can be used to systematically test the vulnerability of a neural network, and use the falsifying examples to make the neural network more robust through retraining. In addition, the scene images can be captured simultaneously in order for sensor fusion tasks, with a method proposed to do automatic calibration between the point clouds and captured scene images. We show a significant improvement in accuracy (+9{\%}) in point cloud segmentation by augmenting the training dataset with the generated synthesized data. Our experiments also show by testing and retraining the network using point clouds from user-configured scenes, the weakness/blind spots of the neural network can be fixed.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1804.00103},
author = {Yue, Xiangyu and Wu, Bichen and Seshia, Sanjit A. and Keutzer, Kurt and Sangiovanni-Vincentelli, Alberto L.},
booktitle = {Proc. 2018 ACM Int. Conf. Multimed. Retr. - ICMR '18},
doi = {10.1145/3206025.3206080},
eprint = {1804.00103},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yue et al/A LiDAR Point Cloud Generator. Yue et al.. 2018.pdf:pdf},
isbn = {9781450350464},
month = {mar},
number = {Nips},
pages = {458--464},
publisher = {ACM Press},
title = {{A LiDAR Point Cloud Generator}},
url = {http://dl.acm.org/citation.cfm?doid=3206025.3206080 http://arxiv.org/abs/1804.00103},
year = {2018}
}
@article{TexasLiDAR,
author = {Carbone, Marco},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Carbone/An introduction to automotive SDR. Carbone. 2018.pdf:pdf},
issn = {00134953},
journal = {Electron. Prod.},
number = {2},
title = {{An introduction to automotive SDR}},
volume = {61},
year = {2018}
}
@book{SpringerOptics,
author = {Rhodes, W. T. and Adibi, A. and Asakura, T. and H$\backslash$¨ansch, T.W. and Kamiya, T. and Krausz, F. and Monemar, B. and Venghaus, H. and Weber, H. and Weinfurter, H.},
editor = {Springer},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Unknown/Springer.High.Order.Modulation.For.Optical.Fiber.Transmission.Jun.2009.ISBN.3540937706.pdf. Unknown. Unknown.pdf:pdf},
title = {{Springer.High.Order.Modulation.For.Optical.Fiber.Transmission.Jun.2009.ISBN.3540937706.pdf}}
}
@article{Lin2014a,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.0312v3},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {arXiv:1405.0312v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lin et al/Microsoft COCO Common objects in context. Lin et al.. 2014.pdf:pdf},
issn = {16113349},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@article{Yue2018a,
abstract = {3D LiDAR scanners are playing an increasingly important role in autonomous driving as they can generate depth information of the environment. However, creating large 3D LiDAR point cloud datasets with point-level labels requires a significant amount of manual annotation. This jeopardizes the efficient development of supervised deep learning algorithms which are often data-hungry. We present a framework to rapidly create point clouds with accurate point-level labels from a computer game. The framework supports data collection from both auto-driving scenes and user-configured scenes. Point clouds from auto-driving scenes can be used as training data for deep learning algorithms, while point clouds from user-configured scenes can be used to systematically test the vulnerability of a neural network, and use the falsifying examples to make the neural network more robust through retraining. In addition, the scene images can be captured simultaneously in order for sensor fusion tasks, with a method proposed to do automatic calibration between the point clouds and captured scene images. We show a significant improvement in accuracy (+9{\%}) in point cloud segmentation by augmenting the training dataset with the generated synthesized data. Our experiments also show by testing and retraining the network using point clouds from user-configured scenes, the weakness/blind spots of the neural network can be fixed.},
archivePrefix = {arXiv},
arxivId = {1804.00103},
author = {Yue, Xiangyu and Wu, Bichen and Seshia, Sanjit A. and Keutzer, Kurt and Sangiovanni-Vincentelli, Alberto L.},
doi = {10.1145/3206025.3206080},
eprint = {1804.00103},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yue et al/A LiDAR Point Cloud Generator. Yue et al.. 2018.pdf:pdf},
isbn = {9781450350464},
journal = {Proc. 2018 ACM Int. Conf. Multimed. Retr.  - ICMR '18},
number = {Nips},
pages = {458--464},
title = {{A LiDAR Point Cloud Generator}},
url = {http://dl.acm.org/citation.cfm?doid=3206025.3206080},
year = {2018}
}
@misc{Templeton,
author = {Templeton, Brad},
title = {{Elon Musk's War On LIDAR: Who Is Right And Why Do They Think That?}},
url = {https://www.forbes.com/sites/bradtempleton/2019/05/06/elon-musks-war-on-lidar-who-is-right-and-why-do-they-think-that/},
urldate = {2019-10-24}
}
@misc{Yu2016,
author = {Yu, Tianyue and Yu, Tianyue and Pacala, Angus},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yu, Yu, Pacala/(12) Patent Application Publication (10) Pub. No. US 20160161600 A1. Yu, Yu, Pacala. 2016.pdf:pdf},
number = {19},
title = {{(12) Patent Application Publication (10) Pub. No.: US 2016/0161600 A1}},
volume = {1},
year = {2016}
}
@misc{Hesai,
title = {{HESAI}},
url = {https://www.hesaitech.com/en},
urldate = {2019-11-28}
}
@misc{EUroads,
title = {{13th Annual Road Safety Performance Index (PIN) Report | ETSC}},
url = {https://etsc.eu/13th-annual-road-safety-performance-index-pin-report/},
urldate = {2019-10-20}
}
@article{Roger1987,
author = {Tsai, Roger Y},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Tsai/Tsai{\_}Calibration.Pdf. Tsai. 1987.pdf:pdf},
journal = {IEEE J. ofRobotvs Autom.},
title = {{Tsai{\_}Calibration.Pdf}},
volume = {RA-3},
year = {1987}
}
@misc{Facet,
author = {Technologies, Facet},
title = {{Facet Technology Launches Its Safe and Secure LiDAR™ Crosstalk Elimination Licensing Program - PR.com}},
url = {https://www.pr.com/press-release/703312},
urldate = {2019-10-25}
}
@misc{MarkoBjelonic,
author = {{Marko Bjelonic}},
title = {{GitHub - leggedrobotics/darknet{\_}ros: YOLO ROS: Real-Time Object Detection for ROS}},
url = {https://github.com/leggedrobotics/darknet{\_}ros https://wiki.ros.org/darknet{\_}ros},
urldate = {2019-04-10}
}
@article{Park2014,
abstract = {Calibration between color camera and 3D Light Detection And Ranging (LIDAR) equipment is an essential process for data fusion. The goal of this paper is to improve the calibration accuracy between a camera and a 3D LIDAR. In particular, we are interested in calibrating a low resolution 3D LIDAR with a relatively small number of vertical sensors. Our goal is achieved by employing a new methodology for the calibration board, which exploits 2D-3D correspondences. The 3D corresponding points are estimated from the scanned laser points on the polygonal planar board with adjacent sides. Since the lengths of adjacent sides are known, we can estimate the vertices of the board as a meeting point of two projected sides of the polygonal board. The estimated vertices from the range data and those detected from the color image serve as the corresponding points for the calibration. Experiments using a low-resolution LIDAR with 32 sensors show robust results. {\textcopyright} 2014 by the authors; licensee MDPI, Basel, Switzerland.},
author = {Park, Yoonsu and Yun, Seokmin and Won, Chee Sun and Cho, Kyungeun and Um, Kyhyun and Sim, Sungdae},
doi = {10.3390/s140305333},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Park et al/Calibration between color camera and 3D LIDAR instruments with a polygonal planar board. Park et al.. 2014.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {3D LIDAR,3D point clouds,Calibration board,Calibration matrix,Camera calibration,Sensor fusion},
number = {3},
pages = {5333--5353},
title = {{Calibration between color camera and 3D LIDAR instruments with a polygonal planar board}},
volume = {14},
year = {2014}
}
@misc{Simonite2017,
author = {Simonite, Tom},
booktitle = {MT Technol. Rev.},
title = {{Self-Driving Cars' Spinning-Laser Problem - MIT Technology Review}},
url = {https://www.technologyreview.com/s/603885/autonomous-cars-lidar-sensors/},
urldate = {2019-01-23},
year = {2017}
}
@misc{PaulA.Hughes,
author = {{Paul A. Hughes}},
booktitle = {1996},
title = {{History of the Electric Car: 1828 - 1912, from Trouve to Morrison}},
url = {https://web.archive.org/web/20111113023143/http://factoidz.com/history-of-the-electric-car-1828-1912-from-trouve-to-morrison/},
urldate = {2019-10-20}
}
@article{Wei2018,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@article{Huang2017,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L2+1) direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
doi = {10.1109/CVPR.2017.243},
eprint = {1608.06993},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Huang et al/Densely connected convolutional networks. Huang et al.. 2017.pdf:pdf},
isbn = {9781538604571},
journal = {Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017},
pages = {2261--2269},
title = {{Densely connected convolutional networks}},
volume = {2017-Janua},
year = {2017}
}
@techreport{VLP16,
address = {San Jose},
author = {{Velodyne LiDAR}, Inc.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rev/VLP-16 User Manual. Rev. 2019.pdf:pdf},
institution = {Velodyne LiDAR, Inc.},
pages = {71},
title = {{VLP-16 User Manual}},
year = {2019}
}
@article{Scaramuzza,
author = {Scaramuzza, Davide and Harati, Ahad and Siegwart, Roland},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Scaramuzza, Harati, Siegwart/Extrinsic self calibration of a camera and a 3D Laser Range Finder. Scaramuzza, Harati, Siegwart. Unknown.pdf:pdf},
keywords = {camera calibration,computer vision,laser calibration,omndirectional vision,omnidirectional camera,p3p algorithm,pnp algorithm,reprojection error,robotics,self calibration},
title = {{Extrinsic self calibration of a camera and a 3D Laser Range Finder}}
}
@misc{Ouster,
author = {Ouster},
title = {{High-resolution digital lidar: autonomous vehicles, robotics, drones | Ouster}},
url = {https://ouster.com/},
urldate = {2019-11-27}
}
@misc{DailyNews2013,
author = {{Daily News}},
title = {{Ford's assembly line turns 100: How it changed manufacturing and society - NY Daily News}},
url = {https://web.archive.org/web/20131130021237/http://www.nydailynews.com/autos/ford-assembly-line-turns-100-changed-society-article-1.1478331},
urldate = {2019-10-20},
year = {2013}
}
@article{Fischler1981,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/ smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing and analysis conditions. Implementation details and computational examples are also presented.},
archivePrefix = {arXiv},
arxivId = {3629719},
author = {Fischler, Martin A. and Bolles, Robert C.},
doi = {10.1145/358669.358692},
eprint = {3629719},
isbn = {0934613338},
issn = {00010782},
journal = {Commun. ACM},
month = {jun},
number = {6},
pages = {381--395},
pmid = {4650729},
title = {{Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography}},
url = {http://portal.acm.org/citation.cfm?doid=358669.358692},
volume = {24},
year = {2002}
}
@article{Wang2018a,
abstract = {We propose a new method for fusing LIDAR point cloud and camera-captured images in deep convolutional neural networks (CNN). The proposed method constructs a new layer called sparse non-homogeneous pooling layer to transform features between bird's eye view and front view. The sparse point cloud is used to construct the mapping between the two views. The pooling layer allows efficient fusion of the multi-view features at any stage of the network. This is favorable for 3D object detection using camera-LIDAR fusion for autonomous driving. A corresponding one-stage detector is designed and tested on the KITTI bird's eye view object detection dataset, which produces 3D bounding boxes from the bird's eye view map. The fusion method shows significant improvement on both speed and accuracy of the pedestrian detection over other fusion-based object detection networks.},
archivePrefix = {arXiv},
arxivId = {1711.06703},
author = {Wang, Zining and Zhan, Wei and Tomizuka, Masayoshi},
doi = {10.1109/IVS.2018.8500387},
eprint = {1711.06703},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wang, Zhan, Tomizuka/Fusing Bird's Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection. Wang, Zhan, Tomizuka. 2018.pdf:pdf},
isbn = {9781538644522},
journal = {IEEE Intell. Veh. Symp. Proc.},
pages = {834--839},
title = {{Fusing Bird's Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection}},
volume = {2018-June},
year = {2018}
}
@article{Fremont2013,
author = {Fremont, Vincent and Alberto, Sergio and Florez, Rodriguez and Bonnifait, Philippe and Targets, Circular and Video, Alignment and Sensors, Lidar and Robotics, Advanced and Stm, Francis},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fremont et al/Circular Targets for 3D Alignment of Video and Lidar Sensors To cite this version Circular Targets for 3D Alignment of Video and Lidar.pdf:pdf},
title = {{Circular Targets for 3D Alignment of Video and Lidar Sensors To cite this version : Circular Targets for 3D Alignment of Video and Lidar Sensors}},
year = {2013}
}
@misc{JohnSteeleGordon2007,
author = {{John Steele Gordon}},
title = {{AmericanHeritage.com / 10 Moments That Made American Business}},
url = {https://web.archive.org/web/20080420194514/http://americanheritage.com/articles/magazine/ah/2007/1/2007{\_}1{\_}23.shtml},
urldate = {2019-10-20},
year = {2007}
}
@article{Pandey2011,
abstract = {In this paper we describe a data set collected by an autonomous ground vehicle testbed, based upon a modified Ford F-250 pickup truck. The vehicle is outfitted with a professional (Applanix POS-LV) and consumer (Xsens MTi-G) inertial measurement unit, a Velodyne three-dimensional lidar scanner, two push-broom forward-looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system. Here we present the time-registered data from these sensors mounted on the vehicle, collected while driving the vehicle around the Ford Research Campus and downtown Dearborn, MI, during November-December 2009. The vehicle path trajectory in these data sets contains several large- and small-scale loop closures, which should be useful for testing various state-of-the-art computer vision and simultaneous localization and mapping algorithms. {\textcopyright} SAGE Publications 2011.},
author = {Pandey, Gaurav and McBride, James R. and Eustice, Ryan M.},
doi = {10.1177/0278364911400640},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pandey, McBride, Eustice/Ford Campus vision and lidar data set. Pandey, McBride, Eustice. 2011.pdf:pdf},
issn = {17413176},
journal = {Int. J. Rob. Res.},
keywords = {Mobile and distributed robotics SLAM,field and service robotics,sensing and perception computer vision},
number = {13},
pages = {1543--1552},
title = {{Ford Campus vision and lidar data set}},
url = {http://robots.engin.umich.edu/SoftwareData/Ford},
volume = {30},
year = {2011}
}
@article{Geiger2012a,
abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera- to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experi- ments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions. I.},
author = {Geiger, Andreas and Moosmann, Frank and Car, {\"{O}}mer and Schuster, Bernhard},
doi = {10.1109/ICRA.2012.6224570},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger et al/Automatic camera and range sensor calibration using a single shot. Geiger et al.. 2012.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {3936--3943},
title = {{Automatic camera and range sensor calibration using a single shot}},
year = {2012}
}
@misc{Redmon2013,
author = {Redmon, Joseph},
title = {{Darknet: Open Source Neural Networks in C}},
url = {https://pjreddie.com/darknet/ http://pjreddie.com/darknet/},
urldate = {2019-10-24},
year = {2013}
}
@inproceedings{Kim2015a,
abstract = {LIDAR scanners are essential components of intelligent vehicles capable of autonomous travel. Mutual interference between LIDAR scanners has not been regarded as a problem yet. Mutual interference was identified as a problem of increased importance because of the appearance of safety functions and the increasing rate of vehicles equipped with LIDAR scanner. This paper will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some generic interference scenarios and report on the current status of the analysis of interference mechanisms.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
booktitle = {2015 12th Int. Conf. Inf. Technol. - New Gener.},
doi = {10.1109/ITNG.2015.113},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/An Experiment of Mutual Interference between Automotive LIDAR Scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {978-1-4799-8828-0},
keywords = {LIDAR scanner,autonomous vehicle,ghost target,mutual interference,obstacle detection},
month = {apr},
pages = {680--685},
publisher = {IEEE},
title = {{An Experiment of Mutual Interference between Automotive LIDAR Scanners}},
url = {http://ieeexplore.ieee.org/document/7113553/},
year = {2015}
}
@misc{Robosense,
title = {{RoboSense LiDAR}},
url = {https://www.robosense.ai/},
urldate = {2019-11-28}
}
@article{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/He et al/Deep residual learning for image recognition. He et al.. 2016.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
volume = {2016-Decem},
year = {2016}
}
@article{Payne2009,
abstract = {A number of full field image sensors have been developed that are capable of simultaneously measuring intensity and distance (range) for every pixel in a given scene using an indirect time-of-flight measurement technique. A light source is intensity modulated at a frequency between 10-100 MHz, and an image sensor is modulated at the same frequency, synchronously sampling light reflected from objects in the scene (homodyne detection). The time of flight is manifested as a phase shift in the illumination modulation envelope, which can be determined from the sampled data simultaneously for each pixel in the scene. This paper presents a method of characterizing the high frequency modulation response of these image sensors, using a pico-second laser pulser. The characterization results allow the optimal operating parameters, such as the modulation frequency, to be identified in order to maximize the range measurement precision for a given sensor. A number of potential sources of error exist when using these sensors, including deficiencies in the modulation waveform shape, duty cycle, or phase, resulting in contamination of the resultant range data. From the characterization data these parameters can be identified and compensated for by modifying the sensor hardware or through post processing of the acquired range measurements.},
author = {Payne, Andrew D. and Dorrington, Adrian A. and Cree, Michael J. and Carnegie, Dale A.},
doi = {10.1117/12.806007},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Payne et al/Characterization of modulated time-of-flight range image sensors. Payne et al.. 2009.pdf:pdf},
issn = {0277786X},
journal = {Three-Dimensional Imaging Metrol.},
number = {June},
pages = {723904},
title = {{Characterization of modulated time-of-flight range image sensors}},
volume = {7239},
year = {2009}
}
@misc{VelodyneLawsuit,
title = {{Velodyne Files Patent Infringement Complaint with ITC Against Hesai and RoboSense - Velodyne Lidar}},
url = {https://velodynelidar.com/newsroom/velodyne-files-patent-infringement-complaint-with-itc-against-hesai-and-robosense/},
urldate = {2019-11-28}
}
@article{Popko2019a,
abstract = {Renal ischemia that occurs intraoperatively during procedures requiring clamping of the renal artery (such as renal procurement for transplantation and partial nephrectomy for renal cancer) is known to have a sig- nificant impact on the viability of that kidney. To better understand the dynamics of intraoperative renal ischemia and recovery of renal oxygenation during reperfusion, a visible reflectance imaging system (VRIS) was developed to measure renal oxygenation during renal artery clamping in both cooled and warm porcine kidneys. For all kidneys, normothermic and hypothermic, visible reflectance imaging demonstrated a spatially distinct decrease in the rel- ative oxy-hemoglobin concentration ({\%}HbO2) of the superior pole of the kidney compared to the middle or inferior pole. Mean relative oxy-hemoglobin concentrations decrease more significantly during ischemia for normothermic kidneys compared to hypothermic kidneys. VRIS may be broadly applicable to provide an indicator of organ ische- mia during open and laparoscopic procedures.},
author = {Popko, Gerald B. and Bao, Yijun and Gaylord, Thomas K. and Valenta, Christopher R.},
doi = {10.1117/1},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Popko et al/Beam path intersections between two coplanar lidar scanners. Popko et al.. 2003.pdf:pdf},
journal = {J. Biomed. Opt.},
keywords = {12,2012,2013,6,9,accepted for publication feb,kidney,oxygenation,paper 12432rr received jul,published online,revised manuscript received feb,swine,visible reflectance spectroscopy},
number = {3},
pages = {035001--7},
title = {{Beam path intersections between two coplanar lidar scanners}},
volume = {18},
year = {2019}
}
@misc{Waymo,
author = {Waymo},
title = {{Open Dataset – Waymo}},
url = {https://waymo.com/open/},
urldate = {2019-10-20}
}
@article{Levenberg1943,
abstract = {The standard method for solving least squares problems which lead to non-linear normal equations depends upon a reduction of the residuals to linear form by first order Taylor approximations taken about an initial or trial solution for the parameters .2 If the usual least squares procedure, performed with these linear approximations , yields new values for the parameters which are not sufficiently close to the initial values, the neglect of second and higher order terms may invalidate the process, and may actually give rise to a larger value of the sum of the squares of the residuals than that corresponding to the initial solution. This failure of the standard method to improve the initial solution has received some notice in statistical applications of least squares3 and has been encountered rather frequently in connection with certain engineering applications involving the approximate representation of one function by another. The purpose of this article is to show how the problem may be solved by an extension of the standard method which insures improvement of the initial solution.4 The process can also be used for solving non-linear simultaneous equations, in which case it may be considered an extension of Newton's method.},
author = {Levenberg, Kenneth and Arsenal, Frankford},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Levenberg, Arsenal/A Method for the Solution of Certain Non-Linear Problems in Least Squares. Levenberg, Arsenal. 1943.pdf:pdf},
journal = {Q. Appl. Math.},
number = {278},
pages = {536--538},
title = {{A Method for the Solution of Certain Non-Linear Problems in Least Squares}},
volume = {1},
year = {1943}
}
@article{Redmon2016,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02640v5},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {arXiv:1506.02640v5},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Redmon et al/You only look once Unified, real-time object detection. Redmon et al.. 2016.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {779--788},
title = {{You only look once: Unified, real-time object detection}},
volume = {2016-Decem},
year = {2016}
}
@article{Surasak2018,
abstract = {Currently, Computer Vision (CV) is one of the most popular research topics in the world. This is because it can support the human daily life. Moreover, CV can also apply to various theories and researches. Human Detection is one of the most popular research topics in Computer Vision. In this paper, we present a study of technique for human detection from video, which is the Histograms of Oriented Gradients or HOG by developing a piece of application to import and detect the human from the video. We use the HOG Algorithm to analyze every frame from the video to find and count people. After analyzing video from starting to the end, the program generate histogram to show the number of detected people versus playing period of the video. As a result, the expected results are obtained, including the detection of people in the video and the histogram generation to show the appearance of human detected in the video file.},
author = {Surasak, Thattapon and Takahiro, Ito and Cheng, Cheng Hsuan and Wang, Chi En and Sheng, Pao You},
doi = {10.1109/ICBIR.2018.8391187},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Surasak et al/Histogram of oriented gradients for human detection in video. Surasak et al.. 2018.pdf:pdf},
isbn = {9781538652541},
journal = {Proc. 2018 5th Int. Conf. Bus. Ind. Res. Smart Technol. Next Gener. Information, Eng. Bus. Soc. Sci. ICBIR 2018},
keywords = {Histogram of Oriented Gradients,Human Detection},
pages = {172--176},
title = {{Histogram of oriented gradients for human detection in video}},
year = {2018}
}
@article{Dalal2010,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
author = {Dalal, Navneet and Triggs, Bill and Dalal, Navneet and Triggs, Bill and Gradients, Oriented and International, Detection and Diego, San and Dalal, Navneet and Triggs, Bill},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Dalal et al/Histograms of Oriented Gradients for Human Detection To cite this version HAL Id inria-00548512 Histograms of Oriented Gradients for H.pdf:pdf},
journal = {IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
keywords = {feature extraction,gradient methods,object detec},
pages = {886--893},
title = {{Histograms of Oriented Gradients for Human Detection To cite this version : HAL Id : inria-00548512 Histograms of Oriented Gradients for Human Detection}},
year = {2010}
}
@misc{Velodyne,
abstract = {Velodyne now offers an improved high definition LiDAR scanner designed for autonomous vehicle navigation, mapping, surveying, industrial automation, and other uses. The S3 version of the HDL-64E provides improved accuracy and a higher data rate than the original version. Capture high definition 3 dimensional information about the surrounding environment. High Field of View, High Frame Rate With its full 360 HFOV by 26.8 VFOV, the HDL-64E provides significantly more environmental information than previously available. With its 5 -20 Hz user-selectable frame rate and over 2.2 million points per second output rate, the HDL-64E S3 provides all the distance sensing data you'll ever need. The unit's development has been focused on high data rate, high robustness, accuracy and simple 100 MBPS Ethernet interfacing to the end user. Traditional LiDAR sensors have relied upon a single laser firing into a mechanically actuated mirror, providing only one plane of view. The HDL-64E S3's patented one-piece design uses 64 fixed-mounted lasers to measure the surrounding environment, each mechanically mounted to a specific vertical angle, with the entire unit spinning. This approach dramatically increases reliability, FOV, and point cloud density.},
author = {Velodyne},
booktitle = {Http://Velodynelidar.Com/},
pages = {2800},
title = {{Velodyne Lidar}},
url = {https://velodynelidar.com/},
urldate = {2019-11-27},
year = {2013}
}
@misc{Ouster,
author = {Ouster},
title = {{Ouster Lidar}},
url = {https://ouster.com/blog/how-multi-beam-flash-lidar-works},
urldate = {2019-10-24}
}
@article{Guindel2018,
abstract = {Sensor setups consisting of a combination of 3D range scanner lasers and stereo vision systems are becoming a popular choice for on-board perception systems in vehicles; however, the combined use of both sources of information implies a tedious calibration process. We present a method for extrinsic calibration of lidar-stereo camera pairs without user intervention. Our calibration approach is aimed to cope with the constraints commonly found in automotive setups, such as low-resolution and specific sensor poses. To demonstrate the performance of our method, we also introduce a novel approach for the quantitative assessment of the calibration results, based on a simulation environment. Tests using real devices have been conducted as well, proving the usability of the system and the improvement over the existing approaches. Code is available at http://wiki.ros.org/velo2cam{\_}calibration},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.04085v3},
author = {Guindel, Carlos and Beltran, Jorge and Martin, David and Garcia, Fernando},
doi = {10.1109/ITSC.2017.8317829},
eprint = {arXiv:1705.04085v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Guindel et al/Automatic extrinsic calibration for lidar-stereo vehicle sensor setups. Guindel et al.. 2018.pdf:pdf},
isbn = {9781538615256},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
pages = {1--6},
title = {{Automatic extrinsic calibration for lidar-stereo vehicle sensor setups}},
volume = {2018-March},
year = {2018}
}
@article{Castorena2016,
abstract = {We present a new method for joint automatic extrinsic calibration and sensor fusion for a multimodal sensor system comprising a LIDAR and an optical camera. Our approach exploits the natural alignment of depth and intensity edges when the calibration parameters are correct. Thus, in contrast to a number of existing approaches, we do not require the presence or identification of known alignment targets. On the other hand, the characteristics of each sensor modality, such as sampling pattern and information measured, are significantly different, making direct edge alignment difficult. To overcome this difficulty, we jointly fuse the data and estimate the calibration parameters. In particular, the joint processing evaluates and optimizes both the quality of edge alignment and the performance of the fusion algorithm using a common cost function on the output. We demonstrate accurate calibration in practical configurations in which depth measurements are sparse and contain no reflectivity information. Experiments on synthetic and real data obtained with a three-dimensional LIDAR sensor demonstrate the effectiveness of our approach.},
author = {Castorena, Juan and Kamilov, Ulugbek S. and Boufounos, Petros T.},
doi = {10.1109/ICASSP.2016.7472200},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Castorena, Kamilov, Boufounos/Autocalibration of lidar and optical cameras via edge alignment. Castorena, Kamilov, Boufounos. 2016.pdf:pdf},
isbn = {9781479999880},
issn = {15206149},
journal = {ICASSP, IEEE Int. Conf. Acoust. Speech Signal Process. - Proc.},
keywords = {Multimodal calibration,depth superresolution,intersensor registration,sensor fusion,total variation},
pages = {2862--2866},
publisher = {IEEE},
title = {{Autocalibration of lidar and optical cameras via edge alignment}},
volume = {2016-May},
year = {2016}
}
@book{Xu1996a,
address = {Dordrecht},
author = {Xu, Gang and Zhang, Zhengyou},
doi = {10.1007/978-94-015-8668-9},
isbn = {978-90-481-4743-4},
publisher = {Springer Netherlands},
series = {Computational Imaging and Vision},
title = {{Epipolar Geometry in Stereo, Motion and Object Recognition}},
url = {http://link.springer.com/10.1007/978-94-015-8668-9},
volume = {6},
year = {1996}
}
@techreport{vlp16,
institution = {Velodyne LiDAR, Inc.},
title = {{VLP-16 User Manual}},
year = {2019}
}
@book{Beck1983,
abstract = {Human and Machine Vision provides information pertinent to an interdisciplinary program of research in visual perception. This book presents a psychophysical study of the human visual system, which provides insights on how to model the flexibility required by a general-purpose visual system. Organized into 17 chapters, this book begins with an overview of how a visual display is segmented into components on the basis of textual differences. This text then proposes three criteria for judging representations of shape. Other chapters consider an increased use of machine vision programs as models of human vision and of data from human vision in developing programs for machine vision. This book discusses as well the diversity and flexibility of systems for representing visual information. The final chapter deals with dot patterns and discusses the process of interring orientation information from collections of them. This book is a valuable resource for psychologists, neurophysiologists, and computer scientists.},
author = {Beck, Jacob and Hope, Barbara and Rosenfeld, Azriel},
doi = {10.1016/C2013-0-10347-9},
edition = {1},
isbn = {9780120843206},
publisher = {Elsevier},
title = {{Human and Machine Vision}},
url = {https://linkinghub.elsevier.com/retrieve/pii/C20130103479},
year = {1983}
}
@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ren et al/Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks. Ren et al.. 2017.pdf:pdf},
issn = {01628828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
keywords = {Object detection,convolutional neural network,region proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
volume = {39},
year = {2017}
}
@inproceedings{Javaheri2017,
abstract = {The increasing availability of point cloud data in recent years is demanding for high performance denoising methods and compression schemes. When point cloud data is directly obtained from depth sensors or extracted from images acquired from different viewpoints, imprecisions on the depth acquisition or in the 3D reconstruction techniques result in noisy point clouds which may include a significant number of outliers. Moreover, the quality assessment of point clouds is a challenging problem since this 3D representation format is unstructured and it is typically not directly visualized. In this paper, selected objective quality metrics are evaluated regarding their correlation with human quality assessment and thus human perception. As far as the authors know, this is the first paper performing the subjective assessment of point cloud denoising algorithms and the evaluation of most used point cloud objective quality metrics. Experimental results show that graph-based denoising algorithms can improve significantly the point cloud quality data and that objective metrics that model the underlying point cloud surface can correlate better with human perception.},
author = {Javaheri, Alireza and Brites, Catarina and Pereira, Fernando and Ascenso, Jo{\~{a}}o},
booktitle = {2017 IEEE 19th Int. Work. Multimed. Signal Process. MMSP 2017},
doi = {10.1109/MMSP.2017.8122239},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Javaheri et al/Subjective and objective quality evaluation of compressed point clouds. Javaheri et al.. 2017.pdf:pdf},
isbn = {9781509036493},
issn = {1047-2797},
keywords = {Point cloud compression,Quality metrics,Subjective quality assessment},
pages = {1--6},
pmid = {10037558},
title = {{Subjective and objective quality evaluation of compressed point clouds}},
volume = {2017-Janua},
year = {2017}
}
@techreport{Foote2014,
author = {Foote, Tully and Purvis, Mike},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Foote, Purvis/Coordinate Frame Conventions. Foote, Purvis. 2014.pdf:pdf},
pages = {1--5},
title = {{Coordinate Frame Conventions}},
year = {2014}
}
@inproceedings{Hebel2018,
abstract = {In this paper, we examine crosstalk effects that can arise in multi-LiDAR configurations, and we show a data-based approach to mitigate these effects. Due to the ability to acquire precise 3D data of the environment, LiDAR-based sensor systems (sensors based on “Light Detection and Ranging”, e.g., laser scanners) increasingly find their way into various applications, e.g. in the automotive sector. However, with an increasing number of LiDAR sensors operating within close vicinity, the problem of potential crosstalk between these devices arises. “Crosstalk” outlines the following effect: In a typical LiDAR-based sensor, short laser pulses are emitted into the scene and the distance between sensor and object is derived from the time measured until an “echo” is received. In case multiple laser pulses of the same wavelength are emitted at the same time, the detector may not be able to distinguish between correct and false matches of laser pulses and echoes, resulting in erroneous range measurements and 3D points. During operation of our own multi-LiDAR sensor system, we were able to observe crosstalk effects in the acquired data. Having compared different spatial filtering approaches for the elimination of erroneous points in the 3D data, we propose a data-based spatio-temporal filtering and show its results, which may be sufficient depending on the application. However, technical solutions are desired for future LiDAR sensors.},
author = {Hebel, Marcus and Hammer, Marcus and Arens, Michael and Diehm, Axel L.},
booktitle = {Electro-Optical Remote Sens. XII},
doi = {10.1117/12.2324305},
editor = {Kamerman, Gary and Steinvall, Ove},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hebel et al/Mitigation of crosstalk effects in multi-LiDAR configurations. Hebel et al.. 2018.pdf:pdf},
isbn = {9781510621756},
issn = {1996756X},
keywords = {Crosstalk,LADAR,Laser radar,Laser scanning,LiDAR,Multi-sensor systems,Mutual interference,Temporal filtering},
month = {oct},
pages = {3},
publisher = {SPIE},
title = {{Mitigation of crosstalk effects in multi-LiDAR configurations}},
url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10796/2324305/Mitigation-of-crosstalk-effects-in-multi-LiDAR-configurations/10.1117/12.2324305.full},
volume = {10796},
year = {2018}
}
@inproceedings{Gelbart2002,
abstract = {A 64x64-pixel Flash (scannerless) lidar system that uses a streak tube to achieve range-resolved images is demonstrated. An array of glass fibers maps light from an area in the focal plane of an imaging lens to multiple rows of fibers on the streak tube's photocathode. The time-resolved backscatter return for all 4096 image pixels is recorded during one integration-time of a CCD camera that is coupled to the streak tube's phosphor screen. Data processing yields 64x64-pixel contrast (intensity) and range images for each laser pulse. Range precision better than 2.5{\%} of the range extent is exhibited for a wide variety of targets and terrains at image rates up to 100Hz. Field test imagery demonstrated the capability of the Flash lidar system for imaging vehicles hidden by a tree canopy as well as for imaging sub-surface mine-like targets in the ocean.},
author = {Gelbart, Asher and Redman, Brian C and Light, Robert S and Schwartzlow, Coreen A and Griffis, Andrew J},
booktitle = {Laser Radar Technol. Appl. VII},
doi = {10.1117/12.476407},
editor = {Kamerman, Gary W},
keywords = {3D,Flash ladar,imaging lidar,laser radar,scannerless lidar,streak tube},
organization = {International Society for Optics and Photonics},
pages = {9--18},
publisher = {SPIE},
title = {{Flash lidar based on multiple-slit streak tube imaging lidar}},
url = {https://doi.org/10.1117/12.476407},
volume = {4723},
year = {2002}
}
@misc{udacity,
title = {{self-driving-car/datasets at master {\textperiodcentered} udacity/self-driving-car {\textperiodcentered} GitHub}},
url = {https://github.com/udacity/self-driving-car/tree/master/datasets},
urldate = {2019-10-20}
}
@article{Simpson2019,
author = {Simpson, John T and Us, T N},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Simpson, Us/( 12 ) United States Patent. Simpson, Us. 2019.pdf:pdf},
number = {12},
title = {{( 12 ) United States Patent}},
volume = {2},
year = {2019}
}
@article{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Girshick/Fast R-CNN. Girshick. 2015.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
pages = {1440--1448},
title = {{Fast R-CNN}},
volume = {2015 Inter},
year = {2015}
}
@article{Geiger2013a,
abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high- resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to inner- city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide},
author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger et al/Vision meets robotics The KITTI dataset. The International Journal of Robotics Research. Geiger et al.. 2013.pdf:pdf},
journal = {Int. J. Robot. Res. - IJRR},
number = {October},
pages = {1--6},
title = {{Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research}},
year = {2013}
}
@article{Liao2019,
abstract = {Fusion of heterogeneous exteroceptive sensors is the most efficient and effective path to the representation of the environment precisely, as it can compromise various drawbacks of each homogeneous sensor. The rigid transformation (aka. extrinsic parameters) of heterogeneous sensory systems is the prerequisite of fusing the multi-sensor information. Researchers have proposed several approaches to estimate the extrinsic parameters. However, these approaches neither rely on human interventions or specifically designed auxiliary object or do not provide the library which makes it hard to test or benchmark. In this paper, we propose a novel extrinsic calibration approach for the extrinsic calibration of a Lidar (Laser Range Finder) and a camera which only based on a polygon board and we offer the relevant tools. In this paper, we firstly track and extract the target polygon from both the image and point-cloud. Then we try to match the polygon between the 2D and 3D feature spaces. With the associated polygon, we are able to get multiple constraints to optimize the extrinsic parameters. At the end, we validate our approach by four configurations , including the simulation, 16/32-beam Lidar and 100-line MEMS-Lidar. The outcome indicates high-precision extrinsic calibration performance.},
author = {Liao, Qinghai and Chen, Zhenyong and Liu, Yang and Wang, Zhe and Liu, Ming},
doi = {10.1109/ROBIO.2018.8665256},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Liao et al/Extrinsic Calibration of Lidar and Camera with Polygon. Liao et al.. 2019.pdf:pdf},
isbn = {9781728103761},
journal = {2018 IEEE Int. Conf. Robot. Biomimetics, ROBIO 2018},
pages = {200--205},
publisher = {IEEE},
title = {{Extrinsic Calibration of Lidar and Camera with Polygon}},
year = {2019}
}
@misc{opencv_doc,
author = {OpenCV},
title = {{Camera Calibration and 3D Reconstruction — OpenCV 2.4.13.7 documentation}},
url = {https://docs.opencv.org/2.4/modules/calib3d/doc/camera{\_}calibration{\_}and{\_}3d{\_}reconstruction.html},
urldate = {2019-04-21}
}
@article{Penate-Sanchez2013a,
abstract = {We propose a novel approach for the estimation of the pose and focal length of a camera from a set of 3D-to-2D point correspondences. Our method compares favorably to competing approaches in that it is both more accurate than existing closed form solutions, as well as faster and also more accurate than iterative ones. Our approach is inspired on the EPnP algorithm, a recent O(n) solution for the calibrated case. Yet we show that considering the focal length as an additional unknown renders the linearization and relinearization techniques of the original approach no longer valid, especially with large amounts of noise. We present new methodologies to circumvent this limitation termed exhaustive linearization and exhaustive relinearization which perform a systematic exploration of the solution space in closed form. The method is evaluated on both real and synthetic data, and our results show that besides producing precise focal length estimation, the retrieved camera pose is almost as accurate as the one computed using the EPnP, which assumes a calibrated camera. {\textcopyright} 1979-2012 IEEE.},
author = {Penate-Sanchez, Adrian and Andrade-Cetto, Juan and Moreno-Noguer, Francesc},
doi = {10.1109/TPAMI.2013.36},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Penate-Sanchez, Andrade-Cetto, Moreno-Noguer/Exhaustive linearization for robust camera pose and focal length estimation. Penate-Sanchez, Andrade-Cetto, Moreno-Noguer. 2013.pdf:pdf},
issn = {01628828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
keywords = {Camera calibration,perspective-n-point problem},
number = {10},
pages = {2387--2400},
title = {{Exhaustive linearization for robust camera pose and focal length estimation}},
volume = {35},
year = {2013}
}
@article{Everingham2010,
abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension. {\textcopyright} 2009 Springer Science+Business Media, LLC.},
author = {Everingham, Mark and {Van Gool}, Luc and Williams, Christopher K.I. and Winn, John and Zisserman, Andrew},
doi = {10.1007/s11263-009-0275-4},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Everingham et al/The pascal visual object classes (VOC) challenge. Everingham et al.. 2010.pdf:pdf},
issn = {09205691},
journal = {Int. J. Comput. Vis.},
keywords = {Benchmark,Database,Object detection,Object recognition},
number = {2},
pages = {303--338},
title = {{The pascal visual object classes (VOC) challenge}},
volume = {88},
year = {2010}
}
@misc{comercial_cameras,
title = {{03 – Different Types of Cameras » Photo Class}},
url = {http://www.r-photoclass.com/03-different-types-of-cameras/},
urldate = {2019-10-23}
}
@techreport{AVTCables,
author = {Vision, Allied},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Vision/AVT IO cable. Vision. 2013.pdf:pdf},
title = {{AVT I/O cable}},
year = {2013}
}
@book{1385,
author = {Richard, Hartley and Andrew, Zisserman},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hartley, Zisserman/Multiple View Geometry in computer vision. Hartley, Zisserman. 2003.pdf:pdf},
isbn = {9780521540513},
pages = {302},
title = {{Multiple View Geometry in computer vision}},
year = {1385}
}
@article{Roca2006,
archivePrefix = {arXiv},
arxivId = {arXiv:1208.5721},
author = {Roca, Sergi Figueras and Cited, References},
doi = {10.1038/incomms1464},
eprint = {arXiv:1208.5721},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Roca, Cited/( 12 ) United States Patent. Roca, Cited. 2006.pdf:pdf},
isbn = {1020080012051},
issn = {2004001828},
number = {12},
pmid = {1000182772},
title = {{( 12 ) United States Patent}},
volume = {2},
year = {2006}
}
@article{Fritsch2013,
abstract = {Detecting the road area and ego-lane ahead of a vehicle is central to modern driver assistance systems. While lane-detection on well-marked roads is already available in modern vehicles, finding the boundaries of unmarked or weakly marked roads and lanes as they appear in inner-city and rural environments remains an unsolved problem due to the high variability in scene layout and illumination conditions, amongst others. While recent years have witnessed great interest in this subject, to date no commonly agreed upon benchmark exists, rendering a fair comparison amongst methods difficult. In this paper, we introduce a novel open-access dataset and benchmark for road area and ego-lane detection. Our dataset comprises 600 annotated training and test images of high variability from the KITTI autonomous driving project, capturing a broad spectrum of urban road scenes. For evaluation, we propose to use the 2D Bird's Eye View (BEV) space as vehicle control usually happens in this 2D world, requiring detection results to be represented in this very same space. Furthermore, we propose a novel, behavior-based metric which judges the utility of the extracted ego-lane area for driver assistance applications by fitting a driving corridor to the road detection results in the BEV. We believe this to be important for a meaningful evaluation as pixel-level performance is of limited value for vehicle control. State-of-the-art road detection algorithms are used to demonstrate results using classical pixel-level metrics in perspective and BEV space as well as the novel behavior-based performance measure. All data and annotations are made publicly available on the KITTI online evaluation website in order to serve as a common benchmark for road terrain detection algorithms.},
author = {Fritsch, Jannik and Kuhnl, Tobias and Geiger, Andreas},
doi = {10.1109/ITSC.2013.6728473},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fritsch, Kuhnl, Geiger/A new performance measure and evaluation benchmark for road detection algorithms. Fritsch, Kuhnl, Geiger. 2013.pdf:pdf},
isbn = {9781479929146},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
pages = {1693--1700},
title = {{A new performance measure and evaluation benchmark for road detection algorithms}},
year = {2013}
}
@article{Jeong2019,
abstract = {This paper presents a framework for the targetless extrinsic calibration of stereo cameras and Light Detection and Ranging (LiDAR) sensors with a non-overlapping Field of View (FOV). In order to solve the extrinsic calibrations problem under such challenging configuration, the proposed solution exploits road markings as static and robust features among the various dynamic objects that are present in urban environment. First, this study utilizes road markings that are commonly captured by the two sensor modalities to select informative images for estimating the extrinsic parameters. In order to accomplish stable optimization, multiple cost functions are defined, including Normalized Information Distance (NID), edge alignment and, plane fitting cost. Therefore a smooth cost curve is formed for global optimization to prevent convergence to the local optimal point. We further evaluate each cost function by examining parameter sensitivity near the optimal point. Another key characteristic of extrinsic calibration, repeatability, is analyzed by conducting the proposed method multiple times with varying randomly perturbed initial points.},
archivePrefix = {arXiv},
arxivId = {1902.10586},
author = {Jeong, Jinyong and Cho, Lucas Y. and Kim, Ayoung},
eprint = {1902.10586},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Jeong, Cho, Kim/Road is Enough! Extrinsic Calibration of Non-overlapping Stereo Camera and LiDAR using Road Information. Jeong, Cho, Kim. 2019.pdf:pdf},
title = {{Road is Enough! Extrinsic Calibration of Non-overlapping Stereo Camera and LiDAR using Road Information}},
url = {http://arxiv.org/abs/1902.10586},
year = {2019}
}
@incollection{mAP,
address = {Boston, MA},
author = {Beitzel, Steven M and Jensen, Eric C and Frieder, Ophir},
booktitle = {Encycl. Database Syst.},
doi = {10.1007/978-0-387-39940-9_492},
editor = {LIU, LING and {\"{O}}ZSU, M TAMER},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Beitzel, Jensen, Frieder/mean Average Precision. Beitzel, Jensen, Frieder. 2009.pdf:pdf},
isbn = {978-0-387-39940-9},
pages = {1691--1692},
publisher = {Springer US},
title = {{mean Average Precision}},
url = {https://doi.org/10.1007/978-0-387-39940-9{\_}3032 https://doi.org/10.1007/978-0-387-39940-9{\_}492},
year = {2009}
}
@misc{WHOvisualizer,
title = {{WHO | Death on the roads}},
url = {https://extranet.who.int/roadsafety/death-on-the-roads/{\#}ticker//all},
urldate = {2019-10-20}
}
@article{Slabaugh,
author = {Slabaugh, Gregory G},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Slabaugh/Computing Euler angles from a rotation matrix. Slabaugh. Unknown.pdf:pdf},
pages = {1--7},
title = {{Computing Euler angles from a rotation matrix}}
}
@inproceedings{Stettner2010,
abstract = {The theory and operation of Advanced Scientific Concepts, Inc.'s (ASC) latest compact 3D Flash LIDAR Video Cameras (3D FLVCs) and a growing number of technical problems and solutions are discussed. The solutions range from space shuttle docking, planetary entry, decent and landing, surveillance, autonomous and manned ground vehicle navigation and 3D imaging through particle obscurants.},
author = {Stettner, Roger},
booktitle = {Laser Radar Technol. Appl. XV},
doi = {10.1117/12.851831},
editor = {Turner, Monte D and Kamerman, Gary W},
keywords = {3D Imaging,Flash LIDAR,Range Gated Imaging},
organization = {International Society for Optics and Photonics},
pages = {39--46},
publisher = {SPIE},
title = {{Compact 3D flash lidar video cameras and applications}},
url = {https://doi.org/10.1117/12.851831},
volume = {7684},
year = {2010}
}
@misc{azurecv,
title = {{Image Processing with the Computer Vision API | Microsoft Azure}},
url = {https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/},
urldate = {2019-10-22}
}
@article{Taylor2013,
abstract = {This paper is about automatic calibration of a camera-lidar system. The method presented is designed to be as general as possible allowing it to be used in a large range of systems and applications. The approach uses normalized mutual information to compare camera images with lidar scans of the same area. A camera model that takes into account orientation, location and focal length is used to create a 2D lidar image, with the intensity of the pixels representing a feature of the lidar scan that is chosen depending on the application. Particle swarm optimization is used to find the optimal model parameters. The method presented is successfully validated on a variety of cameras, lidars and locations, including scans of both urban and natural environments},
author = {Taylor, Zachary and Nieto, Juan},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Taylor, Nieto/Automatic Calibration of Lidar and Camera Images using Normalized Mutual Information. Taylor, Nieto. 2013.pdf:pdf},
journal = {IEEE Int. Conf. Robot. Autom.},
title = {{Automatic Calibration of Lidar and Camera Images using Normalized Mutual Information}},
url = {https://pdfs.semanticscholar.org/fc08/d41be320a827c3f045c4429e13495da5a1cc.pdf{\%}0Ahttp://www-personal.acfr.usyd.edu.au/jnieto/Publications{\_}files/TaylorICRA2013.pdf},
year = {2013}
}
@article{Yoo2018,
abstract = {Lidar, the acronym of light detection and ranging, has received much attention for the automotive industry as a key component for high level automated driving systems due to their high resolution and highly accurate 3D imaging of the surroundings under various weather conditions. However, the price and resolution of lidar sensors still do not meet the target values for the automotive market to be accepted as a basic sensor for ensuring safe autonomous driving. Recent work has focused on MEMS scanning mirrors as a potential solution for affordable long range lidar systems. This paper discusses current developments and research on MEMS-based lidars. The LiDcAR project is introduced for bringing precise and reliable MEMS-based lidars to enable safe and reliable autonomous driving. As a part of development in this project, a test bench for the characterization and performance evaluation of MEMS mirror is introduced. A recently developed MEMS-based lidar will be evaluated by various levels of tests including field tests based on realistic scenarios, aiming for safe and reliable autonomous driving in future automotive industry.},
author = {Yoo, Han Woong and Druml, Norbert and Brunner, David and Schwarzl, Christian and Thurner, Thomas and Hennecke, Marcus and Schitter, Georg},
doi = {10.1007/s00502-018-0635-2},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yoo et al/MEMS-based lidar for autonomous driving. Yoo et al.. 2018.pdf:pdf},
isbn = {0050201806352},
issn = {0932383X},
journal = {Elektrotechnik und Informationstechnik},
keywords = {MEMS scanning mirror,autonomous driving,lidar,metrology platform},
number = {6},
pages = {408--415},
publisher = {The Author(s)},
title = {{MEMS-based lidar for autonomous driving}},
url = {http://dx.doi.org/10.1007/s00502-018-0635-2},
volume = {135},
year = {2018}
}
@article{Petit2015,
abstract = {Autonomous automated vehicles are the next evolution in transportation and will improve safety, traffic efficiency and driving experience. Automated vehicles are equipped with multiple sensors (LiDAR, radar, camera, etc.) enabling lo- cal awareness of their surroundings. A fully automated vehi- cle will unconditionally rely on its sensors readings to make short-term (i.e. safety-related) and long-term (i.e. planning) driving decisions. In this context, sensors have to be robust against intentional or unintentional attacks that aim at low- ering sensor data quality to disrupt the automation system. This paper presents remote attacks on camera-based system and LiDAR using commodity hardware. Results from lab- oratory experiments show effective blinding, jamming, re- play, relay, and spoofing attacks. We propose software and hardware countermeasures that improve sensors resilience against these attacks},
author = {Petit, Jonathan; and Stottelaar, Bas and Feiri, Michael and Kargl, Frank},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Petit et al/Remote Attacks on Automated Vehicles Sensors Experiments on Camera and LiDAR. Petit et al.. 2015.pdf:pdf},
journal = {Blackhat.com},
keywords = {automated vehicle,camera,lidar,remote attack,security},
pages = {1--13},
title = {{Remote Attacks on Automated Vehicles Sensors: Experiments on Camera and LiDAR}},
url = {https://www.blackhat.com/docs/eu-15/materials/eu-15-Petit-Self-Driving-And-Connected-Cars-Fooling-Sensors-And-Tracking-Drivers-wp1.pdf},
year = {2015}
}
@book{MantaVision2013,
author = {Vision, Allied and Gmbh, Technologies},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Vision, Gmbh/AVT GigE Cameras Camera and Driver Features Legal notice. Vision, Gmbh. 2013.pdf:pdf},
isbn = {6048758855},
number = {September},
pages = {1--57},
title = {{AVT GigE Cameras Camera and Driver Features Legal notice}},
year = {2013}
}
@misc{dlib,
title = {{dlib C++ Library}},
url = {http://dlib.net/},
urldate = {2019-10-22}
}
@inproceedings{Hughes2011a,
abstract = {An object recognition system has been developed that uses a new class oflocal image features. The features are invariant to image scaling, translation,and rotation, and partially in- variant to illumination changes and affine or 3D projection. These features share similar properties with neurons in in- ferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local ge- ometric deformations by representing blurred image gradi- ents in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final veri- fication ofeach match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time ofunder 2 seconds},
author = {Hughes, R.A.},
booktitle = {Int. Comput. Corfu},
doi = {10.1130/2011.2482(04)},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hughes/Geoscience data and derived spatial information Societal impacts and benefits, and relevance to geological surveys and agencies. Hughes..pdf:pdf},
isbn = {9780813724829},
issn = {00721077},
month = {sep},
title = {{Object Recognition from Local Scale-Invariant Features}},
url = {https://pubs.geoscienceworld.org/books/book/641/chapter/3806367/},
year = {1999}
}
@misc{Quanergy2018,
author = {Eldada, Louay},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Eldada/THREE - DIMENSIONAL - MAPPING TWO - DIMENSIONAL - SCANNING LIDAR BASED ON ONE - DIMENSIONAL - STEERING OPTICAL PHASED ARRAYS AND METHOD.pdf:pdf},
institution = {QUANERGY SYSTEMS},
title = {{THREE - DIMENSIONAL - MAPPING TWO - DIMENSIONAL - SCANNING LIDAR BASED ON ONE - DIMENSIONAL - STEERING OPTICAL PHASED ARRAYS AND METHOD OF USING SAME}},
volume = {2},
year = {2018}
}
@techreport{VLP16,
address = {San Jose},
author = {Rev, E},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rev/VLP-16 User Manual. Rev. 2019.pdf:pdf},
institution = {Velodyne LiDAR, Inc.},
pages = {71},
title = {{VLP-16 User Manual}},
year = {2019}
}
@article{Open3D,
abstract = {Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.},
archivePrefix = {arXiv},
arxivId = {1801.09847},
author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
eprint = {1801.09847},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Zhou, Park, Koltun/Open3D A Modern Library for 3D Data Processing. Zhou, Park, Koltun. 2018.pdf:pdf},
month = {jan},
title = {{Open3D: A Modern Library for 3D Data Processing}},
url = {http://arxiv.org/abs/1801.09847},
year = {2018}
}
@article{WHOsite,
journal = {WHO},
keywords = {road safety [subject]},
publisher = {World Health Organization},
title = {{WHO | Global status report on road safety 2018}},
url = {https://www.who.int/violence{\_}injury{\_}prevention/road{\_}safety{\_}status/2018/en/},
year = {2019}
}
@article{Fremont2013,
author = {Fremont, Vincent and Alberto, Sergio and Florez, Rodriguez and Bonnifait, Philippe and Targets, Circular and Video, Alignment and Sensors, Lidar and Robotics, Advanced and Stm, Francis},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fremont et al/Circular Targets for 3D Alignment of Video and Lidar Sensors To cite this version Circular Targets for 3D Alignment of Video and Lidar.pdf:pdf},
title = {{Circular Targets for 3D Alignment of Video and Lidar Sensors To cite this version : Circular Targets for 3D Alignment of Video and Lidar Sensors}},
year = {2013}
}
@misc{Quanergy,
author = {Quanergy},
title = {{Quanergy – LiDAR sensors and smart sensing solutions}},
url = {https://quanergy.com/},
urldate = {2019-11-27}
}
@article{Gong2013,
abstract = {This paper presents a novel way to address the extrinsic calibration problem for a system composed of a 3D LIDAR and a camera. The relative transformation between the two sensors is calibrated via a nonlinear least squares (NLS) problem, which is formulated in terms of the geometric constraints associated with a trihedral object. Precise initial estimates of NLS are obtained by dividing it into two sub-problems that are solved individually. With the precise initializations, the calibration parameters are further refined by iteratively optimizing the NLS problem. The algorithm is validated on both simulated and real data, as well as a 3D reconstruction application. Moreover, since the trihedral target used for calibration can be either orthogonal or not, it is very often present in structured environments, making the calibration convenient.},
author = {Gong, Xiaojin and Lin, Ying and Liu, Jilin},
doi = {10.3390/s130201902},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gong, Lin, Liu/3D LIDAR-camera extrinsic calibration using an arbitrary trihedron. Gong, Lin, Liu. 2013.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {3D LIDAR-camera system,Extrinsic calibration,Sensor fusion},
number = {2},
pages = {1902--1918},
title = {{3D LIDAR-camera extrinsic calibration using an arbitrary trihedron}},
volume = {13},
year = {2013}
}
@misc{opencv,
title = {{OpenCV}},
url = {https://opencv.org/},
urldate = {2019-10-22}
}
@article{Shaukat2016,
abstract = {{\textcopyright} 2016 by the authors; licensee MDPI,Basel,Switzerland. In recent decades,terrain modelling and reconstruction techniques have increased research interest in precise short and long distance autonomous navigation,localisation and mapping within field robotics. One of the most challenging applications is in relation to autonomous planetary exploration using mobile robots. Rovers deployed to explore extraterrestrial surfaces are required to perceive and model the environment with little or no intervention from the ground station. Up to date,stereopsis represents the state-of-the art method and can achieve short-distance planetary surface modelling. However,future space missions will require scene reconstruction at greater distance,fidelity and feature complexity,potentially using other sensors like Light Detection And Ranging (LIDAR). LIDAR has been extensively exploited for target detection,identification,and depth estimation in terrestrial robotics,but is still under development to become a viable technology for space robotics. This paper will first review current methods for scene reconstruction and terrain modelling using cameras in planetary robotics and LIDARs in terrestrial robotics; then we will propose camera-LIDAR fusion as a feasible technique to overcome the limitations of either of these individual sensors for planetary exploration. A comprehensive analysis will be presented to demonstrate the advantages of camera-LIDAR fusion in terms of range,fidelity,accuracy and computation.},
author = {Shaukat, Affan and Blacker, Peter C. and Spiteri, Conrad and Gao, Yang},
doi = {10.3390/s16111952},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Shaukat et al/Towards camera-LIDAR fusion-based terrain modelling for planetary surfaces Review and analysis. Shaukat et al.. 2016.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {3-D reconstruction,Hybrid vision systems,LIDAR-camera fusion,Planetary surface perception,Terrain modelling},
number = {11},
title = {{Towards camera-LIDAR fusion-based terrain modelling for planetary surfaces: Review and analysis}},
volume = {16},
year = {2016}
}
@article{Chien2017,
abstract = {{\textcopyright} 2016 IEEE. Recently LiDAR-camera systems have rapidly emerged in many applications. The integration of laser range-finding technologies into existing vision systems enables a more comprehensive understanding of 3D structure of the environment. The advantage, however, relies on a good geometrical calibration between the LiDAR and the image sensors. In this paper we consider visual odometry, a discipline in computer vision and robotics, in the context of recently emerging online sensory calibration studies. By embedding the online calibration problem into a LiDAR-monocular visual odometry technique, the temporal change of extrinsic parameters can be tracked and compensated effectively.},
author = {Chien, Hsiang Jen and Klette, Reinhard and Schneider, Nick and Franke, Uwe},
doi = {10.1109/ICPR.2016.7900068},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Chien et al/Visual odometry driven online calibration for monocular LiDAR-camera systems. Chien et al.. 2017.pdf:pdf},
isbn = {9781509048472},
issn = {10514651},
journal = {Proc. - Int. Conf. Pattern Recognit.},
month = {apr},
pages = {2848--2853},
publisher = {IEEE},
title = {{Visual odometry driven online calibration for monocular LiDAR-camera systems}},
year = {2017}
}
@article{Scaramuzza,
author = {Scaramuzza, Davide and Harati, Ahad and Siegwart, Roland},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Scaramuzza, Harati, Siegwart/Extrinsic self calibration of a camera and a 3D Laser Range Finder. Scaramuzza, Harati, Siegwart. Unknown.pdf:pdf},
keywords = {camera calibration,computer vision,laser calibration,omndirectional vision,omnidirectional camera,p3p algorithm,pnp algorithm,reprojection error,robotics,self calibration},
title = {{Extrinsic self calibration of a camera and a 3D Laser Range Finder}}
}
@article{Pusztai2018,
abstract = {As autonomous driving attracts more and more attention these days, the algorithms and sensors used for machine perception become popular in research, as well. This paper investigates the extrinsic calibration of two frequently-applied sensors: the camera and Light Detection and Ranging (LiDAR). The calibration can be done with the help of ordinary boxes. It contains an iterative refinement step, which is proven to converge to the box in the LiDAR point cloud, and can be used for system calibration containing multiple LiDARs and cameras. For that purpose, a bundle adjustment-like minimization is also presented. The accuracy of the method is evaluated on both synthetic and real-world data, outperforming the state-of-the-art techniques. The method is general in the sense that it is both LiDAR and camera-type independent, and only the intrinsic camera parameters have to be known. Finally, a method for determining the 2D bounding box of the car chassis from LiDAR point clouds is also presented in order to determine the car body border with respect to the calibrated sensors.},
author = {Pusztai, Zolt{\'{a}}n and Eichhardt, Iv{\'{a}}n and Hajder, Levente},
doi = {10.3390/s18072139},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pusztai, Eichhardt, Hajder/Accurate calibration of multi-lidar-multi-camera systems. Pusztai, Eichhardt, Hajder. 2018.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Autonomous driving,Camera,Extrinsic calibration,LiDAR,LiDAR camera system,Machine perception},
number = {7},
pages = {1--22},
title = {{Accurate calibration of multi-lidar-multi-camera systems}},
volume = {18},
year = {2018}
}
@article{Girshick2014,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra and Berkeley, U C and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Girshick et al/1043.0690. Girshick et al.. 2014.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {5000},
pmid = {26656583},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://arxiv.},
volume = {1},
year = {2014}
}
@misc{LeddarTech,
abstract = {Leddar: high performance, affordable LiDAR solid-state sensing technology for automotive, drones, collision avoidance, ITS and more},
author = {Leddartech},
title = {{LeddarTech - Mastering LiDAR Sensor Technology}},
url = {https://leddartech.com/},
urldate = {2019-11-27},
year = {2018}
}
@article{WHO2018,
abstract = {As matter fact road traffic injuries are currently ranked ninth globally among the leading causes of disability adjusted life years lost, and the ranking is projected to rise to third by 2020(Ishrat Riaz, 2018)},
author = {WHO},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/WHO/Global Status Report on Road. WHO. 2018.pdf:pdf},
journal = {World Heal. Organ.},
pages = {20},
title = {{Global Status Report on Road}},
url = {https://www.who.int/violence{\_}injury{\_}prevention/road{\_}safety{\_}status/2018/en/},
year = {2018}
}
@article{Bileschi2009,
author = {Bileschi, Stanley},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Bileschi/Fully automatic calibration of LIDAR and video streams from a vehicle The MIT Faculty has made this article openly available . Please.pdf:pdf},
isbn = {9781424444410},
journal = {Comput. Vis. Work. (ICCV Work.},
keywords = {automatic calibration},
mendeley-tags = {automatic calibration},
pages = {1457--1464},
title = {{Fully automatic calibration of LIDAR and video streams from a vehicle The MIT Faculty has made this article openly available . Please share Bileschi , S . “ Fully automatic calibration of LIDAR and video Publisher Version Accessed Citable Link Terms of Us}},
year = {2009}
}
@article{Scaramuzza,
author = {Scaramuzza, Davide and Harati, Ahad and Siegwart, Roland},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Scaramuzza, Harati, Siegwart/Extrinsic self calibration of a camera and a 3D Laser Range Finder. Scaramuzza, Harati, Siegwart. Unknown.pdf:pdf},
keywords = {camera calibration,computer vision,laser calibration,omndirectional vision,omnidirectional camera,p3p algorithm,pnp algorithm,reprojection error,robotics,self calibration},
title = {{Extrinsic self calibration of a camera and a 3D Laser Range Finder}}
}
@misc{MantaG504C,
author = {Vision, Allied},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Vision/Manta G-504 datasheet. Vision. Unknown.pdf:pdf},
publisher = {Allied Vision},
title = {{Manta G-504 datasheet}}
}
@article{DeSilva2018,
abstract = {Autonomous robots that assist humans in day to day living tasks are becoming increasingly popular. Autonomous mobile robots operate by sensing and perceiving their surrounding environment to make accurate driving decisions. A combination of several different sensors such as LiDAR, radar, ultrasound sensors and cameras are utilized to sense the surrounding environment of autonomous vehicles. These heterogeneous sensors simultaneously capture various physical attributes of the environment. Such multimodality and redundancy of sensing need to be positively utilized for reliable and consistent perception of the environment through sensor data fusion. However, these multimodal sensor data streams are different from each other in many ways, such as temporal and spatial resolution, data format, and geometric alignment. For the subsequent perception algorithms to utilize the diversity offered by multimodal sensing, the data streams need to be spatially, geometrically and temporally aligned with each other. In this paper, we address the problem of fusing the outputs of a Light Detection and Ranging (LiDAR) scanner and a wide-angle monocular image sensor for free space detection. The outputs of LiDAR scanner and the image sensor are of different spatial resolutions and need to be aligned with each other. A geometrical model is used to spatially align the two sensor outputs, followed by a Gaussian Process (GP) regression-based resolution matching algorithm to interpolate the missing data with quantifiable uncertainty. The results indicate that the proposed sensor data fusion framework significantly aids the subsequent perception steps, as illustrated by the performance improvement of a uncertainty aware free space detection algorithm},
archivePrefix = {arXiv},
arxivId = {1710.06230},
author = {{De Silva}, Varuna and Roche, Jamie and Kondoz, Ahmet},
doi = {10.3390/s18082730},
eprint = {1710.06230},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/De Silva, Roche, Kondoz/Robust fusion of LiDAR and wide-angle camera data for autonomous mobile robots. De Silva, Roche, Kondoz. 2018.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Assistive robots,Autonomous vehicles,Depth sensing,Free space detection,Gaussian process regression,LiDAR,Sensor data fusion},
number = {8},
title = {{Robust fusion of LiDAR and wide-angle camera data for autonomous mobile robots}},
volume = {18},
year = {2018}
}
@article{Steder,
author = {Steder, Bastian},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/The Point Cloud Library PCL Thanks to Radu Rusu from Willow Garage for some of the slides!. Steder.pdf:pdf},
title = {{The Point Cloud Library PCL Thanks to Radu Rusu from Willow Garage for some of the slides!}},
url = {http://ais.informatik.uni-freiburg.de/teaching/ws10/robotics2/pdfs/rob2-12-ros-pcl.pdf}
}
@article{Hall2017,
abstract = {Methods and systems for performing multiple pulse LIDAR measurements are presented herein. In one aspect, each LIDAR measurement beam illuminates a location in a three dimensional environment with a sequence of multiple pulses of illumination light. Light reflected from the location is detected by a photosensitive detector of the LIDAR system during a measurement window having a duration that is greater than or equal to the time of flight of light from the LIDAR system out to the programmed range of the LIDAR system, and back. The pulses in a measurement pulse sequence can vary in magnitude and duration. Furthermore, the delay between pulses and the number of pulses in each measurement pulse sequence can also be varied. In some embodiments, the multi-pulse illumination beam is encoded and the return measurement pulse sequence is decoded to distinguish the measurement pulse sequence from exogenous signals.},
author = {Hall, David S. and Kerstens, Pieter J.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hall, Kerstens/Multiple Pulse, LIDAR Based 3-D Imaging. Hall, Kerstens. 2017.pdf:pdf},
number = {19},
title = {{Multiple Pulse, LIDAR Based 3-D Imaging}},
url = {https://patents.google.com/patent/US20170219695A1/},
volume = {1},
year = {2017}
}
@misc{Retterath2016,
abstract = {LiDAR (light detection and ranging) systems use one or more emitters and a detector array to cover a given field of view where the emitters each emit a single pulse or a multi-pulse packet of light that is sampled by the detector array. On each emitter cycle the detector array will sample the incoming signal intensity at the pre-determined sampling frequency that generates two or more samples per emitted light packet to allow for volumetric analysis of the retroreflected signal por tion of each emitted light packet as reflected by one or more objects in the field of view and then received by each detector.},
author = {Retterath, James E. and Laumeyer, Robert A.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Retterath, Laumeyer/( 12 ) United States Patent. Retterath, Laumeyer. 2016.pdf:pdf},
title = {{( 12 ) United States Patent}},
year = {2016}
}
@inproceedings{Hebert,
author = {Hebert, Martial and Krotkov, Eric},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hebert, Krotkov/3-D Measurements From Imaging Laser Radars How Good Are They. Hebert, Krotkov. 1991.pdf:pdf},
title = {{3-D Measurements From Imaging Laser Radars: How Good Are They?}},
year = {1991}
}
@article{Redmon2017,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster R-CNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
doi = {10.1109/CVPR.2017.690},
eprint = {1612.08242},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Redmon, Farhadi/YOLO9000 Better, faster, stronger. Redmon, Farhadi. 2017.pdf:pdf},
isbn = {9781538604571},
journal = {Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017},
pages = {6517--6525},
title = {{YOLO9000: Better, faster, stronger}},
volume = {2017-Janua},
year = {2017}
}
@article{nuScenes2019,
archivePrefix = {arXiv},
arxivId = {1903.11027},
author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
eprint = {1903.11027},
journal = {arXiv Prepr.},
title = {{nuScenes: A multimodal dataset for autonomous driving}},
url = {https://www.nuscenes.org/},
year = {2019}
}
@article{Szeliski2011a,
abstract = {As humans, we perceive the three-dimensional structure of the world around us with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem, and what is the current state of the art?Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging and fun consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos.More than just a source of "recipes", this text/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting this process to produce the best possible descriptions of a scene. Exercises are presented throughout the book, with a heavy emphasis on testing algorithms.Suitable for either an undergraduate or a graduate-level course in computer vision, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries.Dr. Richard Szeliski has over twenty years' experience in computer vision research, most notably at Digital Equipment Corporation and Microsoft.},
author = {Szeliski, Richard},
doi = {10.5860/choice.48-5140},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Szeliski/Computer vision algorithms and applications. Szeliski. 2011.pdf:pdf},
issn = {0009-4978},
journal = {Choice Rev. Online},
number = {09},
pages = {48--5140--48--5140},
title = {{Computer vision: algorithms and applications}},
volume = {48},
year = {2011}
}
@misc{CameronOliver2017,
abstract = {Why did LIDAR take off with self-driving cars? In a word: mapping. LIDAR allows you to generate huge 3D maps (its original application!), which you can then navigate the car or robot predictably within. By using a LIDAR to map and navigate an environment, you can know ahead of time the bounds of a lane, or that there is a stop sign or traffic light 500m ahead. This kind of predictability is exactly what a technology like self-driving cars requires, and has been a big reason for the progress over the last 5 years.
},
author = {{Cameron Oliver}},
booktitle = {Voyage},
keywords = {LIDAR,Self-Driving Cars},
mendeley-tags = {LIDAR,Self-Driving Cars},
title = {{An Introduction to LIDAR: The Key Self-Driving Car Sensor}},
url = {https://news.voyage.auto/an-introduction-to-lidar-the-key-self-driving-car-sensor-a7e405590cff},
urldate = {2018-10-09},
year = {2017}
}
@article{HesaiPhotonicsTechnology,
author = {{Hesai Photonics Technology}, Ltd Co},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hesai Photonics Technology/40-Channel Mechanical. Hesai Photonics Technology. Unknown.pdf:pdf},
title = {{40-Channel Mechanical}}
}
@misc{CGAL,
title = {{The Computational Geometry Algorithms Library}},
url = {https://www.cgal.org/index.html},
urldate = {2019-11-28}
}
@article{Cleeremans1989,
abstract = {We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t-1, together with element t, to predict element t + 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to correspond to the nodes of the grammar, although this correspondence is not necessary for the network to act as a perfect finite-state recognizer. We explore the conditions under which the network can carry information about distant sequential contingencies across intervening elements. Such information is maintained with relative ease if it is relevant at each intermediate step; it tends to be lost when intervening elements do not depend on it. At first glance this may suggest that such networks are not relevant to natural language, in which dependencies may span indefinite distances. However, embeddings in natural language are not completely independent of earlier information. The final simulation shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information.},
author = {Cleeremans, Axel and Servan-Schreiber, David and McClelland, James L.},
doi = {10.1162/neco.1989.1.3.372},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Cleeremans, Servan-Schreiber, McClelland/Finite State Automata and Simple Recurrent Networks. Cleeremans, Servan-Schreiber, McClelland. 1989.pdf:pdf},
issn = {0899-7667},
journal = {Neural Comput.},
number = {3},
pages = {372--381},
title = {{Finite State Automata and Simple Recurrent Networks}},
volume = {1},
year = {1989}
}
@misc{OpenCV_camera_calib,
author = {OpenCV},
title = {{OpenCV: Camera calibration With OpenCV}},
url = {https://docs.opencv.org/3.2.0/d4/d94/tutorial{\_}camera{\_}calibration.html https://opencv-python-tutroals.readthedocs.io/en/latest/py{\_}tutorials/py{\_}calib3d/py{\_}calibration/py{\_}calibration.html},
urldate = {2019-10-22}
}
@misc{Retterath2015WO,
author = {Retterath, Jamie E. and Laumeyer, Robert A.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Retterath, Laumeyer/METHODS AND APPARATUS FOR ARRAY BASED LIDAR SYSTEMS WITH REDUCED INTERFERENCE. Retterath, Laumeyer. 2015.pdf:pdf},
number = {12},
title = {{METHODS AND APPARATUS FOR ARRAY BASED LIDAR SYSTEMS WITH REDUCED INTERFERENCE}},
year = {2015}
}
@article{EvansT.C.GavrilovichE.MihaiR.C.andIsbasescuI.2014,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0403007},
author = {{Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isbasescu, I.}, Easyg Llc and Thelen, Darryl and Martin, J A and Allen, S M and SA, Slane},
doi = {10.1037/t24245-000},
eprint = {0403007},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isbasescu, I. et al/( 12 ) Patent Application Publication ( 10 ) Pub . No . US 2006 0222585 A1 Figure 1. Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isba.pdf:pdf},
isbn = {2009023471},
issn = {13871811},
number = {15},
pages = {354},
pmid = {23110556},
primaryClass = {arXiv:physics},
title = {{( 12 ) Patent Application Publication ( 10 ) Pub . No .: US 2006 / 0222585 A1 Figure 1}},
volume = {002},
year = {2014}
}
@misc{matlabcvtoolbox,
title = {{Computer Vision Toolbox - MATLAB {\&} Simulink}},
url = {https://www.mathworks.com/products/computer-vision.html},
urldate = {2019-10-22}
}
@misc{Bezemek2017,
author = {Bezemek, Mike},
booktitle = {Velodyne LiDAR},
title = {{It Began With a Race...16 Years of Velodyne LiDAR}},
url = {https://velodynelidar.com/newsroom/it-began-with-a-race/},
urldate = {2019-10-20},
year = {2017}
}
@article{Hast2013,
abstract = {Optimal RANSAC - Towards a Repeatable Algorithm for Finding the Optimal Set},
author = {Hast, Anders and Nysj{\"{o}}, Johan and Marchetti, Andrea},
issn = {12136972},
journal = {J. WSCG},
keywords = {3D planes,Feature matching,Image stitching,Local optimisation,Optimal set,RANSAC,Repeatable},
number = {1},
pages = {21--30},
title = {{Optimal RANSAC - Towards a repeatable algorithm for finding the optimal set}},
url = {http://www.cb.uu.se/{~}aht/articles/A53-full.pdf},
volume = {21},
year = {2013}
}
@misc{Retterath2015,
abstract = {An array-based light detection and ranging (LiDAR) unit includes an array of emitter/detector sets configured to cover a field of view for the unit. Each emitter/detector set emits and receives light energy on a specific coincident axis unique for that emitter/detector set. A control system coupled to the array of emitter/detector sets controls initiation of light energy from each emitter and processes time of flight information for light energy received on the coincident axis by the corresponding detector for the emitter/detector set. The time of flight information provides imaging information corresponding to the field of view. Interference among light energy is reduced with respect to detectors in the LiDAR unit not corresponding to the specific coincident axis, and with respect to other LiDAR units and ambient sources of light energy. In one embodiment, multiple array-based LiDAR units are used as part of a control system for an autonomous vehicle.},
author = {Retterath, Jamie E. and Laumeyer, Robert A.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Unknown/Methods and Apparatus for Array Based Lidar Systems with Reduced Interference. Unknown. 2013.pdf:pdf},
institution = {Facet Technology Corp.,},
month = {nov},
title = {{Methods and Apparatus for Array Based Lidar Systems with Reduced Interference}},
url = {https://patents.google.com/patent/US20150131080A1/en},
year = {2015}
}
@misc{jsk_visualization,
author = {Ueda, Ryohei and Okada, Kei and Kakiuchi, Youhei},
title = {jsk{\_}visualization package},
url = {https://wiki.ros.org/jsk{\_}visualization https://github.com/jsk-ros-pkg/jsk{\_}visualization}
}
@misc{vlfeat,
author = {Vedaldi, A. and Fulkerson, B.},
title = {{VLFeat: An Open and Portable Library of Computer Vision Algorithms}},
url = {http://www.vlfeat.org/},
year = {2008}
}
@article{Wang2018a,
abstract = {We propose a new method for fusing LIDAR point cloud and camera-captured images in deep convolutional neural networks (CNN). The proposed method constructs a new layer called sparse non-homogeneous pooling layer to transform features between bird's eye view and front view. The sparse point cloud is used to construct the mapping between the two views. The pooling layer allows efficient fusion of the multi-view features at any stage of the network. This is favorable for 3D object detection using camera-LIDAR fusion for autonomous driving. A corresponding one-stage detector is designed and tested on the KITTI bird's eye view object detection dataset, which produces 3D bounding boxes from the bird's eye view map. The fusion method shows significant improvement on both speed and accuracy of the pedestrian detection over other fusion-based object detection networks.},
archivePrefix = {arXiv},
arxivId = {1711.06703},
author = {Wang, Zining and Zhan, Wei and Tomizuka, Masayoshi},
doi = {10.1109/IVS.2018.8500387},
eprint = {1711.06703},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wang, Zhan, Tomizuka/Fusing Bird's Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection. Wang, Zhan, Tomizuka. 2018.pdf:pdf},
isbn = {9781538644522},
journal = {IEEE Intell. Veh. Symp. Proc.},
pages = {834--839},
title = {{Fusing Bird's Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection}},
volume = {2018-June},
year = {2018}
}
@article{Huang2010,
abstract = {We describe the Lightweight Communications and Marshalling (LCM) library for message passing and data marshalling. The primary goal of LCM is to simplify the development of low-latency message passing systems, especially for real-time robotics research applications. Messages can be transmitted between different processes using LCM's publish/subscribe message-passing system. A platform- and language-independent type specification language separates message description from implementation. Message specifications are automatically compiled into language-specific bindings, eliminating the need for users to implement marshalling code while guaranteeing run-time type safety. LCM is notable in providing a real-time deep traffic inspection tool that can decode and display message traffic with minimal user effort and no impact on overall system performance. This and other features emphasize LCM's focus on simplifying both the development and debugging of message passing systems. In this paper, we explain the design of LCM, evaluate its performance, and describe its application to a number of autonomous land, underwater, and aerial robots. {\textcopyright}2010 IEEE.},
author = {Huang, Albert S. and Olson, Edwin and Moore, David C.},
doi = {10.1109/IROS.2010.5649358},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Huang, Olson, Moore/LCM Lightweight Communications and Marshalling. Huang, Olson, Moore. 2010.pdf:pdf},
isbn = {9781424466757},
journal = {IEEE/RSJ 2010 Int. Conf. Intell. Robot. Syst. IROS 2010 - Conf. Proc.},
number = {Lcm},
pages = {4057--4062},
title = {{LCM: Lightweight Communications and Marshalling}},
year = {2010}
}
@article{Pereira2016,
abstract = {Autonomous navigation is an important field of research and, given the complexity of real world environments, most of the systems rely on a complex perception system combining multiple sensors on board, which reinforces the concern of sensor calibration. Most calibration methods rely on manual or semi-automatic interactive procedures, but reliable fully automatic methods are still missing. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. The idea proposed in this paper is to use a ball in motion in front of a set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation among all pairs of sensors. This paper proposes and describes such a method with results demonstrating its validity.},
author = {Pereira, Marcelo and Silva, David and Santos, Vitor and Dias, Paulo},
doi = {10.1016/j.robot.2016.05.010},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pereira et al/Self calibration of multiple LIDARs and cameras on autonomous vehicles. Pereira et al.. 2016.pdf:pdf},
issn = {09218890},
journal = {Rob. Auton. Syst.},
keywords = {3D data fitting,Extrinsic calibration,Point cloud},
pages = {326--337},
title = {{Self calibration of multiple LIDARs and cameras on autonomous vehicles}},
volume = {83},
year = {2016}
}
@article{Krizhevsky2007,
abstract = {Delineating the tremendous growth in this area, the Handbook of Approximation Algorithms and Metaheuristics covers fundamental, theoretical topics as well as advanced, practical applications. It is the first book to comprehensively study both approximation algorithms and metaheuristics. Starting with basic approaches, the handbook presents the methodologies to design and analyze efficient approximation algorithms for a large class of problems, and to establish inapproximability results for another class of problems. It also discusses local search, neural networks, and metaheuristics, as well as multiobjective problems, sensitivity analysis, and stability. After laying this foundation, the book applies the methodologies to classical problems in combinatorial optimization, computational geometry, and graph problems. In addition, it explores large-scale and emerging applications in networks, bioinformatics, VLSI, game theory, and data analysis. Undoubtedly sparking further developments in the field, this handbook provides the essential techniques to apply approximation algorithms and metaheuristics to a wide range of problems in computer science, operations research, computer engineering, and economics. Armed with this information, researchers can design and analyze efficient algorithms to generate near-optimal solutions for a wide range of computational intractable problems.},
author = {Krizhevsky, Alex and {Ilya Sutskever} and {Geoffrey E. Hinton}},
doi = {10.1201/9781420010749},
editor = {Gonzalez, Teofilo F.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton/ImageNet Classification with Deep Convolutional Neural Networks. Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton. 2007.pdf:pdf},
isbn = {9780429143793},
journal = {Handb. Approx. Algorithms Metaheuristics},
month = {may},
pages = {1--1432},
publisher = {Chapman and Hall/CRC},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://www.taylorfrancis.com/books/9781420010749},
year = {2007}
}
@article{Hesch2011,
author = {Hesch, Joel A and Roumeliotis, Stergios I},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hesch, Roumeliotis/A Direct Least-Squares ( DLS ) Method for PnP. Hesch, Roumeliotis. 2011.pdf:pdf},
isbn = {9781457711022},
pages = {383--390},
title = {{A Direct Least-Squares ( DLS ) Method for PnP}},
year = {2011}
}
@phdthesis{brabec2014,
address = {Prague},
author = {Brabec, Jan},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Brabec/Automated camera calibration from laser scanning data in natural environments. Brabec. 2014.pdf:pdf},
school = {Czech Technical University in Prague},
title = {{Automated camera calibration from laser scanning data in natural environments}},
year = {2014}
}
@article{Carbone2018,
author = {Carbone, Marco},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Carbone/An introduction to automotive SDR. Carbone. 2018.pdf:pdf},
issn = {00134953},
journal = {Electron. Prod.},
number = {2},
title = {{An introduction to automotive SDR}},
volume = {61},
year = {2018}
}
@misc{awsRekognition,
author = {Aws},
title = {{Amazon Rekognition – Video and Image - AWS}},
url = {https://aws.amazon.com/rekognition/},
urldate = {2019-10-22}
}
@article{Pereira2016,
abstract = {Autonomous navigation is an important field of research and, given the complexity of real world environments, most of the systems rely on a complex perception system combining multiple sensors on board, which reinforces the concern of sensor calibration. Most calibration methods rely on manual or semi-automatic interactive procedures, but reliable fully automatic methods are still missing. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. The idea proposed in this paper is to use a ball in motion in front of a set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation among all pairs of sensors. This paper proposes and describes such a method with results demonstrating its validity.},
author = {Pereira, Marcelo and Silva, David and Santos, Vitor and Dias, Paulo},
doi = {10.1016/j.robot.2016.05.010},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pereira et al/Self calibration of multiple LIDARs and cameras on autonomous vehicles. Pereira et al.. 2016.pdf:pdf},
issn = {09218890},
journal = {Rob. Auton. Syst.},
keywords = {3D data fitting,Extrinsic calibration,Point cloud},
pages = {326--337},
title = {{Self calibration of multiple LIDARs and cameras on autonomous vehicles}},
volume = {83},
year = {2016}
}
@article{He2015,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224$\backslash$times 224) input image. This requirement is 'artificial' and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, 'spatial pyramid pooling', to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-The-Art classification results using a single full-image representation and no fine-Tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 $\backslash$times faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank {\#}2 in object detection and {\#}3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
archivePrefix = {arXiv},
arxivId = {1406.4729},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/TPAMI.2015.2389824},
eprint = {1406.4729},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/He et al/Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. He et al.. 2015.pdf:pdf},
isbn = {9783319105772},
issn = {01628828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
keywords = {Convolutional Neural Networks,Image Classification,Object Detection,Spatial Pyramid Pooling},
number = {9},
pages = {1904--1916},
pmid = {26353135},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
volume = {37},
year = {2015}
}
@article{ViolaP2004,
author = {{Viola P.} and {Jones M.}},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Viola P., Jones M/Rapid Object Detection Using a Boosted Cascade of Simple Features.pdf (applicationpdf オブジェクト). Viola P., Jones M.. 2004.pdf:pdf},
title = {{Rapid Object Detection Using a Boosted Cascade of Simple Features}},
url = {file:///C:/Documents and Settings/owner/デスクトップ/Rapid Object Detection Using a Boosted.pdf},
year = {2004}
}
@inproceedings{ADAS1,
author = {Okuda, R and Kajiwara, Y and Terashima, K},
booktitle = {Tech. Pap. 2014 Int. Symp. VLSI Des. Autom. Test},
title = {{A survey of technical trend of ADAS and autonomous driving}}
}
@article{Bimbraw2015,
author = {Bimbraw, K},
doi = {10.5220/0005540501910198},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Bimbraw/Autonomous Cars Past , Present and Future - A Review of the Developments in the Last Century , the Present .... Bimbraw. 2015.pdf:pdf},
isbn = {9789897581229},
journal = {12th Int. Conf. Informatics Control. Autom. Robot.},
keywords = {accomplished in this,and much has been,automation,automation is of interest,autonomous cars,autonomous vehicles,cars,intelligent transportation,mechatronics systems,technologies and systems,the field of autonomous,to researchers},
number = {August},
pages = {191--198},
title = {{Autonomous Cars : Past , Present and Future - A Review of the Developments in the Last Century , the Present ...}},
year = {2015}
}
@article{Conroy2009,
abstract = {Range imaging cameras measure depth simultaneously for every pixel in a given field of view. In most implementations the basic operating principles are the same. A scene is illuminated with an intensity modulated light source and the reflected signal is sampled using a gain-modulated imager. Previously we presented a unique heterodyne range imaging system that employed a bulky and power hungry image intensifier as the high speed gain-modulation mechanism. In this paper we present a new range imager using an internally modulated image sensor that is designed to operate in heterodyne mode, but can also operate in homodyne mode. We discuss homodyne and heterodyne range imaging, and the merits of the various types of hardware used to implement these systems. Following this we describe in detail the hardware and firmware components of our new ranger. We experimentally compare the two operating modes and demonstrate that heterodyne operation is less sensitive to some of the limitations suffered in homodyne mode, resulting in better linearity and ranging precision characteristics. We conclude by showing various qualitative examples that demonstrate the system's three-dimensional measurement performance. {\textcopyright} 2009 SPIE-IS {\&} T.},
author = {Conroy, Richard M. and Dorrington, Adrian A. and K{\"{u}}nnemeyer, Rainer and Cree, Michael J.},
doi = {10.1117/12.806139},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Conroy et al/Range imager performance comparison in homodyne and heterodyne operating modes. Conroy et al.. 2009.pdf:pdf},
issn = {0277786X},
journal = {Three-Dimensional Imaging Metrol.},
keywords = {camera,gain modulation,heterodyne,homodyne,imaging,pmd,range,three-dimensional},
pages = {723905},
title = {{Range imager performance comparison in homodyne and heterodyne operating modes}},
volume = {7239},
year = {2009}
}
@article{Larusso2013a,
abstract = {Abstract: "A common need in machine vision is to compute the 3-D rigid transformation that exists between two sets of points in a coordinate system for which corresponding pairs have been determined. Several solutions to this problem have been devised. In this paper a comparative analysis of four popular algorithms is given. Each of them computes the translation and rotation of the transform in closed-form, as the solution to a least squares formulation of the problem. They differ in terms of the representation of the transform and the method of solution, using respectively: singular value decomposition of a matrix, orthonormal matrices, unit quaternions and dual quaternions. This comparison presents results of several experiments designed to determine (1) the accuracy of each algorithm in the presence of different levels of noise, (2) the stability of each technique with respect to degenerate data sets, and (3) the relative computation time of each approach for different sizes and forms of data."},
author = {Larusso, A and Eggert, DW and Fisher, RB},
doi = {10.5244/c.9.24},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Larusso, Eggert, Fisher/A Comparison of Four Algorithms for Estimating 3-D Rigid Transformations.. Larusso, Eggert, Fisher. 2013(2).pdf:pdf},
pages = {24.1--24.10},
title = {{A Comparison of Four Algorithms for Estimating 3-D Rigid Transformations.}},
year = {2013}
}
@article{Fersch2017,
abstract = {{\textcopyright} 2017 IEEE. We present the application of optical code division multiple access (OCDMA) modulation based on optical orthogonal codes for automotive time-of-flight light detection and ranging (LiDAR) system sensors using avalanche photodiode (APD) detectors. The modulation opens the possibility to discriminate single laser transmissions. This allows the realization of additional features like enhancing the systems' interference robustness or accelerating their scanning rate. The requirements on automotive LiDAR OCDMA modulation differs from telecommunication's demands in several ways: The sensor must guarantee absolute laser safety; front facing devices must be able to provide reliable long range results up to a distance of 150m and beyond; and the signal is transmitted through free space. After outlining the basic functionality of the sort of sensors in consideration, the properties of the optical orthogonal codes (OOC) modulation are compared with the state of the art theoretically. The influence of OOC parameters with respect to scanning LiDAR systems is examined. The modulation technique is then demonstrated experimentally with two examples: The detection and separation of coded and traditional signals is shown using a matched filter detection algorithm. In the same way, differently coded signals can be separated from each other. Impediments of the interference suppression quality are discussed. Finally, an overview of possible applications of the proposed technique in automotive LiDAR systems is given, which are enabled by the OCDMA modulation in the first place.},
author = {Fersch, Thomas and Weigel, Robert and Koelpin, Alexander},
doi = {10.1109/JSEN.2017.2688126},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems. Fersch, Weigel, Koelpin.pdf:pdf},
issn = {1530-437X},
journal = {IEEE Sens. J.},
keywords = {Laser radar,LiDAR,OCDMA modulation,time-of-flight},
month = {jun},
number = {11},
pages = {3507--3516},
title = {{A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems}},
url = {http://ieeexplore.ieee.org/document/7887680/},
volume = {17},
year = {2017}
}
@article{Unnikrishnan2005,
abstract = {External calibration of a camera to a laser rangefinder is a common pre-requisite on todays multi-sensor mobile robot platforms. However, the process of doing so is relatively poorly documented and almost always time-consuming. This document outlines an easy and portable technique for external calibration of a camera to a laser rangefinder. It describes the usage of the Laser-Camera Calibration Toolbox (LCCT), aMatlab R -based graphical user interface that is meant to accompany this document and facilitates the calibration procedure. We also summarize the math behind its development. The software is accessible online at as well as at the VMR Lab Software page at software.html .},
author = {Unnikrishnan, Ranjith and Hebert, Martial},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Unnikrishnan, Hebert/Fast Extrinsic Calibration of a Laser Rangefinder to a Camera. Unnikrishnan, Hebert. 2005.pdf:pdf},
isbn = {CMU-RI-TR-05-09},
journal = {Robotics},
number = {July 2005},
pages = {23},
title = {{Fast Extrinsic Calibration of a Laser Rangefinder to a Camera}},
url = {http://www.ri.cmu.edu/pub{\_}files/pub4/unnikrishnan{\_}ranjith{\_}2005{\_}3/unnikrishnan{\_}ranjith{\_}2005{\_}3.pdf https://www.cs.cmu.edu/{~}ranjith/lcct.html},
volume = {2005},
year = {2005}
}
@article{Chum2005,
abstract = {The problem of model parameters estimation from data with a presence of outlier measurements often arises in computer vision and methods of robust estimation have to be used. The RANSAC algorithm introduced by Fishler and Bolles in 1981 is the a widely used robust estimator in the field of computer vision. The algorithm is capable of providing good estimates from data contaminated by large (even significantly more than 50{\%}) fraction of outliers. RANSAC is an optimization method that uses a data-driven random sampling of the parameter space to find the extremum of the cost function. Samples of data define points of the parameter space in which the cost function is evaluated and model parameters with the best score are output. This thesis provides a detailed analysis of RANSAC, which is recast as time-constrained op- timization – a solution that is optimal with certain confidence is sought in the shortest possible time. Next, the concept of randomized cost function evaluation in RANSAC is introduced and its superiority over the deterministic evaluation is shown. A provably optimal strategy for the ran- domized cost function evaluation is derived. A known discrepancy, caused by noise on inliers, between theoretical prediction of the time required to find the solution and practically observed running times is traced to a tacit assump- tions of RANSAC. The proposed LO-RANSAC algorithm reaches almost perfect agreement with theoretical predictions without any negative impact on the time complexity. A unified method of estimation of model and its degenerate configuration (epipolar geome- try and homography of a dominant plane) at the same time without a priori knowledge of the presence of the degenerate configuration (dominant plane) is derived. Next, it is shown that using oriented geometric constraints that arise from a realistic model of physical camera devices, saves non-negligible fraction of computational time. No negative side effect are related to the application of the oriented constraints. An algorithm exploiting (possibly noisy) match quality to modify the sampling strategy is introduced. The quality of a match is an often freely available quantity in the matching prob- lem. The approach increases the efficiency of the algorithm while keeping the same robustness as RANSAC in the worst-case situation (when the match quality is unrelated to whether a corre- spondence is a mismatch or not). Most of the algorithms in the thesis are motivated by (and presented on) estimation of a multi- view geometry. The algorithms are, however, general robust estimation techniques and can be easily used in other application areas too.},
author = {Chum, Ondrej},
doi = {10.1016/S0921-8777(99)00013-0},
isbn = {3015947974},
issn = {1213-2365},
journal = {Phd Thesis},
pmid = {10422537},
title = {{Two-View Geometry Estimation by Random Sample and Consensus}},
url = {http://cmp.felk.cvut.cz/{~}chum/papers/Chum-PhD.pdf},
year = {2005}
}
@inproceedings{Fersch2017a,
abstract = {This paper discusses the current technical limitations posed on endeavors to miniaturize lidar systems for use in automotive applications and how to possibly extend those limits. The focus is set on long-range scanning direct time of flight LiDAR systems using APD photodetectors. Miniaturization evokes severe problems in ensuring absolute laser safety while maintaining the systems' performance in terms of maximum range, signal-to-noise ratio, detection probability, pixel density, or frame rate. Based on hypothetical but realistic specifications for an exemplary system the complete lidar signal path is calculated. The maximum range of the system is used as a general performance indicator. It is determined with the minimum signal-to-noise ratio required to detect an object. Various system parameters are varied to find their impact on the system's range. The reduction of the laser's pulse width and the right choice for the transimpedance amplifier's amplification have shown to be practicable measures to double the system's range.},
author = {Fersch, Thomas and Weigel, Robert and Koelpin, Alexander},
booktitle = {Three-Dimensional Imaging, Vis. Disp. 2017},
doi = {10.1117/12.2260894},
editor = {Javidi, Bahram and Son, Jung-Young and Matoba, Osamu},
keywords = {automotive,avalanche photo diode,ladar,laser radar,lidar,time-of-flight},
organization = {International Society for Optics and Photonics},
pages = {160--171},
publisher = {SPIE},
title = {{Challenges in miniaturized automotive long-range lidar system design}},
url = {https://doi.org/10.1117/12.2260894},
volume = {10219},
year = {2017}
}
@article{Park2019,
abstract = {We present a deep convolutional neural network (CNN) architecture for high-precision depth estimation by jointly utilizing sparse 3D LiDAR and dense stereo depth information. In this network, the complementary characteristics of sparse 3D LiDAR and dense stereo depth are simultaneously encoded in a boosting manner. Tailored to the LiDAR and stereo fusion problem, the proposed network differs from previous CNNs in the incorporation of a compact convolution module, which can be deployed with the constraints of mobile devices. As training data for the LiDAR and stereo fusion is rather limited, we introduce a simple yet effective approach for reproducing the raw KITTI dataset. The raw LIDAR scans are augmented by adapting an off-the-shelf stereo algorithm and a confidence measure. We evaluate the proposed network on the KITTI benchmark and data collected by our multi-sensor acquisition system. Experiments demonstrate that the proposed network generalizes across datasets and is significantly more accurate than various baseline approaches.},
author = {Park, Kihong and Kim, Seungryong and Sohn, Kwanghoon},
doi = {10.1109/tits.2019.2891788},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Park, Kim, Sohn/High-Precision Depth Estimation Using Uncalibrated LiDAR and Stereo Fusion. Park, Kim, Sohn. 2019.pdf:pdf},
issn = {1524-9050},
journal = {IEEE Trans. Intell. Transp. Syst.},
pages = {1--15},
publisher = {IEEE},
title = {{High-Precision Depth Estimation Using Uncalibrated LiDAR and Stereo Fusion}},
volume = {PP},
year = {2019}
}
@article{Yang2017,
abstract = {Combining active and passive imaging sensors enables creating a more detailed 3D model of the real world. Then, these 3D data can be used for various applications, such as city mapping, indoor navigation, autonomous vehicles, etc. Typically, LiDAR and camera as imaging sensors are installed on these systems. Both of these sensors have advantages and drawbacks. Thus, LiDAR sensor directly provides relatively accurate 3D point cloud, but LiDAR point cloud barely contains the surface textures and details, such as traffic signs and alpha numeric information on facades. As opposed to LiDAR, deriving 3D point cloud from images require more computational resources, and in many cases, the accuracy and point density might be lower due to poor visual or light conditions. This paper investigates a workflow which utilizes factor graph SLAM, dense 3D reconstruction and ICP to efficiently generate the LiDAR and camera point clouds, and then, co-register in a navigation frame to provide a consistent and more detailed reconstruction of the environment. The workflow consists of three processing steps. First, we use factor graph SLAM, GPS/INS odometry and 6DOF scan matching to register the LiDAR point cloud. Then, the stereo images are processed by stereo-scan dense 3D reconstruction technique to generate dense point cloud. Finally, ICP method is used to co-register LiDAR and photogrammetric point clouds into one frame. The proposed method is tested with the KITTI dataset. The results show that data fusion of two point clouds can improve the quality of the 3D model.},
author = {Yang, Yuan and Koppanyi, Zoltan and Toth, Charles K},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yang, Koppanyi, Toth/Stereo Image Point Cloud and Lidar Point Cloud Fusion for the 3D Street Mapping. Yang, Koppanyi, Toth. 2017.pdf:pdf},
journal = {IGTF 2017 – Imaging Geospatial Technol. Forum 2017-ASPRS Annu. Conf.},
keywords = {ICP,LiDAR point cloud,data fusion,factor graph,stereo image 3D reconstruction},
title = {{Stereo Image Point Cloud and Lidar Point Cloud Fusion for the 3D Street Mapping}},
url = {https://pdfs.semanticscholar.org/7c2e/a37a24b6a09266dd75936b5e4d53953d1179.pdf},
year = {2017}
}
@misc{CameronOliver2017a,
abstract = {Why did LIDAR take off with self-driving cars? In a word: mapping. LIDAR allows you to generate huge 3D maps (its original application!), which you can then navigate the car or robot predictably within. By using a LIDAR to map and navigate an environment, you can know ahead of time the bounds of a lane, or that there is a stop sign or traffic light 500m ahead. This kind of predictability is exactly what a technology like self-driving cars requires, and has been a big reason for the progress over the last 5 years.
},
author = {{Cameron Oliver}},
booktitle = {Voyage},
keywords = {LIDAR,Self-Driving Cars},
mendeley-tags = {LIDAR,Self-Driving Cars},
title = {{An Introduction to LIDAR: The Key Self-Driving Car Sensor}},
url = {https://news.voyage.auto/an-introduction-to-lidar-the-key-self-driving-car-sensor-a7e405590cff},
urldate = {2018-10-09},
year = {2017}
}
@article{Guindel2018a,
abstract = {Sensor setups consisting of a combination of 3D range scanner lasers and stereo vision systems are becoming a popular choice for on-board perception systems in vehicles; however, the combined use of both sources of information implies a tedious calibration process. We present a method for extrinsic calibration of lidar-stereo camera pairs without user intervention. Our calibration approach is aimed to cope with the constraints commonly found in automotive setups, such as low-resolution and specific sensor poses. To demonstrate the performance of our method, we also introduce a novel approach for the quantitative assessment of the calibration results, based on a simulation environment. Tests using real devices have been conducted as well, proving the usability of the system and the improvement over the existing approaches. Code is available at http://wiki.ros.org/velo2cam{\_}calibration},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.04085v3},
author = {Guindel, Carlos and Beltran, Jorge and Martin, David and Garcia, Fernando},
doi = {10.1109/ITSC.2017.8317829},
eprint = {arXiv:1705.04085v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Guindel et al/Automatic extrinsic calibration for lidar-stereo vehicle sensor setups. Guindel et al.. 2018(2).pdf:pdf},
isbn = {9781538615256},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
number = {October},
pages = {1--6},
title = {{Automatic extrinsic calibration for lidar-stereo vehicle sensor setups}},
volume = {2018-March},
year = {2018}
}
@misc{bunny,
title = {{pcl/bunny.pcd at master {\textperiodcentered} PointCloudLibrary/pcl}},
url = {https://github.com/PointCloudLibrary/pcl/blob/master/test/bunny.pcd},
urldate = {2019-10-22}
}
@article{Messom2006,
abstract = {This paper introduces an extended set of Haarlike features beyond the standard vertically and horizontally aligned Haar-like features [Viola and Jones, 2001a; 2001b] and the 45o twisted Haar-like features [Lienhart and Maydt, 2002; Lienhart et al., 2003a; 2003b]. The extended rotated Haar-like features are based on the standard Haar-like features that have been rotated based on whole integer pixel based rotations. These rotated feature values can also be calculated using rotated integral images which means that they can be fast and efficiently calculated with just 8 operations irrespective of the feature size. In general each feature requires another 8 operations based on an identity integral image so that appropriate scaling corrections can be applied. These scaling corrections are needed due to the rounding errors associated with scaling the features. The errors introduced by these rotated features on natural images are small enough to allow rotated classifiers to be implemented using a classifier trained on only vertically aligned images. This is a significant improvement in training time for a classifier that is invariant to the rotations represented in the parallel classifier.},
author = {Messom, Chris and Barczak, Andre},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Messom, Barczak/Fast and efficient rotated haar-like features using rotated integral images. Messom, Barczak. 2006.pdf:pdf},
isbn = {9780958758383},
journal = {Proc. 2006 Australas. Conf. Robot. Autom. ACRA 2006},
pages = {4--9},
title = {{Fast and efficient rotated haar-like features using rotated integral images}},
year = {2006}
}
@article{Christopher2015,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0403007},
author = {Christopher, Inventors and Ramsey, Paul and Chant, Garry Richard and Lockley, Andrew Robert and Gb, Wantage and Fields, Brian and Watson, Martin John and Rachel, Eleanor and Hyde, Ann and Gb, Wantage and Jasper, A},
doi = {10.1016/j.(73)},
eprint = {0403007},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Christopher et al/( 12 ) United States Patent ( 10 ) Patent No .. Christopher et al.. 2015.pdf:pdf},
isbn = {2010512510},
issn = {2470-0010},
number = {12},
pmid = {1000182772},
primaryClass = {arXiv:physics},
title = {{( 12 ) United States Patent ( 10 ) Patent No .:}},
volume = {2},
year = {2015}
}
@article{Bay2008,
abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1016/j.cviu.2007.09.014},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Bay et al/Speeded-Up Robust Features (SURF). Bay et al.. 2008.pdf:pdf},
issn = {10773142},
journal = {Comput. Vis. Image Underst.},
keywords = {Camera calibration,Feature description,Interest points,Local features,Object recognition},
number = {3},
pages = {346--359},
title = {{Speeded-Up Robust Features (SURF)}},
volume = {110},
year = {2008}
}
@article{Ekstrom2015,
abstract = {Place cells are a fundamental component of the rodent navigational system. One intriguing implication of place cells is that humans, by extension, have "map-like" (or GPS-like) knowledge that we use to represent space. Here, we review both behavioral and neural studies of human navigation, suggesting that how we process visual information forms a critical component of how we represent space. These include cellular and brain systems devoted to coding visual information during navigation in addition to a location coding system similar to that described in rodents. Together, these findings suggest that while it is highly useful to think of our navigation system involving internal "maps," we should not neglect the importance of high-resolution visual representations to how we navigate space.},
author = {Ekstrom, Arne D.},
doi = {10.1002/hipo.22449},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ekstrom/Why vision is important to how we navigate. Ekstrom. 2015.pdf:pdf},
issn = {10981063},
journal = {Hippocampus},
keywords = {Allocentric,Cognitive map,Egocentric,Hippocampus,Humans,Path integration,Spatial navigation},
number = {6},
pages = {731--735},
title = {{Why vision is important to how we navigate}},
volume = {25},
year = {2015}
}
@article{MartinVelas2013,
abstract = {Quinolone-resistant clinical Escherichia coli isolates were examined for mutations in the marRAB operon of the multiple antibiotic resistance (mar) locus. Among 23 strains evaluated, 8 were chosen for further study: 3 that showed relatively high levels of uninduced, i.e., constitutive, expression of the operon and 5 with variable responses to induction by salicylate or tetracyclines. The marR genes, specifying the repressor of the operon, cloned from the three strains constitutively expressing the operon did not reduce the level of expression of beta-galactosidase from a marO::lacZ transcriptional fusion and were therefore mutant; however, marR genes cloned from the five other clinical strains repressed LacZ expression and were wild type. All three mutant marR genes contained more than one mutation: a deletion and a point mutation. Inactivation of the mar locus in the three known marR mutant strains with a kanamycin resistance cassette introduced by homologous recombination reduced resistance to quinolones and multiple antibiotics. These findings indicate that mar operon mutations exist in quinolone-resistant clinical E. coli isolates and contribute to quinolone and multidrug resistance.},
archivePrefix = {arXiv},
arxivId = {1505.00729},
author = {Velas, Martin and Spanel, Michal and Materna, Zdenek and Herout, Adam},
doi = {10.1016/B978-0-08-098242-7.00008-0},
eprint = {1505.00729},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Martin Velas, Michal Spanel, Zdenek Materna/Calibration of RGB Camera With Velodyne LiDAR. Martin Velas, Michal Spanel, Zdenek Materna. 2013.pdf:pdf},
isbn = {9781461434801},
issn = {0066-4804},
journal = {Flight Dyn. Princ.},
keywords = {L{\'{i}}DAR,Velodyne,calibration,camera,marker},
month = {jul},
number = {7},
pages = {219--241},
pmid = {8807064},
publisher = {Elsevier},
title = {{Calibration of RGB Camera With Velodyne LiDAR}},
url = {https://linkinghub.elsevier.com/retrieve/pii/B9780080982427000080},
volume = {40},
year = {2013}
}
@misc{TomasKrejci,
author = {{Tomas Krejci}},
title = {{GitHub - tomas789/kitti2bag: Convert KITTI dataset to ROS bag file the easy way!}},
url = {https://github.com/tomas789/kitti2bag}
}
@techreport{Redmon2018,
author = {Redmon, Joseph and Farhadi, Ali},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Redmon, Farhadi/YOLOv3 An Incremental Improvement. Redmon, Farhadi. 2018.pdf:pdf},
pages = {1--6},
title = {{YOLO v.3}},
url = {https://pjreddie.com/media/files/papers/YOLOv3.pdf},
year = {2018}
}
@article{camera_models,
author = {Hata, Kenji and Savarese, Silvio},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hata, Savarese/CS231A Course Notes 1 Camera Models. Hata, Savarese. Unknown.pdf:pdf},
pages = {1--16},
title = {{CS231A Course Notes 1: Camera Models}}
}
@misc{Ford,
author = {Ford},
title = {{Ford Motor Company Timeline | Ford.com}},
url = {https://corporate.ford.com/history.html},
urldate = {2019-10-20}
}
@book{Xu1996,
author = {Xú, Gang. and Zhang, Zhengyou},
isbn = {0792341996},
pages = {313},
publisher = {Kluwer Academic Publishers},
title = {{Epipolar geometry in stereo, motion, and object recognition : a unified approach}},
url = {https://books.google.com/books?id=DnFaUidM-B0C{\&}pg=PA7{\&}dq=pinhole+intitle:{\%}22Epipolar+geometry{\%}22},
year = {1996}
}
@article{Huang2010a,
abstract = {We describe the Lightweight Communications and Marshalling (LCM) library for message passing and data marshalling. The primary goal of LCM is to simplify the development of low-latency message passing systems, especially for real-time robotics research applications. Messages can be transmitted between different processes using LCM's publish/subscribe message-passing system. A platform- and language-independent type specification language separates message description from implementation. Message specifications are automatically compiled into language-specific bindings, eliminating the need for users to implement marshalling code while guaranteeing run-time type safety. LCM is notable in providing a real-time deep traffic inspection tool that can decode and display message traffic with minimal user effort and no impact on overall system performance. This and other features emphasize LCM's focus on simplifying both the development and debugging of message passing systems. In this paper, we explain the design of LCM, evaluate its performance, and describe its application to a number of autonomous land, underwater, and aerial robots. {\textcopyright}2010 IEEE.},
author = {Huang, Albert S. and Olson, Edwin and Moore, David C.},
doi = {10.1109/IROS.2010.5649358},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Huang, Olson, Moore/LCM Lightweight Communications and Marshalling. Huang, Olson, Moore. 2010.pdf:pdf},
isbn = {9781424466757},
journal = {IEEE/RSJ 2010 Int. Conf. Intell. Robot. Syst. IROS 2010 - Conf. Proc.},
number = {Lcm},
pages = {4057--4062},
title = {{LCM: Lightweight Communications and Marshalling}},
year = {2010}
}
@misc{Cameron,
abstract = {Retrofit a Car for self-driving using LIDAR, camera and Radar},
author = {Cameron, Oliver and Voyage},
keywords = {LIDAR,Self-Driving Cars,Sensor Fusion},
mendeley-tags = {LIDAR,Self-Driving Cars,Sensor Fusion},
title = {{The Story of Homer: Voyage's First Self-Driving Taxi}},
url = {https://news.voyage.auto/the-story-of-homer-voyages-first-self-driving-taxi-f0a6466718af},
urldate = {2018-10-09}
}
@misc{Britannica,
author = {Britannica},
title = {{Model T | automobile | Britannica.com}},
url = {https://www.britannica.com/technology/Model-T},
urldate = {2019-10-20}
}
@article{Pandey2012,
abstract = {This paper reports on a mutual information (MI) based algo- rithm for automatic extrinsic calibration of a 3D laser scan- ner and optical camera system. By using MI as the regis- tration criterion, our method is able to work in situ without the need for any specific calibration targets, which makes it practical for in-field calibration. The calibration parameters are estimated by maximizing the mutual information obtained between the sensor-measured surface intensities. We calcu- late the Cramer-Rao-Lower-Bound (CRLB) and show that the sample variance of the estimated parameters empirically ap- proaches the CRLB for a sufficient number of views. Fur- thermore, we compare the calibration results to independent ground-truth and observe that the mean error also empirically approaches to zero as the number of views are increased. This indicates that the proposed algorithm, in the limiting case, calculates a minimum variance unbiased (MVUB) estimate of the calibration parameters. Experimental results are pre- sented for data collected by a vehicle mounted with a 3D laser scanner and an omnidirectional camera system.},
author = {Pandey, Gaurav and McBride, James R and Savarese, Silvio and Eustice, Ryan M},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pandey et al/Automatic targetless extrinsic calibration of a 3d lidar and camera by maximizing mutual information. Pandey et al.. 2012.pdf:pdf},
journal = {Proc. {\{}AAAI{\}} Natl. Conf. Artif. Intell.},
keywords = {calibration,ladybug,omnidirectional,velodyne},
pages = {2053--2059},
title = {{Automatic targetless extrinsic calibration of a 3d lidar and camera by maximizing mutual information}},
year = {2012}
}
@misc{boofcv,
title = {{BoofCV}},
url = {https://boofcv.org/index.php?title=Main{\_}Page},
urldate = {2019-10-22}
}
@article{Zhang2000,
abstract = {We propose a flexible new technique to easily calibrate a camera. It is well suited for use without specialized knowledge of 3D geometry or computer vision. The technique only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique, and very good results have been obtained. Compared with classical techniques which use expensive equipments such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one step from laboratory environments to real world use.},
author = {Zhang, Zhengyou},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Zhengyou/A Flexible New Technique for Camera Calibration. Zhengyou. 2000.pdf:pdf},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
number = {11},
pages = {1330--1334},
title = {{A Flexible New Technique for Camera Calibration}},
url = {https://www.microsoft.com/en-us/research/publication/a-flexible-new-technique-for-camera-calibration/},
volume = {22},
year = {2000}
}
@article{manuapphotogrammetry,
author = {Jones, Alan D},
doi = {10.1080/00690805.1982.10438226},
journal = {Cartography},
number = {4},
pages = {258},
publisher = {Taylor {\&} Francis},
title = {{Manual of Photogrammetry, eds C.C. Slama, C. Theurer and S.W. Hendrikson, American Society of Photogrammetry, Falls Church, Va., 1980, Fourth Edition, 180 × 260mm, xvi and 1056 pages (with index), 72 tables, 866 figures. ISBN 0 937294 01 2.}},
url = {https://doi.org/10.1080/00690805.1982.10438226},
volume = {12},
year = {1982}
}
@book{mvg_book,
author = {Hartley, Richard and Zisserman, Andrew},
edition = {2},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hartley, Zisserman/Multiple View Geometry in computer vision. Hartley, Zisserman. 2003.pdf:pdf},
isbn = {9780521540513},
publisher = {Cambridge University Press New York, NY, USA},
title = {{Multiple View Geometry in computer vision}},
year = {2003}
}
@inproceedings{Heikkila1997,
abstract = {In geometrical camera calibration the objective is to determine a set of camera parameters that describe the mapping between 3-D reference coordinates and 2-D image coordinates. Various methods for camera calibration can be found from the literature. However surprisingly little attention has been paid to the whole calibration procedure, i.e., control point extraction from images, model fitting, image correction, and errors originating in these stages. The main interest has been in model fitting, although the other stages are also important. In this paper we present a four-step calibration procedure that is an extension to the two-step method. There is an additional step to compensate for distortion caused by circular features, and a step for correcting the distorted image coordinates. The image correction is performed with an empirical inverse model that accurately compensates for radial and tangential distortions. Finally, a linear method for solving the parameters of the inverse model is presented.},
author = {Heikkila, J and Silven, O},
booktitle = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
doi = {10.1109/CVPR.1997.609468},
keywords = {Calibration,Cameras,Closed-form solution,Error correction,Geometrical optics,Inverse problems,Machine vision,Mathematical model,Minimization methods,Nonlinear distortion,calibration,camera calibration,computer vision,geometrical camera calibration,image correction,implicit image correction,inverse model,model fitting,three-dimensional machine vision},
month = {jun},
pages = {1106--1112},
title = {{A four-step camera calibration procedure with implicit image correction}},
year = {1997}
}
@article{Morrison1960,
author = {Morrison, David D.},
journal = {Proc. Jet Propuls. Lab. Semin. Track. Programs Orbit Determ.},
pages = {1--9},
title = {{Methods for nonlinear least squares problems and convergence proofs}},
year = {1960}
}
@article{Simon2019,
abstract = {Accurate detection of 3D objects is a fundamental problem in computer vision and has an enormous impact on autonomous cars, augmented/virtual reality and many applications in robotics. In this work we present a novel fusion of neural network based state-of-the-art 3D detector and visual semantic segmentation in the context of autonomous driving. Additionally, we introduce Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable evaluation metric for comparison of object detections, which speeds up our inference time up to 20$\backslash${\%} and halves training time. On top, we apply state-of-the-art online multi target feature tracking on the object measurements to further increase accuracy and robustness utilizing temporal information. Our experiments on KITTI show that we achieve same results as state-of-the-art in all related categories, while maintaining the performance and accuracy trade-off and still run in real-time. Furthermore, our model is the first one that fuses visual semantic with 3D object detection.},
archivePrefix = {arXiv},
arxivId = {1904.07537},
author = {Simon, Martin and Amende, Karl and Kraus, Andrea and Honer, Jens and S{\"{a}}mann, Timo and Kaulbersch, Hauke and Milz, Stefan and Gross, Horst Michael},
eprint = {1904.07537},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Simon et al/Complexer-YOLO Real-Time 3D Object Detection and Tracking on Semantic Point Clouds. Simon et al.. 2019.pdf:pdf},
title = {{Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds}},
url = {http://arxiv.org/abs/1904.07537},
year = {2019}
}
@article{Mirzaei2012,
abstract = {In this paper we address the problem of estimating the intrinsic parameters of a 3D LIDAR while at the same time computing its extrinsic calibration with respect to a rigidly connected camera. Existing approaches to solve this nonlinear estimation problem are based on iterative minimization of nonlinear cost functions. In such cases, the accuracy of the resulting solution hinges on the availability of a precise initial estimate, which is often not available. In order to address this issue, we divide the problem into two least-squares sub-problems, and analytically solve each one to determine a precise initial estimate for the unknown parameters. We further increase the accuracy of these initial estimates by iteratively minimizing a batch nonlinear least-squares cost function. In addition, we provide the minimal identifiability conditions, under which it is possible to accurately estimate the unknown parameters. Experimental results consisting of photorealistic 3D reconstruction of indoor and outdoor scenes, as well as standard metrics of the calibration errors, are used to assess the validity of our approach.},
author = {Mirzaei, Faraz M. and Kottas, Dimitrios G. and Roumeliotis, Stergios I.},
doi = {10.1177/0278364911435689},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Mirzaei, Kottas, Roumeliotis/3D LIDAR-camera intrinsic and extrinsic calibration Identifiability and analytical least-squares-based initialization. Mirzaei, Kottas,.pdf:pdf},
issn = {02783649},
journal = {Int. J. Rob. Res.},
keywords = {Sensing and perception,calibration and identification,computer vision,range sensing},
number = {4},
pages = {452--467},
title = {{3D LIDAR-camera intrinsic and extrinsic calibration: Identifiability and analytical least-squares-based initialization}},
volume = {31},
year = {2012}
}
@misc{TetraVue,
author = {TetraVue},
title = {{TetraVue | Home}},
url = {https://www.tetravue.com/},
urldate = {2019-11-27}
}
@book{Merklinger1993,
author = {Merklinger, Harold M.},
isbn = {0969502524},
pages = {104},
publisher = {H.M. Merklinger},
title = {{Focusing the view camera : a scientific way to focus the view camera and estimate depth of field}},
year = {1993}
}
@misc{Cameron,
abstract = {Retrofit a Car for self-driving using LIDAR, camera and Radar},
author = {Cameron, Oliver and Voyage},
keywords = {LIDAR,Self-Driving Cars,Sensor Fusion},
mendeley-tags = {LIDAR,Self-Driving Cars,Sensor Fusion},
title = {{The Story of Homer: Voyage's First Self-Driving Taxi}},
url = {https://news.voyage.auto/the-story-of-homer-voyages-first-self-driving-taxi-f0a6466718af},
urldate = {2018-10-09}
}
@article{Lourakis2005,
abstract = {In order to obtain optimal 3D structure and viewing parameter estimates, bundle adjustment is often used as the last step of feature-based structure and motion estimation algorithms. Bundle adjustment involves the formulation of a large scale, yet sparse minimization problem, which is traditionally solved using a sparse variant of the Levenberg-Marquardt optimization algorithm that avoids storing and operating on zero entries. This paper argues that considerable computational benefits can be gained by substituting the sparse Levenberg-Marquardt algorithm in the implementation of bundle adjustment with a sparse variant of Powell's dog leg non-linear least squares technique. Detailed comparative experimental results provide strong evidence supporting this claim. {\textcopyright} 2005 IEEE.},
author = {Lourakis, Manolis I.A. and Argyros, Antonis A.},
doi = {10.1109/ICCV.2005.128},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lourakis, Argyros/Is Levenberg-Marquardt the most efficient optimization algorithm for implementing bundle adjustment. Lourakis, Argyros. 2005.pdf:pdf},
isbn = {076952334X},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
pages = {1526--1531},
title = {{Is Levenberg-Marquardt the most efficient optimization algorithm for implementing bundle adjustment?}},
volume = {II},
year = {2005}
}
@article{Kim2017,
author = {Kim, Gunzung and Eom, Jeongsook and Choi, Jeonghee and Park, Yongwan},
doi = {10.14372/iemek.2017.12.1.43},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Mutual Interference on Mobile Pulsed Scanning LIDAR. Kim et al.. 2017.pdf:pdf},
issn = {1975-5066},
journal = {IEMEK J. Embed. Syst. Appl.},
month = {feb},
number = {1},
pages = {43--62},
title = {{Mutual Interference on Mobile Pulsed Scanning LIDAR}},
url = {http://koreascience.or.kr/journal/view.jsp?kj=OBDDBE{\&}py=2017{\&}vnc=v12n1{\&}sp=43},
volume = {12},
year = {2017}
}
@article{Thrun2006,
author = {Thrun, Sebastian and Montemerlo, Mike and Dahlkamp, Hendrik and Stavens, David and Aron, Andrei and Diebel, James and Fong, Philip and Gale, John and Halpenny, Morgan and Hoffmann, Gabriel and Lau, Kenny and Oakley, Celia and Palatucci, Mark and Pratt, Vaughan and Stang, Pascal and Strohband, Sven and Dupont, Cedric and Jendrossek, Lars-Erik and Koelen, Christian and Markey, Charles and Rummel, Carlo and van Niekerk, Joe and Jensen, Eric and Alessandrini, Philippe and Bradski, Gary and Davies, Bob and Ettinger, Scott and Kaehler, Adrian and Nefian, Ara and Mahoney, Pamela},
doi = {10.1002/rob.20147},
issn = {15564959},
journal = {J. F. Robot.},
month = {sep},
number = {9},
pages = {661--692},
title = {{Stanley: The robot that won the DARPA Grand Challenge}},
url = {http://doi.wiley.com/10.1002/rob.20147},
volume = {23},
year = {2006}
}
@article{Hata,
author = {Hata, Kenji and Savarese, Silvio},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hata, Savarese/CS231A Course Notes 1 Camera Models. Hata, Savarese. Unknown.pdf:pdf},
pages = {1--16},
title = {{CS231A Course Notes 1: Camera Models}}
}
@article{Liang2019,
abstract = {In this paper we propose to exploit multiple related tasks for accurate multi-sensor 3D object detection. Towards this goal we present an end-to-end learnable architecture that reasons about 2D and 3D object detection as well as ground estimation and depth completion. Our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels. Importantly, our approach leads the KITTI benchmark on 2D, 3D and bird's eye view object detection, while being real-time.},
author = {Liang, Ming and Yang, Bin and Chen, Yun and Hu, Rui and Urtasun, Raquel},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Liang et al/Multi-Task Multi-Sensor Fusion for 3D Object Detection. Liang et al.. 2019.pdf:pdf},
journal = {IEEE Conf. Comput. Vis. Pattern Recognition, Proceedings, CVPR},
keywords = {3d object detection,autonomous,multi-sensor fusion},
pages = {7345--7353},
title = {{Multi-Task Multi-Sensor Fusion for 3D Object Detection}},
url = {https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Multi-Task-Multi-Sensor-Fusion-for-3D-Object-Detection.pdf},
year = {2019}
}
@article{Wang2017,
abstract = {Optical phased array (OPA) is a key enabling element for solid state LIDAR (light detection and ranging). In this paper, we demonstrate a novel MEMS micromirror array OPA. Vertical combdrive actuators are integrated underneath the mirrors to achieve a small pitch (2.4$\mu$m) and a large field of view (22° at 905nm wavelength and 40° at 1550nm). The OPA has 2$\mu$s response time, and 10V actuation voltage.},
author = {Wang, Youmin and Wu, Ming C.},
doi = {10.1109/MEMSYS.2017.7863553},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wang, Wu/Micromirror based optical phased array for wide-angle beamsteering. Wang, Wu. 2017.pdf:pdf},
isbn = {9781509050789},
issn = {10846999},
journal = {Proc. IEEE Int. Conf. Micro Electro Mech. Syst.},
pages = {897--900},
publisher = {IEEE},
title = {{Micromirror based optical phased array for wide-angle beamsteering}},
year = {2017}
}
@misc{Eigenv3,
author = {Guennebaud, Ga$\backslash$"{\{}e{\}}l and Jacob, Beno$\backslash${\^{}}{\{}i{\}}t and Others},
title = {{Eigen v3}},
year = {2010}
}
@article{Gao2003,
abstract = {We use two approaches to solve the perspective-three-point (P3P) problem: the algebraic approach and the geometric approach. In the algebraic approach, we use Wu-Ritt's zero decomposition algorithm to give a complete triangular decomposition for the P3P equation system. This decomposition provides the first complete analytical solution to the P3P problem. We also give a complete solution classification for the P3P equation system, i.e., we give explicit criteria for the P3P problem to have one, two, three, and four solutions. Combining the analytical solutions with the criteria, we provide an algorithm, CASSC, which may be used to find complete and robust numerical solutions to the P3P problem. In the geometric approach, we give some pure geometric criteria for the number of real physical solutions.},
author = {Gao, Xiao Shan and Hou, Xiao Rong and Tang, Jianliang and Cheng, Hang Fei},
doi = {10.1109/TPAMI.2003.1217599},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gao et al/Complete solution classification for the perspective-three-point problem. Gao et al.. 2003.pdf:pdf},
issn = {01628828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
keywords = {Analytical solutions,Geometric criteria,Perspective-Three-Point problem,Pose determination,Solution classification,Wu-Ritt's zero decomposition method},
number = {8},
pages = {930--943},
title = {{Complete solution classification for the perspective-three-point problem}},
volume = {25},
year = {2003}
}
@article{KresimirKusevicOttawaCA;PaulMrstikOttawaCA;CraigLenGlennieSpring2017,
author = {{Kresimir Kusevic, Ottawa (CA); Paul Mrstik, Ottawa (CA); Craig Len Glennie, Spring}, TX (US)},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Method and System for Aligninga Line Scan Camerawith a Lidar Scanner for Real Time Data Fusion in Three Dimiensions. Kresimir Kusevic, O.pdf:pdf},
isbn = {2222222222},
number = {19},
title = {{Method and System for Aligninga Line Scan Camerawith a Lidar Scanner for Real Time Data Fusion in Three Dimiensions}},
volume = {1},
year = {2017}
}
@incollection{MartinVelasMichalSpanelZdenekMaterna2013,
abstract = {Quinolone-resistant clinical Escherichia coli isolates were examined for mutations in the marRAB operon of the multiple antibiotic resistance (mar) locus. Among 23 strains evaluated, 8 were chosen for further study: 3 that showed relatively high levels of uninduced, i.e., constitutive, expression of the operon and 5 with variable responses to induction by salicylate or tetracyclines. The marR genes, specifying the repressor of the operon, cloned from the three strains constitutively expressing the operon did not reduce the level of expression of beta-galactosidase from a marO::lacZ transcriptional fusion and were therefore mutant; however, marR genes cloned from the five other clinical strains repressed LacZ expression and were wild type. All three mutant marR genes contained more than one mutation: a deletion and a point mutation. Inactivation of the mar locus in the three known marR mutant strains with a kanamycin resistance cassette introduced by homologous recombination reduced resistance to quinolones and multiple antibiotics. These findings indicate that mar operon mutations exist in quinolone-resistant clinical E. coli isolates and contribute to quinolone and multidrug resistance.},
archivePrefix = {arXiv},
arxivId = {1505.00729},
author = {{Martin Velas, Michal Spanel, Zdenek Materna}, Adam Herout},
booktitle = {Flight Dyn. Princ.},
doi = {10.1016/B978-0-08-098242-7.00008-0},
eprint = {1505.00729},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Martin Velas, Michal Spanel, Zdenek Materna/Calibration of RGB Camera With Velodyne LiDAR. Martin Velas, Michal Spanel, Zdenek Materna. 2013.pdf:pdf},
isbn = {9781461434801},
issn = {0066-4804},
keywords = {L{\'{i}}DAR,Velodyne,calibration,camera,marker},
month = {jul},
number = {7},
pages = {219--241},
pmid = {8807064},
publisher = {Elsevier},
title = {{Calibration of RGB Camera With Velodyne LiDAR}},
url = {https://linkinghub.elsevier.com/retrieve/pii/B9780080982427000080},
volume = {40},
year = {2013}
}
@article{Ali2019,
abstract = {Object detection and classification in 3D is a key task in Automated Driving (AD). LiDAR sensors are employed to provide the 3D point cloud reconstruction of the surrounding environment, while the task of 3D object bounding box detection in real time remains a strong algorithmic challenge. In this paper, we build on the success of the one-shot regression meta-architecture in the 2D perspective image space and extend it to generate oriented 3D object bounding boxes from LiDAR point cloud. Our main contribution is in extending the loss function of YOLO v2 to include the yaw angle, the 3D box center in Cartesian coordinates and the height of the box as a direct regression problem. This formulation enables real-time performance, which is essential for automated driving. Our results are showing promising figures on KITTI benchmark, achieving real-time performance (40 fps) on Titan X GPU.},
archivePrefix = {arXiv},
arxivId = {arXiv:1808.02350v1},
author = {Ali, Waleed and Abdelkarim, Sherif and Zidan, Mahmoud and Zahran, Mohamed and Sallab, Ahmad El},
doi = {10.1007/978-3-030-11015-4_54},
eprint = {arXiv:1808.02350v1},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ali et al/YOLO3D End-to-end real-time 3D oriented object bounding box detection from LiDAR point cloud. Ali et al.. 2019.pdf:pdf},
isbn = {9783030110147},
issn = {16113349},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
keywords = {3D object detection,LiDAR,Real-time},
number = {August},
pages = {716--728},
title = {{YOLO3D: End-to-end real-time 3D oriented object bounding box detection from LiDAR point cloud}},
volume = {11131 LNCS},
year = {2019}
}
@article{Roca2006a,
archivePrefix = {arXiv},
arxivId = {arXiv:1208.5721},
author = {Roca, Sergi Figueras and Cited, References},
doi = {10.1038/incomms1464},
eprint = {arXiv:1208.5721},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Roca, Cited/( 12 ) United States Patent. Roca, Cited. 2006.pdf:pdf},
isbn = {1020080012051},
issn = {2004001828},
number = {12},
pmid = {1000182772},
title = {{( 12 ) United States Patent}},
volume = {2},
year = {2006}
}
@misc{Hall2011,
author = {Hall, David S.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Mccaffrey/( 12 ) United States Patent. Mccaffrey. 2019.pdf:pdf},
title = {{( 12 ) United States Patent}},
year = {2011}
}
@article{Shin2017,
abstract = {With the advancement in computing, sensing, and vehicle electronics, autonomous vehicles are being realized. For autonomous driving, environment perception sensors such as radars, lidars, and vision sensors play core roles as the eyes of a vehicle; therefore, their reliability cannot be compromised. In this work, we present a spoofing by relaying attack, which can not only induce illusions in the lidar output but can also cause the illusions to appear closer than the location of a spoofing device. In a recent work, the former attack is shown to be effective, but the latter one was never shown. Additionally, we present a novel saturation attack against lidars, which can completely incapacitate a lidar from sensing a certain direction. The effectiveness of both the approaches is experimentally verified against Velodyne's VLP-16.},
author = {Shin, Hocheol and Kim, Dohyun and Kwon, Yujin and Kim, Yongdae},
doi = {10.1007/978-3-319-66787-4_22},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Shin et al/Illusion and dazzle Adversarial optical channel exploits against lidars for automotive applications. Shin et al.. 2017.pdf:pdf},
isbn = {9783319667867},
issn = {16113349},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
keywords = {Attack,Autonomous car,Lidar,Saturating,Sensor,Spoofing},
pages = {445--467},
title = {{Illusion and dazzle: Adversarial optical channel exploits against lidars for automotive applications}},
volume = {10529 LNCS},
year = {2017}
}
@inproceedings{Yoneda2014,
abstract = {In recent years, automated vehicle researches move on to the next stage, that is auto-driving experiments on public roads. Major challenge is how to robustly drive at complicated situations such as narrow or non-featured road. In order to realize practical performance, some static information should be kept on memory such as road topology, building shape, white line, curb, traffic light and so on. Currently, some measurement companies have already begun to prepare map database for automated vehicles. They are able to provide highly-precise 3-D map for robust automated driving. This study focuses on what kind of data should be observed during automated driving with such precise database. In particular, we focus on the accurate localization based on the use of lidar data and precise 3-D map, and propose a feature quantity for scan data based on distribution of clusters. Localization experiment shows that our method can measure surrounding uncertainty and guarantee accurate localization.},
author = {Yoneda, Keisuke and Tehrani, Hossein and Ogawa, Takashi and Hukuyama, Naohisa and Mita, Seiichi},
booktitle = {IEEE Intell. Veh. Symp. Proc.},
doi = {10.1109/IVS.2014.6856596},
isbn = {9781479936380},
issn = {10901981},
pages = {1345--1350},
pmid = {12693519},
title = {{Lidar scan feature for localization with highly precise 3-D map}},
year = {2014}
}
@article{Geiger2012,
abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
doi = {10.1109/CVPR.2012.6248074},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger, Lenz, Urtasun/Are we ready for autonomous driving the KITTI vision benchmark suite. Geiger, Lenz, Urtasun. 2012.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {3354--3361},
title = {{Are we ready for autonomous driving? the KITTI vision benchmark suite}},
year = {2012}
}
@article{NGuessan2017,
abstract = {There exist many iterative methods for computing the maximum likelihood estimator but most of them suffer from one or several drawbacks such as the need to inverse a Hessian matrix and the need to find good initial approximations of the parameters that are unknown in practice. In this paper, we present an estimation method without matrix inversion based on a linear approximation of the likelihood equations in a neighborhood of the constrained maximum likelihood estimator. We obtain closed-form approximations of solutions and standard errors. Then, we propose an iterative algorithm which cycles through the components of the vector parameter and updates one component at a time. The initial solution, which is necessary to start the iterative procedure, is automated. The proposed algorithm is compared to some of the best iterative optimization algorithms available on R and MATLAB software through a simulation study and applied to the statistical analysis of a road safety measure.},
author = {N'Guessan, Assi and Geraldo, Issa Cherif and Hafidi, Bezza},
doi = {10.4236/ojs.2017.71011},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/N'Guessan, Geraldo, Hafidi/An Approximation Method for a Maximum Likelihood Equation System and Application to the Analysis of Accidents Data. N'Guessan, Geraldo.pdf:pdf},
issn = {2161-718X},
journal = {Open J. Stat.},
keywords = {Constrained Maximum Likelihood, Partial Linear App,constrained maximum likelihood,iterative algorithms,partial linear approximation,road safety measure,s complement,schur},
number = {01},
pages = {132--152},
title = {{An Approximation Method for a Maximum Likelihood Equation System and Application to the Analysis of Accidents Data}},
volume = {07},
year = {2017}
}
@article{Hecht2018,
author = {Hecht, Jeff},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hecht/Lidar for Self-Dr riving Cars. Hecht. 2018.pdf:pdf},
number = {January},
pages = {26--33},
title = {{Lidar for Self-Dr riving Cars}},
year = {2018}
}
@misc{Bouguet2010,
author = {Bouguet, Jean-Yves},
title = {{Camera Calibration Toolbox for Matlab}},
url = {http://www.vision.caltech.edu/bouguetj/calib{\_}doc/},
urldate = {2019-10-22}
}
@article{Fersch2017b,
abstract = {{\textcopyright} 2017 IEEE. We present the application of optical code division multiple access (OCDMA) modulation based on optical orthogonal codes for automotive time-of-flight light detection and ranging (LiDAR) system sensors using avalanche photodiode (APD) detectors. The modulation opens the possibility to discriminate single laser transmissions. This allows the realization of additional features like enhancing the systems' interference robustness or accelerating their scanning rate. The requirements on automotive LiDAR OCDMA modulation differs from telecommunication's demands in several ways: The sensor must guarantee absolute laser safety; front facing devices must be able to provide reliable long range results up to a distance of 150m and beyond; and the signal is transmitted through free space. After outlining the basic functionality of the sort of sensors in consideration, the properties of the optical orthogonal codes (OOC) modulation are compared with the state of the art theoretically. The influence of OOC parameters with respect to scanning LiDAR systems is examined. The modulation technique is then demonstrated experimentally with two examples: The detection and separation of coded and traditional signals is shown using a matched filter detection algorithm. In the same way, differently coded signals can be separated from each other. Impediments of the interference suppression quality are discussed. Finally, an overview of possible applications of the proposed technique in automotive LiDAR systems is given, which are enabled by the OCDMA modulation in the first place.},
author = {Fersch, Thomas and Weigel, Robert and Koelpin, Alexander},
doi = {10.1109/JSEN.2017.2688126},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems. Fersch, Weigel, Koelpin.pdf:pdf},
issn = {1530437X},
journal = {IEEE Sens. J.},
keywords = {Laser radar,LiDAR,OCDMA modulation,time-of-flight},
number = {11},
pages = {3507--3516},
title = {{A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems}},
volume = {17},
year = {2017}
}
@inproceedings{Popko2019b,
abstract = {Signal interference between two light detection and ranging (lidar) scanners can occur when the transmitted laser energy from one lidar is scattered from a target and returned to a second lidar. By modeling lidar transmission paths via ray tracing, it is shown that signal interference can be modeled by the coincidence of intersection between two lidar transmission paths and a scattering target. The evaluation of experimental observation and an analytical framework of lidar signal interference is presented that compares results of a Monte Carlo simulation to interference observations from circularly scanning lidar sensors. The comparison between simulated and experimentally observed interference events suggests that lidar interference may largely explained by geometry and angular conditions. The model provides preliminary explanation as to the angular distribution of interference events and distinct transitions between occurrences of different interference modes. However, further radiometric refinement is likely needed to best explain the manifestation of some interference events.},
author = {Popko, Gerald B. and Gaylord, Thomas K. and Valenta, Christopher R.},
booktitle = {Laser Radar Technol. Appl. XXIV},
doi = {10.1117/12.2518228},
editor = {Turner, Monte D. and Kamerman, Gary W.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Popko, Gaylord, Valenta/Signal interactions between lidar scanners. Popko, Gaylord, Valenta. 2019.pdf:pdf},
isbn = {9781510626751},
keywords = {autonomous vehicles,ladar,lidar,point clouds,signal interference},
month = {may},
pages = {18},
publisher = {SPIE},
title = {{Signal interactions between lidar scanners}},
url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11005/2518228/Signal-interactions-between-lidar-scanners/10.1117/12.2518228.full},
volume = {11005},
year = {2019}
}
@article{Viola2001,
abstract = {This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction ofa new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number ofcritical visual features and yields extremely efficient classifiers [6]. The third contribution is a method for combining classifiers in a “cascade” which allows background regions ofthe image to be quickly discarded while spending more computation on promising object-like regions. A set ofexperiments in the domain offace detection are presented. The system yields face detection performace comparable to the best previous systems [18, 13, 16, 12, 1]. Implemented on a conventional desktop, face detection proceeds at 15 frames per second},
author = {Viola, Paul and Jones, Michael},
doi = {10.3917/ving.094.0121},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fitzpatrick/Une histoire trs catholique Rvisionnisme et orthodoxie dans l'historiographie irlandaise. Fitzpatrick. 2007.pdf:pdf},
issn = {0294-1759},
journal = {Second Int. Work. Stat. Comput. Theor. Vis. –MODELING,LEARNING,COMPUTING, Sampl.},
title = {{Robust Real-time Object Detection Paul}},
year = {2001}
}
@article{Giuggioli2015,
abstract = {Animal coordinated movement interactions are commonly explained by assuming unspecified social forces of attraction, repulsion and alignment with parameters drawn from observed movement data. Here we propose and test a biologically realistic and quantifiable biosonar movement interaction mechanism for echolocating bats based on spatial perceptual bias, i.e. actual sound field, a reaction delay, and observed motor constraints in speed and acceleration. We found that foraging pairs of bats flying over a water surface swapped leader-follower roles and performed chases or coordinated manoeuvres by copying the heading a nearby individual has had up to 500 ms earlier. Our proposed mechanism based on the interplay between sensory-motor constraints and delayed alignment was able to recreate the observed spatial actor-reactor patterns. Remarkably, when we varied model parameters (response delay, hearing threshold and echolocation directionality) beyond those observed in nature, the spatio-temporal interaction patterns created by the model only recreated the observed interactions, i.e. chases, and best matched the observed spatial patterns for just those response delays, hearing thresholds and echolocation directionalities found to be used by bats. This supports the validity of our sensory ecology approach of movement coordination, where interacting bats localise each other by active echolocation rather than eavesdropping.},
author = {Giuggioli, Luca and McKetterick, Thomas J. and Holderied, Marc},
doi = {10.1371/journal.pcbi.1004089},
editor = {Ayers, Joseph},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Giuggioli, McKetterick, Holderied/Delayed Response and Biosonar Perception Explain Movement Coordination in Trawling Bats. Giuggioli, McKetterick, Holderied. 2015.pdf:pdf},
issn = {1553-7358},
journal = {PLOS Comput. Biol.},
month = {mar},
number = {3},
pages = {e1004089},
title = {{Delayed Response and Biosonar Perception Explain Movement Coordination in Trawling Bats}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1004089},
volume = {11},
year = {2015}
}
@article{Wei2018a,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@misc{Condliffe2015,
author = {Condliffe, Jamie},
booktitle = {Gizmodo},
title = {{A {\$}60 Hack Can Fool the LIDAR Sensors Used on Most Self-Driving Cars}},
url = {https://gizmodo.com/a-60-hack-can-fool-the-lidar-sensors-used-on-most-self-1729272292},
urldate = {2018-11-28},
year = {2015}
}
@article{Fischler2002,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/ smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing and analysis conditions. Implementation details and computational examples are also presented.},
archivePrefix = {arXiv},
arxivId = {3629719},
author = {Fischler, Martin A. and Bolles, Robert C.},
doi = {10.1145/358669.358692},
eprint = {3629719},
isbn = {0934613338},
issn = {00010782},
journal = {Commun. ACM},
month = {jun},
number = {6},
pages = {381--395},
pmid = {4650729},
title = {{Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography}},
url = {http://portal.acm.org/citation.cfm?doid=358669.358692},
volume = {24},
year = {2002}
}
@inproceedings{Hebel2018,
abstract = {In this paper, we examine crosstalk effects that can arise in multi-LiDAR configurations, and we show a data-based approach to mitigate these effects. Due to the ability to acquire precise 3D data of the environment, LiDAR-based sensor systems (sensors based on “Light Detection and Ranging”, e.g., laser scanners) increasingly find their way into various applications, e.g. in the automotive sector. However, with an increasing number of LiDAR sensors operating within close vicinity, the problem of potential crosstalk between these devices arises. “Crosstalk” outlines the following effect: In a typical LiDAR-based sensor, short laser pulses are emitted into the scene and the distance between sensor and object is derived from the time measured until an “echo” is received. In case multiple laser pulses of the same wavelength are emitted at the same time, the detector may not be able to distinguish between correct and false matches of laser pulses and echoes, resulting in erroneous range measurements and 3D points. During operation of our own multi-LiDAR sensor system, we were able to observe crosstalk effects in the acquired data. Having compared different spatial filtering approaches for the elimination of erroneous points in the 3D data, we propose a data-based spatio-temporal filtering and show its results, which may be sufficient depending on the application. However, technical solutions are desired for future LiDAR sensors.},
author = {Hebel, Marcus and Hammer, Marcus and Arens, Michael and Diehm, Axel L.},
booktitle = {Electro-Optical Remote Sens. XII},
doi = {10.1117/12.2324305},
editor = {Kamerman, Gary and Steinvall, Ove},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hebel et al/Mitigation of crosstalk effects in multi-LiDAR configurations. Hebel et al.. 2018.pdf:pdf},
isbn = {9781510621756},
issn = {1996756X},
keywords = {Crosstalk,LADAR,Laser radar,Laser scanning,LiDAR,Multi-sensor systems,Mutual interference,Temporal filtering},
month = {oct},
pages = {3},
publisher = {SPIE},
title = {{Mitigation of crosstalk effects in multi-LiDAR configurations}},
url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10796/2324305/Mitigation-of-crosstalk-effects-in-multi-LiDAR-configurations/10.1117/12.2324305.full},
volume = {10796},
year = {2018}
}
@article{Fridman2017,
abstract = {For the foreseeble future, human beings will likely remain an integral part of the driving task, monitoring the AI system as it performs anywhere from just over 0{\%} to just under 100{\%} of the driving. The governing objectives of the MIT Autonomous Vehicle Technology (MIT-AVT) study are to (1) undertake large-scale real-world driving data collection that includes high-definition video to fuel the development of deep learning based internal and external perception systems, (2) gain a holistic understanding of how human beings interact with vehicle automation technology by integrating video data with vehicle state data, driver characteristics, mental models, and self-reported experiences with technology, and (3) identify how technology and other factors related to automation adoption and use can be improved in ways that save lives. In pursuing these objectives, we have instrumented 23 Tesla Model S and Model X vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6 vehicles for both long-term (over a year per driver) and medium term (one month per driver) naturalistic driving data collection. Furthermore, we are continually developing new methods for analysis of the massive-scale dataset collected from the instrumented vehicle fleet. The recorded data streams include IMU, GPS, CAN messages, and high-definition video streams of the driver face, the driver cabin, the forward roadway, and the instrument cluster (on select vehicles). The study is on-going and growing. To date, we have 122 participants, 15,610 days of participation, 511,638 miles, and 7.1 billion video frames. This paper presents the design of the study, the data collection hardware, the processing of the data, and the computer vision algorithms currently being used to extract actionable knowledge from the data.},
archivePrefix = {arXiv},
arxivId = {1711.06976},
author = {Fridman, Lex and Brown, Daniel E. and Glazer, Michael and Angell, William and Dodd, Spencer and Jenik, Benedikt and Terwilliger, Jack and Patsekin, Aleksandr and Kindelsberger, Julia and Ding, Li and Seaman, Sean and Mehler, Alea and Sipperley, Andrew and Pettinato, Anthony and Seppelt, Bobbie and Angell, Linda and Mehler, Bruce and Reimer, Bryan},
eprint = {1711.06976},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fridman et al/MIT Autonomous Vehicle Technology Study Large-Scale Deep Learning Based Analysis of Driver Behavior and Interaction with Automation. Fri.pdf:pdf},
pages = {1--16},
title = {{MIT Autonomous Vehicle Technology Study: Large-Scale Deep Learning Based Analysis of Driver Behavior and Interaction with Automation}},
url = {http://arxiv.org/abs/1711.06976},
year = {2017}
}
@article{Gavin2019,
abstract = {The Levenberg-Marquardt algorithm was developed in the early 1960's to solve ne onlinear least squares problems. Least squares problems arise in the context of fitting a parameterized function to a set of measured data points by minimizing the sum of the squares of the errors between the data points and the function. If the fit function is not linear in the parameters the least squares problem is nonlinear. Nonlinear least squares methods iteratively reduce the sum of the squares of the errors between the function and the measured data points through a sequence of updates to parameter values. The Levenberg-Marquardt algorithm combines two minimization methods: the gradient descent method and the Gauss-Newton method. In the gradient descent method, the sum of the squared errors is reduced by updating the parameters in the steepest-descent direction. In the Gauss-Newton method, the sum of the squared errors is reduced by assuming the least squares function is locally quadratic, and finding the minimum of the quadratic. The Levenberg-Marquardt method acts more like a gradient-descent method when the parameters are far from their optimal value, and acts more like the Gauss-Newton method when the parameters are close to their optimal value. This document describes these methods and illustrates the use of software to solve nonlinear least squares curve-fitting problems.},
author = {Gavin, Henri P.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gavin/The Levenburg-Marqurdt Algorithm For Nonlinear Least Squares Curve-Fitting Problems. Gavin. 2019.pdf:pdf},
journal = {Duke Univ.},
pages = {1--19},
title = {{The Levenburg-Marqurdt Algorithm For Nonlinear Least Squares Curve-Fitting Problems}},
url = {http://people.duke.edu/{~}hpgavin/ce281/lm.pdf},
year = {2019}
}
@article{VelodyneHDL64,
author = {Rate, High Frame},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rate/HDL-64E. Rate. Unknown.pdf:pdf},
pages = {3--4},
title = {{HDL-64E}}
}
@article{Naroditsky2011,
abstract = {We propose a new method for extrinsic calibration of a line-scan LIDAR with a perspective projection camera. Our method is a closed-form, minimal solution to the problem. The solution is a symbolic template found via variable elimination and the multi-polynomial Macaulay resultant. It does not require initialization, and can be used in an automatic calibration setting when paired with RANSAC and least-squares refinement. We show the efficacy of our approach through a set of simulations and a real calibration. {\textcopyright} 2011 IEEE.},
author = {Naroditsky, Oleg and {Patterson IV}, Alexander and Daniilidis, Kostas},
doi = {10.1109/ICRA.2011.5980513},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Naroditsky, Patterson IV, Daniilidis/Automatic alignment of a camera with a line scan LIDAR system. Naroditsky, Patterson IV, Daniilidis. 2011.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {3429--3434},
publisher = {IEEE},
title = {{Automatic alignment of a camera with a line scan LIDAR system}},
year = {2011}
}
@article{Li2016,
abstract = {Detailed 3D modeling of indoor scene has become an important topic in many research fields. It can provide extensive information about the environment and boost various location based services, such as interactive gaming and indoor navigation. This paper presents an indoor scene construction approach using 2D line-scan LiDAR and entry-level digital camera. Both devices are mounted rigidly on a robotic servo, which sweeps vertically to cover the third dimension. Fiducial target based extrinsic calibration is applied to acquire transformation matrices between LiDAR and camera. Based on the transformation matrix, we perform registration to fuse the color images from the camera with the 3D points cloud from the LiDAR. The whole system setup has much lower cost as compared to systems using 3D LiDAR and omnidirectional camera. Using pre-calculated transformation matrices instead of feature extraction techniques such as SIFT or SURF in registration gives better fusion result and lower computational complexity. The experiments carried out in office building environment show promising results of our approach.},
author = {Li, Juan and He, Xiang and Li, Jia},
doi = {10.1109/NAECON.2015.7443100},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Li, He, Li/2D LiDAR and camera fusion in 3D modeling of indoor environment. Li, He, Li. 2016.pdf:pdf},
isbn = {9781467375658},
issn = {23792027},
journal = {Proc. IEEE Natl. Aerosp. Electron. Conf. NAECON},
keywords = {2D line-scan LiDAR,3D indoor modeling,digital camera,extrinsic calibration,sensor fusion},
number = {June 2015},
pages = {379--383},
title = {{2D LiDAR and camera fusion in 3D modeling of indoor environment}},
volume = {2016-March},
year = {2016}
}
@book{1385,
author = {غلامحسین, ثنایی},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hartley, Zisserman/Multiple View Geometry in computer vision. Hartley, Zisserman. 2003.pdf:pdf},
isbn = {9780521540513},
pages = {302},
title = {{Multiple View Geometry in computer vision}},
year = {1385}
}
@article{Shi2018,
abstract = {In this paper, we propose PointRCNN for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for refining proposals in the canonical coordinates to obtain the final detection results. Instead of generating proposals from RGB image or projecting point cloud to bird's view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of the whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box refinement and confidence prediction. Extensive experiments on the 3D detection benchmark of KITTI dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input. The code is available at https://github.com/sshaoshuai/PointRCNN.},
archivePrefix = {arXiv},
arxivId = {1812.04244},
author = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
eprint = {1812.04244},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Shi, Wang, Li/PointRCNN 3D Object Proposal Generation and Detection from Point Cloud. Shi, Wang, Li. 2018.pdf:pdf},
title = {{PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud}},
url = {http://arxiv.org/abs/1812.04244},
year = {2018}
}
@misc{googlevision,
title = {{Vision AI | Derive Image Insights via ML | Cloud Vision API | Google Cloud}},
url = {https://cloud.google.com/vision/},
urldate = {2019-10-22}
}
@article{Pusztai2018a,
abstract = {As autonomous driving attracts more and more attention these days, the algorithms and sensors used for machine perception become popular in research, as well. This paper investigates the extrinsic calibration of two frequently-applied sensors: the camera and Light Detection and Ranging (LiDAR). The calibration can be done with the help of ordinary boxes. It contains an iterative refinement step, which is proven to converge to the box in the LiDAR point cloud, and can be used for system calibration containing multiple LiDARs and cameras. For that purpose, a bundle adjustment-like minimization is also presented. The accuracy of the method is evaluated on both synthetic and real-world data, outperforming the state-of-the-art techniques. The method is general in the sense that it is both LiDAR and camera-type independent, and only the intrinsic camera parameters have to be known. Finally, a method for determining the 2D bounding box of the car chassis from LiDAR point clouds is also presented in order to determine the car body border with respect to the calibrated sensors.},
author = {Pusztai, Zolt{\'{a}}n and Eichhardt, Iv{\'{a}}n and Hajder, Levente},
doi = {10.3390/s18072139},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pusztai, Eichhardt, Hajder/Accurate calibration of multi-lidar-multi-camera systems. Pusztai, Eichhardt, Hajder. 2018.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Autonomous driving,Camera,Extrinsic calibration,LiDAR,LiDAR camera system,Machine perception},
number = {7},
pages = {1--22},
title = {{Accurate calibration of multi-lidar-multi-camera systems}},
volume = {18},
year = {2018}
}
@article{Kim2015b,
abstract = {The LIDAR scanner is at the heart of object detection of the self-driving car. Mutual interference between LIDAR scanners has not been regarded as a problem because the percentage of vehicles equipped with LIDAR scanners was very rare. With the growing number of autonomous vehicle equipped with LIDAR scanner operated close to each other at the same time, the LIDAR scanner may receive laser pulses from other LIDAR scanners. In this paper, three types of experiments and their results are shown, according to the arrangement of two LIDAR scanners. We will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some typical mutual interference scenario and report an analysis of the interference mechanism. {\textcopyright} 2015 COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Seonghyeon and Park, Yongwan},
doi = {10.1117/12.2178502},
editor = {Prochazka, Ivan and Sobolewski, Roman and James, Ralph B.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Occurrence and characteristics of mutual interference between LIDAR scanners. Kim et al.. 2015.pdf:pdf},
journal = {Phot. Count. Appl. 2015},
keywords = {LIDAR scanner,ghost target,mutual interference,self-driving car,time of flight},
month = {may},
number = {September},
pages = {95040K},
publisher = {International Society for Optics and Photonics},
title = {{Occurrence and characteristics of mutual interference between LIDAR scanners}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2178502},
volume = {9504},
year = {2015}
}
@book{Setright2003,
abstract = {Previously published: Palawan Press, 2002. Includes index. pt. 1. An historical review. -- To 1895 -- 1896-1905 -- 1906-15 -- 1916-25 -- 1926-35 -- 1936-45 -- 1946-55 -- 1956-65 -- 1966-75 -- 1976-85 -- From 1986 ... -- pt. 2. The yoke and the spur. -- Extortion and distortion -- Small expectations: the decadent history of a decaying idea -- Scapegoat and idol -- The liberator -- pt. 3. The face of the earth. -- The big city -- Somewhere to stop -- pt. 4. The turn of the wheel: revolutions in technology. -- The nature of revolution -- ForWarD -- The can ... of worms? -- The popular front -- Re-inventing the wheel -- Inconstant mesh -- Tyres and timing -- Fear of flying -- Fluid power -- Computer control -- The air inside -- Four-wheel equality -- pt. 5. Personal effects. -- Making a start -- Changing gears in changing times -- What to wear -- Sport -- Arts and fashions -- Appendix: Timescale.},
author = {Setright, L. J. K.},
isbn = {1862076987},
pages = {405},
publisher = {Granta},
title = {{Drive on! : a social history of the motor car}},
year = {2003}
}
@techreport{Pandar40UserGuide,
author = {{Hesai Photonics Technology Co Ltd}},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hesai Photonics Technology Co Ltd/Pandar40 40-Channel Mechanical LiDAR User's Manual. Hesai Photonics Technology Co Ltd. 2018.pdf:pdf},
pages = {41},
title = {{Pandar40 40-Channel Mechanical LiDAR User's Manual}},
year = {2018}
}
@misc{AVTROSdriver,
author = {{Miquel Massot}},
title = {{avt{\_}vimba{\_}camera - ROS Wiki}},
url = {https://wiki.ros.org/avt{\_}vimba{\_}camera},
urldate = {2019-10-27}
}
@article{Silva2018,
author = {Silva, Varuna De and Roche, Jamie and Kondoz, Ahmet},
doi = {10.3390/s18082730},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Silva, Roche, Kondoz/Robust Fusion of LiDAR and Wide-Angle Camera. Silva, Roche, Kondoz. 2018.pdf:pdf},
issn = {1424-8220},
keywords = {assistive robots,autonomous vehicles,depth sensing,detection,free space,gaussian process regression,lidar,sensor data fusion},
title = {{Robust Fusion of LiDAR and Wide-Angle Camera}},
year = {2018}
}
