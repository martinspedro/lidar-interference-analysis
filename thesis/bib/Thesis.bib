Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Liang2019,
abstract = {In this paper we propose to exploit multiple related tasks for accurate multi-sensor 3D object detection. Towards this goal we present an end-to-end learnable architecture that reasons about 2D and 3D object detection as well as ground estimation and depth completion. Our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels. Importantly, our approach leads the KITTI benchmark on 2D, 3D and bird's eye view object detection, while being real-time.},
author = {Liang, Ming and Yang, Bin and Chen, Yun and Hu, Rui and Urtasun, Raquel},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Liang et al/Multi-Task Multi-Sensor Fusion for 3D Object Detection. Liang et al.. 2019.pdf:pdf},
journal = {IEEE Conf. Comput. Vis. Pattern Recognition, Proceedings, CVPR},
keywords = {3d object detection,autonomous,multi-sensor fusion},
pages = {7345--7353},
title = {{Multi-Task Multi-Sensor Fusion for 3D Object Detection}},
url = {https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Multi-Task-Multi-Sensor-Fusion-for-3D-Object-Detection.pdf},
year = {2019}
}
@misc{udacity,
title = {{self-driving-car/datasets at master {\textperiodcentered} udacity/self-driving-car {\textperiodcentered} GitHub}},
url = {https://github.com/udacity/self-driving-car/tree/master/datasets},
urldate = {2019-10-20}
}
@book{1385,
author = {غلامحسین, ثنایی},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hartley, Zisserman/Multiple View Geometry in computer vision. Hartley, Zisserman. 2003.pdf:pdf},
isbn = {9780521540513},
pages = {302},
title = {{Multiple View Geometry in computer vision}},
year = {1385}
}
@article{Taylor2013,
abstract = {This paper is about automatic calibration of a camera-lidar system. The method presented is designed to be as general as possible allowing it to be used in a large range of systems and applications. The approach uses normalized mutual information to compare camera images with lidar scans of the same area. A camera model that takes into account orientation, location and focal length is used to create a 2D lidar image, with the intensity of the pixels representing a feature of the lidar scan that is chosen depending on the application. Particle swarm optimization is used to find the optimal model parameters. The method presented is successfully validated on a variety of cameras, lidars and locations, including scans of both urban and natural environments},
author = {Taylor, Zachary and Nieto, Juan},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Taylor, Nieto/Automatic Calibration of Lidar and Camera Images using Normalized Mutual Information. Taylor, Nieto. 2013.pdf:pdf},
journal = {IEEE Int. Conf. Robot. Autom.},
title = {{Automatic Calibration of Lidar and Camera Images using Normalized Mutual Information}},
url = {https://pdfs.semanticscholar.org/fc08/d41be320a827c3f045c4429e13495da5a1cc.pdf{\%}0Ahttp://www-personal.acfr.usyd.edu.au/jnieto/Publications{\_}files/TaylorICRA2013.pdf},
year = {2013}
}
@article{Park2014,
abstract = {Calibration between color camera and 3D Light Detection And Ranging (LIDAR) equipment is an essential process for data fusion. The goal of this paper is to improve the calibration accuracy between a camera and a 3D LIDAR. In particular, we are interested in calibrating a low resolution 3D LIDAR with a relatively small number of vertical sensors. Our goal is achieved by employing a new methodology for the calibration board, which exploits 2D-3D correspondences. The 3D corresponding points are estimated from the scanned laser points on the polygonal planar board with adjacent sides. Since the lengths of adjacent sides are known, we can estimate the vertices of the board as a meeting point of two projected sides of the polygonal board. The estimated vertices from the range data and those detected from the color image serve as the corresponding points for the calibration. Experiments using a low-resolution LIDAR with 32 sensors show robust results. {\textcopyright} 2014 by the authors; licensee MDPI, Basel, Switzerland.},
author = {Park, Yoonsu and Yun, Seokmin and Won, Chee Sun and Cho, Kyungeun and Um, Kyhyun and Sim, Sungdae},
doi = {10.3390/s140305333},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Park et al/Calibration between color camera and 3D LIDAR instruments with a polygonal planar board. Park et al.. 2014.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {3D LIDAR,3D point clouds,Calibration board,Calibration matrix,Camera calibration,Sensor fusion},
number = {3},
pages = {5333--5353},
title = {{Calibration between color camera and 3D LIDAR instruments with a polygonal planar board}},
volume = {14},
year = {2014}
}
@article{Wei2018a,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@article{Kim2015c,
abstract = {The LIDAR scanner is at the heart of object detection of the self-driving car. Mutual interference between LIDAR scanners has not been regarded as a problem because the percentage of vehicles equipped with LIDAR scanners was very rare. With the growing number of autonomous vehicle equipped with LIDAR scanner operated close to each other at the same time, the LIDAR scanner may receive laser pulses from other LIDAR scanners. In this paper, three types of experiments and their results are shown, according to the arrangement of two LIDAR scanners. We will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some typical mutual interference scenario and report an analysis of the interference mechanism. {\textcopyright} 2015 COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Seonghyeon and Park, Yongwan},
doi = {10.1117/12.2178502},
editor = {Prochazka, Ivan and Sobolewski, Roman and James, Ralph B.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Occurrence and characteristics of mutual interference between LIDAR scanners. Kim et al.. 2015.pdf:pdf},
journal = {Phot. Count. Appl. 2015},
keywords = {LIDAR scanner,ghost target,mutual interference,self-driving car,time of flight},
month = {may},
number = {September},
pages = {95040K},
publisher = {International Society for Optics and Photonics},
title = {{Occurrence and characteristics of mutual interference between LIDAR scanners}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2178502},
volume = {9504},
year = {2015}
}
@misc{Camerona,
abstract = {Retrofit a Car for self-driving using LIDAR, camera and Radar},
author = {Cameron, Oliver and Voyage},
keywords = {LIDAR,Self-Driving Cars,Sensor Fusion},
mendeley-tags = {LIDAR,Self-Driving Cars,Sensor Fusion},
title = {{The Story of Homer: Voyage's First Self-Driving Taxi}},
url = {https://news.voyage.auto/the-story-of-homer-voyages-first-self-driving-taxi-f0a6466718af},
urldate = {2018-10-09}
}
@article{KresimirKusevicOttawaCA;PaulMrstikOttawaCA;CraigLenGlennieSpring2017,
author = {{Kresimir Kusevic, Ottawa (CA); Paul Mrstik, Ottawa (CA); Craig Len Glennie, Spring}, TX (US)},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Method and System for Aligninga Line Scan Camerawith a Lidar Scanner for Real Time Data Fusion in Three Dimiensions. Kresimir Kusevic, O.pdf:pdf},
isbn = {2222222222},
number = {19},
title = {{Method and System for Aligninga Line Scan Camerawith a Lidar Scanner for Real Time Data Fusion in Three Dimiensions}},
volume = {1},
year = {2017}
}
@article{Fremont2013,
author = {Fremont, Vincent and Alberto, Sergio and Florez, Rodriguez and Bonnifait, Philippe and Targets, Circular and Video, Alignment and Sensors, Lidar and Robotics, Advanced and Stm, Francis},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fremont et al/Circular Targets for 3D Alignment of Video and Lidar Sensors To cite this version Circular Targets for 3D Alignment of Video and Lidar.pdf:pdf},
title = {{Circular Targets for 3D Alignment of Video and Lidar Sensors To cite this version : Circular Targets for 3D Alignment of Video and Lidar Sensors}},
year = {2013}
}
@article{Pereira2016,
abstract = {Autonomous navigation is an important field of research and, given the complexity of real world environments, most of the systems rely on a complex perception system combining multiple sensors on board, which reinforces the concern of sensor calibration. Most calibration methods rely on manual or semi-automatic interactive procedures, but reliable fully automatic methods are still missing. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. The idea proposed in this paper is to use a ball in motion in front of a set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation among all pairs of sensors. This paper proposes and describes such a method with results demonstrating its validity.},
author = {Pereira, Marcelo and Silva, David and Santos, Vitor and Dias, Paulo},
doi = {10.1016/j.robot.2016.05.010},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pereira et al/Self calibration of multiple LIDARs and cameras on autonomous vehicles. Pereira et al.. 2016.pdf:pdf},
issn = {09218890},
journal = {Rob. Auton. Syst.},
keywords = {3D data fitting,Extrinsic calibration,Point cloud},
pages = {326--337},
title = {{Self calibration of multiple LIDARs and cameras on autonomous vehicles}},
volume = {83},
year = {2016}
}
@article{Hecht2018,
author = {Hecht, Jeff},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hecht/Lidar for Self-Dr riving Cars. Hecht. 2018.pdf:pdf},
number = {January},
pages = {26--33},
title = {{Lidar for Self-Dr riving Cars}},
year = {2018}
}
@misc{azurecv,
title = {{Image Processing with the Computer Vision API | Microsoft Azure}},
url = {https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/},
urldate = {2019-10-22}
}
@misc{Britannica,
author = {Britannica},
title = {{Model T | automobile | Britannica.com}},
url = {https://www.britannica.com/technology/Model-T},
urldate = {2019-10-20}
}
@misc{Simonite2017,
author = {Simonite, Tom},
booktitle = {MT Technol. Rev.},
title = {{Self-Driving Cars' Spinning-Laser Problem - MIT Technology Review}},
url = {https://www.technologyreview.com/s/603885/autonomous-cars-lidar-sensors/},
urldate = {2019-01-23},
year = {2017}
}
@article{Shaukat2016,
abstract = {{\textcopyright} 2016 by the authors; licensee MDPI,Basel,Switzerland. In recent decades,terrain modelling and reconstruction techniques have increased research interest in precise short and long distance autonomous navigation,localisation and mapping within field robotics. One of the most challenging applications is in relation to autonomous planetary exploration using mobile robots. Rovers deployed to explore extraterrestrial surfaces are required to perceive and model the environment with little or no intervention from the ground station. Up to date,stereopsis represents the state-of-the art method and can achieve short-distance planetary surface modelling. However,future space missions will require scene reconstruction at greater distance,fidelity and feature complexity,potentially using other sensors like Light Detection And Ranging (LIDAR). LIDAR has been extensively exploited for target detection,identification,and depth estimation in terrestrial robotics,but is still under development to become a viable technology for space robotics. This paper will first review current methods for scene reconstruction and terrain modelling using cameras in planetary robotics and LIDARs in terrestrial robotics; then we will propose camera-LIDAR fusion as a feasible technique to overcome the limitations of either of these individual sensors for planetary exploration. A comprehensive analysis will be presented to demonstrate the advantages of camera-LIDAR fusion in terms of range,fidelity,accuracy and computation.},
author = {Shaukat, Affan and Blacker, Peter C. and Spiteri, Conrad and Gao, Yang},
doi = {10.3390/s16111952},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Shaukat et al/Towards camera-LIDAR fusion-based terrain modelling for planetary surfaces Review and analysis. Shaukat et al.. 2016.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {3-D reconstruction,Hybrid vision systems,LIDAR-camera fusion,Planetary surface perception,Terrain modelling},
number = {11},
title = {{Towards camera-LIDAR fusion-based terrain modelling for planetary surfaces: Review and analysis}},
volume = {16},
year = {2016}
}
@inproceedings{Kim2015c,
abstract = {The LIDAR scanner is at the heart of object detection of the self-driving car. Mutual interference between LIDAR scanners has not been regarded as a problem because the percentage of vehicles equipped with LIDAR scanners was very rare. With the growing number of autonomous vehicle equipped with LIDAR scanner operated close to each other at the same time, the LIDAR scanner may receive laser pulses from other LIDAR scanners. In this paper, three types of experiments and their results are shown, according to the arrangement of two LIDAR scanners. We will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some typical mutual interference scenario and report an analysis of the interference mechanism. {\textcopyright} 2015 COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Seonghyeon and Park, Yongwan},
booktitle = {Phot. Count. Appl. 2015},
doi = {10.1117/12.2178502},
editor = {Prochazka, Ivan and Sobolewski, Roman and James, Ralph B.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Occurrence and characteristics of mutual interference between LIDAR scanners. Kim et al.. 2015.pdf:pdf},
keywords = {LIDAR scanner,ghost target,mutual interference,self-driving car,time of flight},
month = {may},
pages = {95040K},
publisher = {International Society for Optics and Photonics},
title = {{Occurrence and characteristics of mutual interference between LIDAR scanners}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2178502},
volume = {9504},
year = {2015}
}
@article{Chum2005,
abstract = {The problem of model parameters estimation from data with a presence of outlier measurements often arises in computer vision and methods of robust estimation have to be used. The RANSAC algorithm introduced by Fishler and Bolles in 1981 is the a widely used robust estimator in the field of computer vision. The algorithm is capable of providing good estimates from data contaminated by large (even significantly more than 50{\%}) fraction of outliers. RANSAC is an optimization method that uses a data-driven random sampling of the parameter space to find the extremum of the cost function. Samples of data define points of the parameter space in which the cost function is evaluated and model parameters with the best score are output. This thesis provides a detailed analysis of RANSAC, which is recast as time-constrained op- timization – a solution that is optimal with certain confidence is sought in the shortest possible time. Next, the concept of randomized cost function evaluation in RANSAC is introduced and its superiority over the deterministic evaluation is shown. A provably optimal strategy for the ran- domized cost function evaluation is derived. A known discrepancy, caused by noise on inliers, between theoretical prediction of the time required to find the solution and practically observed running times is traced to a tacit assump- tions of RANSAC. The proposed LO-RANSAC algorithm reaches almost perfect agreement with theoretical predictions without any negative impact on the time complexity. A unified method of estimation of model and its degenerate configuration (epipolar geome- try and homography of a dominant plane) at the same time without a priori knowledge of the presence of the degenerate configuration (dominant plane) is derived. Next, it is shown that using oriented geometric constraints that arise from a realistic model of physical camera devices, saves non-negligible fraction of computational time. No negative side effect are related to the application of the oriented constraints. An algorithm exploiting (possibly noisy) match quality to modify the sampling strategy is introduced. The quality of a match is an often freely available quantity in the matching prob- lem. The approach increases the efficiency of the algorithm while keeping the same robustness as RANSAC in the worst-case situation (when the match quality is unrelated to whether a corre- spondence is a mismatch or not). Most of the algorithms in the thesis are motivated by (and presented on) estimation of a multi- view geometry. The algorithms are, however, general robust estimation techniques and can be easily used in other application areas too.},
author = {Chum, Ondrej},
doi = {10.1016/S0921-8777(99)00013-0},
isbn = {3015947974},
issn = {1213-2365},
journal = {Phd Thesis},
pmid = {10422537},
title = {{Two-View Geometry Estimation by Random Sample and Consensus}},
url = {http://cmp.felk.cvut.cz/{~}chum/papers/Chum-PhD.pdf},
year = {2005}
}
@inproceedings{Yoneda2014,
abstract = {In recent years, automated vehicle researches move on to the next stage, that is auto-driving experiments on public roads. Major challenge is how to robustly drive at complicated situations such as narrow or non-featured road. In order to realize practical performance, some static information should be kept on memory such as road topology, building shape, white line, curb, traffic light and so on. Currently, some measurement companies have already begun to prepare map database for automated vehicles. They are able to provide highly-precise 3-D map for robust automated driving. This study focuses on what kind of data should be observed during automated driving with such precise database. In particular, we focus on the accurate localization based on the use of lidar data and precise 3-D map, and propose a feature quantity for scan data based on distribution of clusters. Localization experiment shows that our method can measure surrounding uncertainty and guarantee accurate localization.},
author = {Yoneda, Keisuke and Tehrani, Hossein and Ogawa, Takashi and Hukuyama, Naohisa and Mita, Seiichi},
booktitle = {IEEE Intell. Veh. Symp. Proc.},
doi = {10.1109/IVS.2014.6856596},
isbn = {9781479936380},
issn = {10901981},
pages = {1345--1350},
pmid = {12693519},
title = {{Lidar scan feature for localization with highly precise 3-D map}},
year = {2014}
}
@misc{googlevision,
title = {{Vision AI | Derive Image Insights via ML | Cloud Vision API | Google Cloud}},
url = {https://cloud.google.com/vision/},
urldate = {2019-10-22}
}
@article{DeSilva2018,
abstract = {Autonomous robots that assist humans in day to day living tasks are becoming increasingly popular. Autonomous mobile robots operate by sensing and perceiving their surrounding environment to make accurate driving decisions. A combination of several different sensors such as LiDAR, radar, ultrasound sensors and cameras are utilized to sense the surrounding environment of autonomous vehicles. These heterogeneous sensors simultaneously capture various physical attributes of the environment. Such multimodality and redundancy of sensing need to be positively utilized for reliable and consistent perception of the environment through sensor data fusion. However, these multimodal sensor data streams are different from each other in many ways, such as temporal and spatial resolution, data format, and geometric alignment. For the subsequent perception algorithms to utilize the diversity offered by multimodal sensing, the data streams need to be spatially, geometrically and temporally aligned with each other. In this paper, we address the problem of fusing the outputs of a Light Detection and Ranging (LiDAR) scanner and a wide-angle monocular image sensor for free space detection. The outputs of LiDAR scanner and the image sensor are of different spatial resolutions and need to be aligned with each other. A geometrical model is used to spatially align the two sensor outputs, followed by a Gaussian Process (GP) regression-based resolution matching algorithm to interpolate the missing data with quantifiable uncertainty. The results indicate that the proposed sensor data fusion framework significantly aids the subsequent perception steps, as illustrated by the performance improvement of a uncertainty aware free space detection algorithm},
archivePrefix = {arXiv},
arxivId = {1710.06230},
author = {{De Silva}, Varuna and Roche, Jamie and Kondoz, Ahmet},
doi = {10.3390/s18082730},
eprint = {1710.06230},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/De Silva, Roche, Kondoz/Robust fusion of LiDAR and wide-angle camera data for autonomous mobile robots. De Silva, Roche, Kondoz. 2018.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Assistive robots,Autonomous vehicles,Depth sensing,Free space detection,Gaussian process regression,LiDAR,Sensor data fusion},
number = {8},
title = {{Robust fusion of LiDAR and wide-angle camera data for autonomous mobile robots}},
volume = {18},
year = {2018}
}
@article{Park2019,
abstract = {We present a deep convolutional neural network (CNN) architecture for high-precision depth estimation by jointly utilizing sparse 3D LiDAR and dense stereo depth information. In this network, the complementary characteristics of sparse 3D LiDAR and dense stereo depth are simultaneously encoded in a boosting manner. Tailored to the LiDAR and stereo fusion problem, the proposed network differs from previous CNNs in the incorporation of a compact convolution module, which can be deployed with the constraints of mobile devices. As training data for the LiDAR and stereo fusion is rather limited, we introduce a simple yet effective approach for reproducing the raw KITTI dataset. The raw LIDAR scans are augmented by adapting an off-the-shelf stereo algorithm and a confidence measure. We evaluate the proposed network on the KITTI benchmark and data collected by our multi-sensor acquisition system. Experiments demonstrate that the proposed network generalizes across datasets and is significantly more accurate than various baseline approaches.},
author = {Park, Kihong and Kim, Seungryong and Sohn, Kwanghoon},
doi = {10.1109/tits.2019.2891788},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Park, Kim, Sohn/High-Precision Depth Estimation Using Uncalibrated LiDAR and Stereo Fusion. Park, Kim, Sohn. 2019.pdf:pdf},
issn = {1524-9050},
journal = {IEEE Trans. Intell. Transp. Syst.},
pages = {1--15},
publisher = {IEEE},
title = {{High-Precision Depth Estimation Using Uncalibrated LiDAR and Stereo Fusion}},
volume = {PP},
year = {2019}
}
@article{Kim2017,
author = {Kim, Gunzung and Eom, Jeongsook and Choi, Jeonghee and Park, Yongwan},
doi = {10.14372/iemek.2017.12.1.43},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Mutual Interference on Mobile Pulsed Scanning LIDAR. Kim et al.. 2017.pdf:pdf},
issn = {1975-5066},
journal = {IEMEK J. Embed. Syst. Appl.},
month = {feb},
number = {1},
pages = {43--62},
title = {{Mutual Interference on Mobile Pulsed Scanning LIDAR}},
url = {http://koreascience.or.kr/journal/view.jsp?kj=OBDDBE{\&}py=2017{\&}vnc=v12n1{\&}sp=43},
volume = {12},
year = {2017}
}
@misc{boofcv,
title = {{BoofCV}},
url = {https://boofcv.org/index.php?title=Main{\_}Page},
urldate = {2019-10-22}
}
@article{Castorena2016,
abstract = {We present a new method for joint automatic extrinsic calibration and sensor fusion for a multimodal sensor system comprising a LIDAR and an optical camera. Our approach exploits the natural alignment of depth and intensity edges when the calibration parameters are correct. Thus, in contrast to a number of existing approaches, we do not require the presence or identification of known alignment targets. On the other hand, the characteristics of each sensor modality, such as sampling pattern and information measured, are significantly different, making direct edge alignment difficult. To overcome this difficulty, we jointly fuse the data and estimate the calibration parameters. In particular, the joint processing evaluates and optimizes both the quality of edge alignment and the performance of the fusion algorithm using a common cost function on the output. We demonstrate accurate calibration in practical configurations in which depth measurements are sparse and contain no reflectivity information. Experiments on synthetic and real data obtained with a three-dimensional LIDAR sensor demonstrate the effectiveness of our approach.},
author = {Castorena, Juan and Kamilov, Ulugbek S. and Boufounos, Petros T.},
doi = {10.1109/ICASSP.2016.7472200},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Castorena, Kamilov, Boufounos/Autocalibration of lidar and optical cameras via edge alignment. Castorena, Kamilov, Boufounos. 2016.pdf:pdf},
isbn = {9781479999880},
issn = {15206149},
journal = {ICASSP, IEEE Int. Conf. Acoust. Speech Signal Process. - Proc.},
keywords = {Multimodal calibration,depth superresolution,intersensor registration,sensor fusion,total variation},
pages = {2862--2866},
publisher = {IEEE},
title = {{Autocalibration of lidar and optical cameras via edge alignment}},
volume = {2016-May},
year = {2016}
}
@book{Merklinger1993,
author = {Merklinger, Harold M.},
isbn = {0969502524},
pages = {104},
publisher = {H.M. Merklinger},
title = {{Focusing the view camera : a scientific way to focus the view camera and estimate depth of field}},
year = {1993}
}
@article{Chien2017,
abstract = {{\textcopyright} 2016 IEEE. Recently LiDAR-camera systems have rapidly emerged in many applications. The integration of laser range-finding technologies into existing vision systems enables a more comprehensive understanding of 3D structure of the environment. The advantage, however, relies on a good geometrical calibration between the LiDAR and the image sensors. In this paper we consider visual odometry, a discipline in computer vision and robotics, in the context of recently emerging online sensory calibration studies. By embedding the online calibration problem into a LiDAR-monocular visual odometry technique, the temporal change of extrinsic parameters can be tracked and compensated effectively.},
author = {Chien, Hsiang Jen and Klette, Reinhard and Schneider, Nick and Franke, Uwe},
doi = {10.1109/ICPR.2016.7900068},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Chien et al/Visual odometry driven online calibration for monocular LiDAR-camera systems. Chien et al.. 2017.pdf:pdf},
isbn = {9781509048472},
issn = {10514651},
journal = {Proc. - Int. Conf. Pattern Recognit.},
month = {apr},
pages = {2848--2853},
publisher = {IEEE},
title = {{Visual odometry driven online calibration for monocular LiDAR-camera systems}},
year = {2017}
}
@incollection{MartinVelasMichalSpanelZdenekMaterna2013,
abstract = {Quinolone-resistant clinical Escherichia coli isolates were examined for mutations in the marRAB operon of the multiple antibiotic resistance (mar) locus. Among 23 strains evaluated, 8 were chosen for further study: 3 that showed relatively high levels of uninduced, i.e., constitutive, expression of the operon and 5 with variable responses to induction by salicylate or tetracyclines. The marR genes, specifying the repressor of the operon, cloned from the three strains constitutively expressing the operon did not reduce the level of expression of beta-galactosidase from a marO::lacZ transcriptional fusion and were therefore mutant; however, marR genes cloned from the five other clinical strains repressed LacZ expression and were wild type. All three mutant marR genes contained more than one mutation: a deletion and a point mutation. Inactivation of the mar locus in the three known marR mutant strains with a kanamycin resistance cassette introduced by homologous recombination reduced resistance to quinolones and multiple antibiotics. These findings indicate that mar operon mutations exist in quinolone-resistant clinical E. coli isolates and contribute to quinolone and multidrug resistance.},
archivePrefix = {arXiv},
arxivId = {1505.00729},
author = {{Martin Velas, Michal Spanel, Zdenek Materna}, Adam Herout},
booktitle = {Flight Dyn. Princ.},
doi = {10.1016/B978-0-08-098242-7.00008-0},
eprint = {1505.00729},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Martin Velas, Michal Spanel, Zdenek Materna/Calibration of RGB Camera With Velodyne LiDAR. Martin Velas, Michal Spanel, Zdenek Materna. 2013.pdf:pdf},
isbn = {9781461434801},
issn = {0066-4804},
keywords = {L{\'{i}}DAR,Velodyne,calibration,camera,marker},
month = {jul},
number = {7},
pages = {219--241},
pmid = {8807064},
publisher = {Elsevier},
title = {{Calibration of RGB Camera With Velodyne LiDAR}},
url = {https://linkinghub.elsevier.com/retrieve/pii/B9780080982427000080},
volume = {40},
year = {2013}
}
@misc{matlabcvtoolbox,
title = {{Computer Vision Toolbox - MATLAB {\&} Simulink}},
url = {https://www.mathworks.com/products/computer-vision.html},
urldate = {2019-10-22}
}
@article{Christopher2015,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0403007},
author = {Christopher, Inventors and Ramsey, Paul and Chant, Garry Richard and Lockley, Andrew Robert and Gb, Wantage and Fields, Brian and Watson, Martin John and Rachel, Eleanor and Hyde, Ann and Gb, Wantage and Jasper, A},
doi = {10.1016/j.(73)},
eprint = {0403007},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Christopher et al/( 12 ) United States Patent ( 10 ) Patent No .. Christopher et al.. 2015.pdf:pdf},
isbn = {2010512510},
issn = {2470-0010},
number = {12},
pmid = {1000182772},
primaryClass = {arXiv:physics},
title = {{( 12 ) United States Patent ( 10 ) Patent No .:}},
volume = {2},
year = {2015}
}
@article{doi:10.1080/00690805.1982.10438226,
author = {Jones, Alan D},
doi = {10.1080/00690805.1982.10438226},
journal = {Cartography},
number = {4},
pages = {258},
publisher = {Taylor {\&} Francis},
title = {{Manual of Photogrammetry, eds C.C. Slama, C. Theurer and S.W. Hendrikson, American Society of Photogrammetry, Falls Church, Va., 1980, Fourth Edition, 180 × 260mm, xvi and 1056 pages (with index), 72 tables, 866 figures. ISBN 0 937294 01 2.}},
url = {https://doi.org/10.1080/00690805.1982.10438226},
volume = {12},
year = {1982}
}
@misc{awsRekognition,
author = {Aws},
title = {{Amazon Rekognition – Video and Image - AWS}},
url = {https://aws.amazon.com/rekognition/},
urldate = {2019-10-22}
}
@article{Fremont2013,
author = {Fremont, Vincent and Alberto, Sergio and Florez, Rodriguez and Bonnifait, Philippe and Targets, Circular and Video, Alignment and Sensors, Lidar and Robotics, Advanced and Stm, Francis},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fremont et al/Circular Targets for 3D Alignment of Video and Lidar Sensors To cite this version Circular Targets for 3D Alignment of Video and Lidar.pdf:pdf},
title = {{Circular Targets for 3D Alignment of Video and Lidar Sensors To cite this version : Circular Targets for 3D Alignment of Video and Lidar Sensors}},
year = {2013}
}
@article{Silva2018,
author = {Silva, Varuna De and Roche, Jamie and Kondoz, Ahmet},
doi = {10.3390/s18082730},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Silva, Roche, Kondoz/Robust Fusion of LiDAR and Wide-Angle Camera. Silva, Roche, Kondoz. 2018.pdf:pdf},
issn = {1424-8220},
keywords = {assistive robots,autonomous vehicles,depth sensing,detection,free space,gaussian process regression,lidar,sensor data fusion},
title = {{Robust Fusion of LiDAR and Wide-Angle Camera}},
year = {2018}
}
@article{EvansT.C.GavrilovichE.MihaiR.C.andIsbasescuI.2014,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0403007},
author = {{Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isbasescu, I.}, Easyg Llc and Thelen, Darryl and Martin, J A and Allen, S M and SA, Slane},
doi = {10.1037/t24245-000},
eprint = {0403007},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isbasescu, I. et al/( 12 ) Patent Application Publication ( 10 ) Pub . No . US 2006 0222585 A1 Figure 1. Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isba.pdf:pdf},
isbn = {2009023471},
issn = {13871811},
number = {15},
pages = {354},
pmid = {23110556},
primaryClass = {arXiv:physics},
title = {{( 12 ) Patent Application Publication ( 10 ) Pub . No .: US 2006 / 0222585 A1 Figure 1}},
volume = {002},
year = {2014}
}
@misc{WHOvisualizer,
title = {{WHO | Death on the roads}},
url = {https://extranet.who.int/roadsafety/death-on-the-roads/{\#}ticker//all},
urldate = {2019-10-20}
}
@misc{vlfeat,
author = {Vedaldi, A. and Fulkerson, B.},
title = {{VLFeat: An Open and Portable Library of Computer Vision Algorithms}},
url = {http://www.vlfeat.org/},
year = {2008}
}
@inproceedings{Yue2018,
abstract = {3D LiDAR scanners are playing an increasingly important role in autonomous driving as they can generate depth information of the environment. However, creating large 3D LiDAR point cloud datasets with point-level labels requires a significant amount of manual annotation. This jeopardizes the efficient development of supervised deep learning algorithms which are often data-hungry. We present a framework to rapidly create point clouds with accurate point-level labels from a computer game. The framework supports data collection from both auto-driving scenes and user-configured scenes. Point clouds from auto-driving scenes can be used as training data for deep learning algorithms, while point clouds from user-configured scenes can be used to systematically test the vulnerability of a neural network, and use the falsifying examples to make the neural network more robust through retraining. In addition, the scene images can be captured simultaneously in order for sensor fusion tasks, with a method proposed to do automatic calibration between the point clouds and captured scene images. We show a significant improvement in accuracy (+9{\%}) in point cloud segmentation by augmenting the training dataset with the generated synthesized data. Our experiments also show by testing and retraining the network using point clouds from user-configured scenes, the weakness/blind spots of the neural network can be fixed.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1804.00103},
author = {Yue, Xiangyu and Wu, Bichen and Seshia, Sanjit A. and Keutzer, Kurt and Sangiovanni-Vincentelli, Alberto L.},
booktitle = {Proc. 2018 ACM Int. Conf. Multimed. Retr. - ICMR '18},
doi = {10.1145/3206025.3206080},
eprint = {1804.00103},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yue et al/A LiDAR Point Cloud Generator. Yue et al.. 2018.pdf:pdf},
isbn = {9781450350464},
month = {mar},
number = {Nips},
pages = {458--464},
publisher = {ACM Press},
title = {{A LiDAR Point Cloud Generator}},
url = {http://dl.acm.org/citation.cfm?doid=3206025.3206080 http://arxiv.org/abs/1804.00103},
year = {2018}
}
@article{Mirzaei2012,
abstract = {In this paper we address the problem of estimating the intrinsic parameters of a 3D LIDAR while at the same time computing its extrinsic calibration with respect to a rigidly connected camera. Existing approaches to solve this nonlinear estimation problem are based on iterative minimization of nonlinear cost functions. In such cases, the accuracy of the resulting solution hinges on the availability of a precise initial estimate, which is often not available. In order to address this issue, we divide the problem into two least-squares sub-problems, and analytically solve each one to determine a precise initial estimate for the unknown parameters. We further increase the accuracy of these initial estimates by iteratively minimizing a batch nonlinear least-squares cost function. In addition, we provide the minimal identifiability conditions, under which it is possible to accurately estimate the unknown parameters. Experimental results consisting of photorealistic 3D reconstruction of indoor and outdoor scenes, as well as standard metrics of the calibration errors, are used to assess the validity of our approach.},
author = {Mirzaei, Faraz M. and Kottas, Dimitrios G. and Roumeliotis, Stergios I.},
doi = {10.1177/0278364911435689},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Mirzaei, Kottas, Roumeliotis/3D LIDAR-camera intrinsic and extrinsic calibration Identifiability and analytical least-squares-based initialization. Mirzaei, Kottas,.pdf:pdf},
issn = {02783649},
journal = {Int. J. Rob. Res.},
keywords = {Sensing and perception,calibration and identification,computer vision,range sensing},
number = {4},
pages = {452--467},
title = {{3D LIDAR-camera intrinsic and extrinsic calibration: Identifiability and analytical least-squares-based initialization}},
volume = {31},
year = {2012}
}
@article{Pandey2012,
abstract = {This paper reports on a mutual information (MI) based algo- rithm for automatic extrinsic calibration of a 3D laser scan- ner and optical camera system. By using MI as the regis- tration criterion, our method is able to work in situ without the need for any specific calibration targets, which makes it practical for in-field calibration. The calibration parameters are estimated by maximizing the mutual information obtained between the sensor-measured surface intensities. We calcu- late the Cramer-Rao-Lower-Bound (CRLB) and show that the sample variance of the estimated parameters empirically ap- proaches the CRLB for a sufficient number of views. Fur- thermore, we compare the calibration results to independent ground-truth and observe that the mean error also empirically approaches to zero as the number of views are increased. This indicates that the proposed algorithm, in the limiting case, calculates a minimum variance unbiased (MVUB) estimate of the calibration parameters. Experimental results are pre- sented for data collected by a vehicle mounted with a 3D laser scanner and an omnidirectional camera system.},
author = {Pandey, Gaurav and McBride, James R and Savarese, Silvio and Eustice, Ryan M},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pandey et al/Automatic targetless extrinsic calibration of a 3d lidar and camera by maximizing mutual information. Pandey et al.. 2012.pdf:pdf},
journal = {Proc. {\{}AAAI{\}} Natl. Conf. Artif. Intell.},
keywords = {calibration,ladybug,omnidirectional,velodyne},
pages = {2053--2059},
title = {{Automatic targetless extrinsic calibration of a 3d lidar and camera by maximizing mutual information}},
year = {2012}
}
@article{Yang2017,
abstract = {Combining active and passive imaging sensors enables creating a more detailed 3D model of the real world. Then, these 3D data can be used for various applications, such as city mapping, indoor navigation, autonomous vehicles, etc. Typically, LiDAR and camera as imaging sensors are installed on these systems. Both of these sensors have advantages and drawbacks. Thus, LiDAR sensor directly provides relatively accurate 3D point cloud, but LiDAR point cloud barely contains the surface textures and details, such as traffic signs and alpha numeric information on facades. As opposed to LiDAR, deriving 3D point cloud from images require more computational resources, and in many cases, the accuracy and point density might be lower due to poor visual or light conditions. This paper investigates a workflow which utilizes factor graph SLAM, dense 3D reconstruction and ICP to efficiently generate the LiDAR and camera point clouds, and then, co-register in a navigation frame to provide a consistent and more detailed reconstruction of the environment. The workflow consists of three processing steps. First, we use factor graph SLAM, GPS/INS odometry and 6DOF scan matching to register the LiDAR point cloud. Then, the stereo images are processed by stereo-scan dense 3D reconstruction technique to generate dense point cloud. Finally, ICP method is used to co-register LiDAR and photogrammetric point clouds into one frame. The proposed method is tested with the KITTI dataset. The results show that data fusion of two point clouds can improve the quality of the 3D model.},
author = {Yang, Yuan and Koppanyi, Zoltan and Toth, Charles K},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yang, Koppanyi, Toth/Stereo Image Point Cloud and Lidar Point Cloud Fusion for the 3D Street Mapping. Yang, Koppanyi, Toth. 2017.pdf:pdf},
journal = {IGTF 2017 – Imaging Geospatial Technol. Forum 2017-ASPRS Annu. Conf.},
keywords = {ICP,LiDAR point cloud,data fusion,factor graph,stereo image 3D reconstruction},
title = {{Stereo Image Point Cloud and Lidar Point Cloud Fusion for the 3D Street Mapping}},
url = {https://pdfs.semanticscholar.org/7c2e/a37a24b6a09266dd75936b5e4d53953d1179.pdf},
year = {2017}
}
@techreport{vlp16,
institution = {Velodyne LiDAR, Inc.},
title = {{VLP-16 User Manual}},
year = {2019}
}
@book{1385,
author = {Richard, Hartley and Andrew, Zisserman},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hartley, Zisserman/Multiple View Geometry in computer vision. Hartley, Zisserman. 2003.pdf:pdf},
isbn = {9780521540513},
pages = {302},
title = {{Multiple View Geometry in computer vision}},
year = {1385}
}
@article{Wang2018a,
abstract = {We propose a new method for fusing LIDAR point cloud and camera-captured images in deep convolutional neural networks (CNN). The proposed method constructs a new layer called sparse non-homogeneous pooling layer to transform features between bird's eye view and front view. The sparse point cloud is used to construct the mapping between the two views. The pooling layer allows efficient fusion of the multi-view features at any stage of the network. This is favorable for 3D object detection using camera-LIDAR fusion for autonomous driving. A corresponding one-stage detector is designed and tested on the KITTI bird's eye view object detection dataset, which produces 3D bounding boxes from the bird's eye view map. The fusion method shows significant improvement on both speed and accuracy of the pedestrian detection over other fusion-based object detection networks.},
archivePrefix = {arXiv},
arxivId = {1711.06703},
author = {Wang, Zining and Zhan, Wei and Tomizuka, Masayoshi},
doi = {10.1109/IVS.2018.8500387},
eprint = {1711.06703},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wang, Zhan, Tomizuka/Fusing Bird's Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection. Wang, Zhan, Tomizuka. 2018.pdf:pdf},
isbn = {9781538644522},
journal = {IEEE Intell. Veh. Symp. Proc.},
pages = {834--839},
title = {{Fusing Bird's Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection}},
volume = {2018-June},
year = {2018}
}
@article{Fridman2017,
abstract = {For the foreseeble future, human beings will likely remain an integral part of the driving task, monitoring the AI system as it performs anywhere from just over 0{\%} to just under 100{\%} of the driving. The governing objectives of the MIT Autonomous Vehicle Technology (MIT-AVT) study are to (1) undertake large-scale real-world driving data collection that includes high-definition video to fuel the development of deep learning based internal and external perception systems, (2) gain a holistic understanding of how human beings interact with vehicle automation technology by integrating video data with vehicle state data, driver characteristics, mental models, and self-reported experiences with technology, and (3) identify how technology and other factors related to automation adoption and use can be improved in ways that save lives. In pursuing these objectives, we have instrumented 23 Tesla Model S and Model X vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6 vehicles for both long-term (over a year per driver) and medium term (one month per driver) naturalistic driving data collection. Furthermore, we are continually developing new methods for analysis of the massive-scale dataset collected from the instrumented vehicle fleet. The recorded data streams include IMU, GPS, CAN messages, and high-definition video streams of the driver face, the driver cabin, the forward roadway, and the instrument cluster (on select vehicles). The study is on-going and growing. To date, we have 122 participants, 15,610 days of participation, 511,638 miles, and 7.1 billion video frames. This paper presents the design of the study, the data collection hardware, the processing of the data, and the computer vision algorithms currently being used to extract actionable knowledge from the data.},
archivePrefix = {arXiv},
arxivId = {1711.06976},
author = {Fridman, Lex and Brown, Daniel E. and Glazer, Michael and Angell, William and Dodd, Spencer and Jenik, Benedikt and Terwilliger, Jack and Patsekin, Aleksandr and Kindelsberger, Julia and Ding, Li and Seaman, Sean and Mehler, Alea and Sipperley, Andrew and Pettinato, Anthony and Seppelt, Bobbie and Angell, Linda and Mehler, Bruce and Reimer, Bryan},
eprint = {1711.06976},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fridman et al/MIT Autonomous Vehicle Technology Study Large-Scale Deep Learning Based Analysis of Driver Behavior and Interaction with Automation. Fri.pdf:pdf},
pages = {1--16},
title = {{MIT Autonomous Vehicle Technology Study: Large-Scale Deep Learning Based Analysis of Driver Behavior and Interaction with Automation}},
url = {http://arxiv.org/abs/1711.06976},
year = {2017}
}
@article{Geiger2012,
abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
doi = {10.1109/CVPR.2012.6248074},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger, Lenz, Urtasun/Are we ready for autonomous driving the KITTI vision benchmark suite. Geiger, Lenz, Urtasun. 2012.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {3354--3361},
title = {{Are we ready for autonomous driving? the KITTI vision benchmark suite}},
year = {2012}
}
@article{Roger1987,
author = {Tsai, Roger Y},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Tsai/Tsai{\_}Calibration.Pdf. Tsai. 1987.pdf:pdf},
journal = {IEEE J. ofRobotvs Autom.},
title = {{Tsai{\_}Calibration.Pdf}},
volume = {RA-3},
year = {1987}
}
@misc{Cameron,
abstract = {Retrofit a Car for self-driving using LIDAR, camera and Radar},
author = {Cameron, Oliver and Voyage},
keywords = {LIDAR,Self-Driving Cars,Sensor Fusion},
mendeley-tags = {LIDAR,Self-Driving Cars,Sensor Fusion},
title = {{The Story of Homer: Voyage's First Self-Driving Taxi}},
url = {https://news.voyage.auto/the-story-of-homer-voyages-first-self-driving-taxi-f0a6466718af},
urldate = {2018-10-09}
}
@misc{Bezemek2017,
author = {Bezemek, Mike},
booktitle = {Velodyne LiDAR},
title = {{It Began With a Race...16 Years of Velodyne LiDAR}},
url = {https://velodynelidar.com/newsroom/it-began-with-a-race/},
urldate = {2019-10-20},
year = {2017}
}
@article{Guindel2018a,
abstract = {Sensor setups consisting of a combination of 3D range scanner lasers and stereo vision systems are becoming a popular choice for on-board perception systems in vehicles; however, the combined use of both sources of information implies a tedious calibration process. We present a method for extrinsic calibration of lidar-stereo camera pairs without user intervention. Our calibration approach is aimed to cope with the constraints commonly found in automotive setups, such as low-resolution and specific sensor poses. To demonstrate the performance of our method, we also introduce a novel approach for the quantitative assessment of the calibration results, based on a simulation environment. Tests using real devices have been conducted as well, proving the usability of the system and the improvement over the existing approaches. Code is available at http://wiki.ros.org/velo2cam{\_}calibration},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.04085v3},
author = {Guindel, Carlos and Beltran, Jorge and Martin, David and Garcia, Fernando},
doi = {10.1109/ITSC.2017.8317829},
eprint = {arXiv:1705.04085v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Guindel et al/Automatic extrinsic calibration for lidar-stereo vehicle sensor setups. Guindel et al.. 2018(2).pdf:pdf},
isbn = {9781538615256},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
number = {October},
pages = {1--6},
title = {{Automatic extrinsic calibration for lidar-stereo vehicle sensor setups}},
volume = {2018-March},
year = {2018}
}
@article{Rate,
author = {Rate, High Frame},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rate/HDL-64E. Rate. Unknown.pdf:pdf},
pages = {3--4},
title = {{HDL-64E}}
}
@misc{Cameron,
abstract = {Retrofit a Car for self-driving using LIDAR, camera and Radar},
author = {Cameron, Oliver and Voyage},
keywords = {LIDAR,Self-Driving Cars,Sensor Fusion},
mendeley-tags = {LIDAR,Self-Driving Cars,Sensor Fusion},
title = {{The Story of Homer: Voyage's First Self-Driving Taxi}},
url = {https://news.voyage.auto/the-story-of-homer-voyages-first-self-driving-taxi-f0a6466718af},
urldate = {2018-10-09}
}
@article{Bileschi2009,
author = {Bileschi, Stanley},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Bileschi/Fully automatic calibration of LIDAR and video streams from a vehicle The MIT Faculty has made this article openly available . Please.pdf:pdf},
isbn = {9781424444410},
journal = {Comput. Vis. Work. (ICCV Work.},
keywords = {automatic calibration},
mendeley-tags = {automatic calibration},
pages = {1457--1464},
title = {{Fully automatic calibration of LIDAR and video streams from a vehicle The MIT Faculty has made this article openly available . Please share Bileschi , S . “ Fully automatic calibration of LIDAR and video Publisher Version Accessed Citable Link Terms of Us}},
year = {2009}
}
@misc{TomasKrejci,
author = {{Tomas Krejci}},
title = {{GitHub - tomas789/kitti2bag: Convert KITTI dataset to ROS bag file the easy way!}},
url = {https://github.com/tomas789/kitti2bag}
}
@article{Levenberg1943,
abstract = {The standard method for solving least squares problems which lead to non-linear normal equations depends upon a reduction of the residuals to linear form by first order Taylor approximations taken about an initial or trial solution for the parameters .2 If the usual least squares procedure, performed with these linear approximations , yields new values for the parameters which are not sufficiently close to the initial values, the neglect of second and higher order terms may invalidate the process, and may actually give rise to a larger value of the sum of the squares of the residuals than that corresponding to the initial solution. This failure of the standard method to improve the initial solution has received some notice in statistical applications of least squares3 and has been encountered rather frequently in connection with certain engineering applications involving the approximate representation of one function by another. The purpose of this article is to show how the problem may be solved by an extension of the standard method which insures improvement of the initial solution.4 The process can also be used for solving non-linear simultaneous equations, in which case it may be considered an extension of Newton's method.},
author = {Levenberg, Kenneth and Arsenal, Frankford},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Levenberg, Arsenal/A Method for the Solution of Certain Non-Linear Problems in Least Squares. Levenberg, Arsenal. 1943.pdf:pdf},
journal = {Q. Appl. Math.},
number = {278},
pages = {536--538},
title = {{A Method for the Solution of Certain Non-Linear Problems in Least Squares}},
volume = {1},
year = {1943}
}
@article{Steder,
author = {Steder, Bastian},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/The Point Cloud Library PCL Thanks to Radu Rusu from Willow Garage for some of the slides!. Steder.pdf:pdf},
title = {{The Point Cloud Library PCL Thanks to Radu Rusu from Willow Garage for some of the slides!}},
url = {http://ais.informatik.uni-freiburg.de/teaching/ws10/robotics2/pdfs/rob2-12-ros-pcl.pdf}
}
@phdthesis{brabec2014,
address = {Prague},
author = {Brabec, Jan},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Brabec/Automated camera calibration from laser scanning data in natural environments. Brabec. 2014.pdf:pdf},
school = {Czech Technical University in Prague},
title = {{Automated camera calibration from laser scanning data in natural environments}},
year = {2014}
}
@article{Castorena2016,
abstract = {We present a new method for joint automatic extrinsic calibration and sensor fusion for a multimodal sensor system comprising a LIDAR and an optical camera. Our approach exploits the natural alignment of depth and intensity edges when the calibration parameters are correct. Thus, in contrast to a number of existing approaches, we do not require the presence or identification of known alignment targets. On the other hand, the characteristics of each sensor modality, such as sampling pattern and information measured, are significantly different, making direct edge alignment difficult. To overcome this difficulty, we jointly fuse the data and estimate the calibration parameters. In particular, the joint processing evaluates and optimizes both the quality of edge alignment and the performance of the fusion algorithm using a common cost function on the output. We demonstrate accurate calibration in practical configurations in which depth measurements are sparse and contain no reflectivity information. Experiments on synthetic and real data obtained with a three-dimensional LIDAR sensor demonstrate the effectiveness of our approach.},
author = {Castorena, Juan and Kamilov, Ulugbek S. and Boufounos, Petros T.},
doi = {10.1109/ICASSP.2016.7472200},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Castorena, Kamilov, Boufounos/Autocalibration of lidar and optical cameras via edge alignment. Castorena, Kamilov, Boufounos. 2016.pdf:pdf},
isbn = {9781479999880},
issn = {15206149},
journal = {ICASSP, IEEE Int. Conf. Acoust. Speech Signal Process. - Proc.},
keywords = {Multimodal calibration,depth superresolution,intersensor registration,sensor fusion,total variation},
pages = {2862--2866},
publisher = {IEEE},
title = {{Autocalibration of lidar and optical cameras via edge alignment}},
volume = {2016-May},
year = {2016}
}
@misc{Jean_Yves,
author = {Bouguet, Jean-Yves},
title = {{Camera Calibration Toolbox for Matlab}},
url = {http://www.vision.caltech.edu/bouguetj/calib{\_}doc/},
urldate = {2019-10-22}
}
@article{Pereira2016,
abstract = {Autonomous navigation is an important field of research and, given the complexity of real world environments, most of the systems rely on a complex perception system combining multiple sensors on board, which reinforces the concern of sensor calibration. Most calibration methods rely on manual or semi-automatic interactive procedures, but reliable fully automatic methods are still missing. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. The idea proposed in this paper is to use a ball in motion in front of a set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation among all pairs of sensors. This paper proposes and describes such a method with results demonstrating its validity.},
author = {Pereira, Marcelo and Silva, David and Santos, Vitor and Dias, Paulo},
doi = {10.1016/j.robot.2016.05.010},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pereira et al/Self calibration of multiple LIDARs and cameras on autonomous vehicles. Pereira et al.. 2016.pdf:pdf},
issn = {09218890},
journal = {Rob. Auton. Syst.},
keywords = {3D data fitting,Extrinsic calibration,Point cloud},
pages = {326--337},
title = {{Self calibration of multiple LIDARs and cameras on autonomous vehicles}},
volume = {83},
year = {2016}
}
@article{Ishikawa2018,
abstract = {In this paper, we propose a method of targetless and automatic Camera-LiDAR calibration. Our approach is an extension of hand-eye calibration framework to 2D-3D calibration. By using the sensor fusion odometry method, the scaled camera motions are calculated with high accuracy. In addition to this, we clarify the suitable motion for this calibration method. The proposed method only requires the three-dimensional point cloud and the camera image and does not need other information such as reflectance of LiDAR and to give initial extrinsic parameter. In the experiments, we demonstrate our method using several sensor configurations in indoor and outdoor scenes to verify the effectiveness. The accuracy of our method achieves more than other comparable state-of-the-art methods.},
author = {Ishikawa, Ryoichi and Oishi, Takeshi and Ikeuchi, Katsushi},
doi = {10.1109/IROS.2018.8593360},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ishikawa, Oishi, Ikeuchi/LiDAR and Camera Calibration Using Motions Estimated by Sensor Fusion Odometry. Ishikawa, Oishi, Ikeuchi. 2018(2).pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE Int. Conf. Intell. Robot. Syst.},
number = {October 2018},
pages = {7342--7349},
title = {{LiDAR and Camera Calibration Using Motions Estimated by Sensor Fusion Odometry}},
year = {2018}
}
@misc{MarkoBjelonic,
author = {{Marko Bjelonic}},
title = {{GitHub - leggedrobotics/darknet{\_}ros: YOLO ROS: Real-Time Object Detection for ROS}},
url = {https://github.com/leggedrobotics/darknet{\_}ros https://wiki.ros.org/darknet{\_}ros},
urldate = {2019-04-10}
}
@article{Hata,
author = {Hata, Kenji and Savarese, Silvio},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hata, Savarese/CS231A Course Notes 1 Camera Models. Hata, Savarese. Unknown.pdf:pdf},
pages = {1--16},
title = {{CS231A Course Notes 1: Camera Models}}
}
@article{nuScenes2019,
archivePrefix = {arXiv},
arxivId = {1903.11027},
author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
eprint = {1903.11027},
journal = {arXiv Prepr.},
title = {{nuScenes: A multimodal dataset for autonomous driving}},
url = {https://www.nuscenes.org/},
year = {2019}
}
@article{Geiger2012a,
abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera- to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experi- ments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions. I.},
author = {Geiger, Andreas and Moosmann, Frank and Car, {\"{O}}mer and Schuster, Bernhard},
doi = {10.1109/ICRA.2012.6224570},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger et al/Automatic camera and range sensor calibration using a single shot. Geiger et al.. 2012.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {3936--3943},
title = {{Automatic camera and range sensor calibration using a single shot}},
year = {2012}
}
@article{camera_models,
author = {Hata, Kenji and Savarese, Silvio},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hata, Savarese/CS231A Course Notes 1 Camera Models. Hata, Savarese. Unknown.pdf:pdf},
pages = {1--16},
title = {{CS231A Course Notes 1: Camera Models}}
}
@misc{PaulA.Hughes,
author = {{Paul A. Hughes}},
booktitle = {1996},
title = {{History of the Electric Car: 1828 - 1912, from Trouve to Morrison}},
url = {https://web.archive.org/web/20111113023143/http://factoidz.com/history-of-the-electric-car-1828-1912-from-trouve-to-morrison/},
urldate = {2019-10-20}
}
@misc{DailyNews2013,
author = {{Daily News}},
title = {{Ford's assembly line turns 100: How it changed manufacturing and society - NY Daily News}},
url = {https://web.archive.org/web/20131130021237/http://www.nydailynews.com/autos/ford-assembly-line-turns-100-changed-society-article-1.1478331},
urldate = {2019-10-20},
year = {2013}
}
@article{Fersch2017,
abstract = {{\textcopyright} 2017 IEEE. We present the application of optical code division multiple access (OCDMA) modulation based on optical orthogonal codes for automotive time-of-flight light detection and ranging (LiDAR) system sensors using avalanche photodiode (APD) detectors. The modulation opens the possibility to discriminate single laser transmissions. This allows the realization of additional features like enhancing the systems' interference robustness or accelerating their scanning rate. The requirements on automotive LiDAR OCDMA modulation differs from telecommunication's demands in several ways: The sensor must guarantee absolute laser safety; front facing devices must be able to provide reliable long range results up to a distance of 150m and beyond; and the signal is transmitted through free space. After outlining the basic functionality of the sort of sensors in consideration, the properties of the optical orthogonal codes (OOC) modulation are compared with the state of the art theoretically. The influence of OOC parameters with respect to scanning LiDAR systems is examined. The modulation technique is then demonstrated experimentally with two examples: The detection and separation of coded and traditional signals is shown using a matched filter detection algorithm. In the same way, differently coded signals can be separated from each other. Impediments of the interference suppression quality are discussed. Finally, an overview of possible applications of the proposed technique in automotive LiDAR systems is given, which are enabled by the OCDMA modulation in the first place.},
author = {Fersch, Thomas and Weigel, Robert and Koelpin, Alexander},
doi = {10.1109/JSEN.2017.2688126},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems. Fersch, Weigel, Koelpin.pdf:pdf},
issn = {1530-437X},
journal = {IEEE Sens. J.},
keywords = {Laser radar,LiDAR,OCDMA modulation,time-of-flight},
month = {jun},
number = {11},
pages = {3507--3516},
title = {{A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems}},
url = {http://ieeexplore.ieee.org/document/7887680/},
volume = {17},
year = {2017}
}
@misc{CameronOliver2017a,
abstract = {Why did LIDAR take off with self-driving cars? In a word: mapping. LIDAR allows you to generate huge 3D maps (its original application!), which you can then navigate the car or robot predictably within. By using a LIDAR to map and navigate an environment, you can know ahead of time the bounds of a lane, or that there is a stop sign or traffic light 500m ahead. This kind of predictability is exactly what a technology like self-driving cars requires, and has been a big reason for the progress over the last 5 years.
},
author = {{Cameron Oliver}},
booktitle = {Voyage},
keywords = {LIDAR,Self-Driving Cars},
mendeley-tags = {LIDAR,Self-Driving Cars},
title = {{An Introduction to LIDAR: The Key Self-Driving Car Sensor}},
url = {https://news.voyage.auto/an-introduction-to-lidar-the-key-self-driving-car-sensor-a7e405590cff},
urldate = {2018-10-09},
year = {2017}
}
@inproceedings{Kim2015,
abstract = {LIDAR scanners are essential components of intelligent vehicles capable of autonomous travel. Mutual interference between LIDAR scanners has not been regarded as a problem yet. Mutual interference was identified as a problem of increased importance because of the appearance of safety functions and the increasing rate of vehicles equipped with LIDAR scanner. This paper will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some generic interference scenarios and report on the current status of the analysis of interference mechanisms.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
booktitle = {2015 12th Int. Conf. Inf. Technol. - New Gener.},
doi = {10.1109/ITNG.2015.113},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/An Experiment of Mutual Interference between Automotive LIDAR Scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {978-1-4799-8828-0},
keywords = {LIDAR scanner,autonomous vehicle,ghost target,mutual interference,obstacle detection},
month = {apr},
pages = {680--685},
publisher = {IEEE},
title = {{An Experiment of Mutual Interference between Automotive LIDAR Scanners}},
url = {http://ieeexplore.ieee.org/document/7113553/},
year = {2015}
}
@article{Gavin2019,
abstract = {The Levenberg-Marquardt algorithm was developed in the early 1960's to solve ne onlinear least squares problems. Least squares problems arise in the context of fitting a parameterized function to a set of measured data points by minimizing the sum of the squares of the errors between the data points and the function. If the fit function is not linear in the parameters the least squares problem is nonlinear. Nonlinear least squares methods iteratively reduce the sum of the squares of the errors between the function and the measured data points through a sequence of updates to parameter values. The Levenberg-Marquardt algorithm combines two minimization methods: the gradient descent method and the Gauss-Newton method. In the gradient descent method, the sum of the squared errors is reduced by updating the parameters in the steepest-descent direction. In the Gauss-Newton method, the sum of the squared errors is reduced by assuming the least squares function is locally quadratic, and finding the minimum of the quadratic. The Levenberg-Marquardt method acts more like a gradient-descent method when the parameters are far from their optimal value, and acts more like the Gauss-Newton method when the parameters are close to their optimal value. This document describes these methods and illustrates the use of software to solve nonlinear least squares curve-fitting problems.},
author = {Gavin, Henri P.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gavin/The Levenburg-Marqurdt Algorithm For Nonlinear Least Squares Curve-Fitting Problems. Gavin. 2019.pdf:pdf},
journal = {Duke Univ.},
pages = {1--19},
title = {{The Levenburg-Marqurdt Algorithm For Nonlinear Least Squares Curve-Fitting Problems}},
url = {http://people.duke.edu/{~}hpgavin/ce281/lm.pdf},
year = {2019}
}
@article{Kim2017,
author = {Kim, Gunzung and Eom, Jeongsook and Choi, Jeonghee and Park, Yongwan},
doi = {10.14372/iemek.2017.12.1.43},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Mutual Interference on Mobile Pulsed Scanning LIDAR. Kim et al.. 2017.pdf:pdf},
issn = {1975-5066},
journal = {IEMEK J. Embed. Syst. Appl.},
month = {feb},
number = {1},
pages = {43--62},
title = {{Mutual Interference on Mobile Pulsed Scanning LIDAR}},
url = {http://koreascience.or.kr/journal/view.jsp?kj=OBDDBE{\&}py=2017{\&}vnc=v12n1{\&}sp=43},
volume = {12},
year = {2017}
}
@article{MartinVelas2013,
abstract = {Quinolone-resistant clinical Escherichia coli isolates were examined for mutations in the marRAB operon of the multiple antibiotic resistance (mar) locus. Among 23 strains evaluated, 8 were chosen for further study: 3 that showed relatively high levels of uninduced, i.e., constitutive, expression of the operon and 5 with variable responses to induction by salicylate or tetracyclines. The marR genes, specifying the repressor of the operon, cloned from the three strains constitutively expressing the operon did not reduce the level of expression of beta-galactosidase from a marO::lacZ transcriptional fusion and were therefore mutant; however, marR genes cloned from the five other clinical strains repressed LacZ expression and were wild type. All three mutant marR genes contained more than one mutation: a deletion and a point mutation. Inactivation of the mar locus in the three known marR mutant strains with a kanamycin resistance cassette introduced by homologous recombination reduced resistance to quinolones and multiple antibiotics. These findings indicate that mar operon mutations exist in quinolone-resistant clinical E. coli isolates and contribute to quinolone and multidrug resistance.},
archivePrefix = {arXiv},
arxivId = {1505.00729},
author = {Velas, Martin and Spanel, Michal and Materna, Zdenek and Herout, Adam},
doi = {10.1016/B978-0-08-098242-7.00008-0},
eprint = {1505.00729},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Martin Velas, Michal Spanel, Zdenek Materna/Calibration of RGB Camera With Velodyne LiDAR. Martin Velas, Michal Spanel, Zdenek Materna. 2013.pdf:pdf},
isbn = {9781461434801},
issn = {0066-4804},
journal = {Flight Dyn. Princ.},
keywords = {L{\'{i}}DAR,Velodyne,calibration,camera,marker},
month = {jul},
number = {7},
pages = {219--241},
pmid = {8807064},
publisher = {Elsevier},
title = {{Calibration of RGB Camera With Velodyne LiDAR}},
url = {https://linkinghub.elsevier.com/retrieve/pii/B9780080982427000080},
volume = {40},
year = {2013}
}
@article{Roca2006a,
archivePrefix = {arXiv},
arxivId = {arXiv:1208.5721},
author = {Roca, Sergi Figueras and Cited, References},
doi = {10.1038/incomms1464},
eprint = {arXiv:1208.5721},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Roca, Cited/( 12 ) United States Patent. Roca, Cited. 2006.pdf:pdf},
isbn = {1020080012051},
issn = {2004001828},
number = {12},
pmid = {1000182772},
title = {{( 12 ) United States Patent}},
volume = {2},
year = {2006}
}
@misc{CameronOliver2017,
abstract = {Why did LIDAR take off with self-driving cars? In a word: mapping. LIDAR allows you to generate huge 3D maps (its original application!), which you can then navigate the car or robot predictably within. By using a LIDAR to map and navigate an environment, you can know ahead of time the bounds of a lane, or that there is a stop sign or traffic light 500m ahead. This kind of predictability is exactly what a technology like self-driving cars requires, and has been a big reason for the progress over the last 5 years.
},
author = {{Cameron Oliver}},
booktitle = {Voyage},
keywords = {LIDAR,Self-Driving Cars},
mendeley-tags = {LIDAR,Self-Driving Cars},
title = {{An Introduction to LIDAR: The Key Self-Driving Car Sensor}},
url = {https://news.voyage.auto/an-introduction-to-lidar-the-key-self-driving-car-sensor-a7e405590cff},
urldate = {2018-10-09},
year = {2017}
}
@article{Bimbraw2015,
author = {Bimbraw, K},
doi = {10.5220/0005540501910198},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Bimbraw/Autonomous Cars Past , Present and Future - A Review of the Developments in the Last Century , the Present .... Bimbraw. 2015.pdf:pdf},
isbn = {9789897581229},
journal = {12th Int. Conf. Informatics Control. Autom. Robot.},
keywords = {accomplished in this,and much has been,automation,automation is of interest,autonomous cars,autonomous vehicles,cars,intelligent transportation,mechatronics systems,technologies and systems,the field of autonomous,to researchers},
number = {August},
pages = {191--198},
title = {{Autonomous Cars : Past , Present and Future - A Review of the Developments in the Last Century , the Present ...}},
year = {2015}
}
@misc{OpenCV_camera_calib,
author = {OpenCV},
title = {{OpenCV: Camera calibration With OpenCV}},
url = {https://docs.opencv.org/3.2.0/d4/d94/tutorial{\_}camera{\_}calibration.html https://opencv-python-tutroals.readthedocs.io/en/latest/py{\_}tutorials/py{\_}calib3d/py{\_}calibration/py{\_}calibration.html},
urldate = {2019-10-22}
}
@misc{JohnSteeleGordon2007,
author = {{John Steele Gordon}},
title = {{AmericanHeritage.com / 10 Moments That Made American Business}},
url = {https://web.archive.org/web/20080420194514/http://americanheritage.com/articles/magazine/ah/2007/1/2007{\_}1{\_}23.shtml},
urldate = {2019-10-20},
year = {2007}
}
@article{Sullivan2016,
abstract = {Analysis of LiDAR technology for Advanced Safety Preface Safety and its improvement is a concern paramount to all passenger vehicle manufacturers. The value proposition for driver, passenger and pedestrian safety has become equally important as engine performance and fuel economy. This paper addresses how light detection and range (LiDAR) technology will impact Advanced Driver Assistance Systems and explore the term, " autonomous driving intelligence. " The paper will consider Lidar technology today and how it stands to capture a large market share of automotive sensor technology in the future. Lidar is poised to penetrate the market in 2016 as the lead technology in automotive safety and autonomous systems. Three primary approaches to Lidar development are considered, including Hybrid Solid-State Lidar, MEMS Lidar, and Mechanical Mechanism Lidar. Mechanical mechanism Lidar is the oldest and most traditional technology. MEMS Lidar technology is in the beginning stage of development as a low cost Lidar solution for low level automotive safety. Solid-state Hybrid Lidar (SH Lidar) was introduced in 2005 as a result of the Darpa Robotic Car Races. The technology has been tested for autonomous safety over the years and the cost for SH Lidar dropped dramatically in 2015. With planned mass production to meet the growing demand for autonomous navigation and advanced safety, further dramatic cost reduction is expected in 2016 – 2017. The development of Solid-State Hybrid Lidar (SH Lidar) was a break away from the traditional mechanical mechanism of single Lidar technology, and it is described in detail in this paper. The technology simplified what was previously a complex mechanical system of parts into one robust solid-state part. The solid-state developmental enabled faster data capture in 3D, capturing pictures instantaneously while moving in real-time at speeds of 30-40MPH. SH Lidar technology has matured from an extremely costly technology and large system to being affordable, smaller in size, and headed toward mass production. SH Lidar technology is poised to be commercialized in 2015-16 and radically change the way we move about in the world.},
author = {Sullivan, Frost {\&}},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Sullivan/LiDAR Driving the Future of Autonomous Navigation. Sullivan. 2016.pdf:pdf},
pages = {1--30},
title = {{LiDAR: Driving the Future of Autonomous Navigation}},
year = {2016}
}
@misc{opencv_doc,
author = {OpenCV},
title = {{Camera Calibration and 3D Reconstruction — OpenCV 2.4.13.7 documentation}},
url = {https://docs.opencv.org/2.4/modules/calib3d/doc/camera{\_}calibration{\_}and{\_}3d{\_}reconstruction.html},
urldate = {2019-04-21}
}
@article{Jeong2019,
abstract = {This paper presents a framework for the targetless extrinsic calibration of stereo cameras and Light Detection and Ranging (LiDAR) sensors with a non-overlapping Field of View (FOV). In order to solve the extrinsic calibrations problem under such challenging configuration, the proposed solution exploits road markings as static and robust features among the various dynamic objects that are present in urban environment. First, this study utilizes road markings that are commonly captured by the two sensor modalities to select informative images for estimating the extrinsic parameters. In order to accomplish stable optimization, multiple cost functions are defined, including Normalized Information Distance (NID), edge alignment and, plane fitting cost. Therefore a smooth cost curve is formed for global optimization to prevent convergence to the local optimal point. We further evaluate each cost function by examining parameter sensitivity near the optimal point. Another key characteristic of extrinsic calibration, repeatability, is analyzed by conducting the proposed method multiple times with varying randomly perturbed initial points.},
archivePrefix = {arXiv},
arxivId = {1902.10586},
author = {Jeong, Jinyong and Cho, Lucas Y. and Kim, Ayoung},
eprint = {1902.10586},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Jeong, Cho, Kim/Road is Enough! Extrinsic Calibration of Non-overlapping Stereo Camera and LiDAR using Road Information. Jeong, Cho, Kim. 2019.pdf:pdf},
title = {{Road is Enough! Extrinsic Calibration of Non-overlapping Stereo Camera and LiDAR using Road Information}},
url = {http://arxiv.org/abs/1902.10586},
year = {2019}
}
@inproceedings{Popko2019,
abstract = {Signal interference between two light detection and ranging (lidar) scanners can occur when the transmitted laser energy from one lidar is scattered from a target and returned to a second lidar. By modeling lidar transmission paths via ray tracing, it is shown that signal interference can be modeled by the coincidence of intersection between two lidar transmission paths and a scattering target. The evaluation of experimental observation and an analytical framework of lidar signal interference is presented that compares results of a Monte Carlo simulation to interference observations from circularly scanning lidar sensors. The comparison between simulated and experimentally observed interference events suggests that lidar interference may largely explained by geometry and angular conditions. The model provides preliminary explanation as to the angular distribution of interference events and distinct transitions between occurrences of different interference modes. However, further radiometric refinement is likely needed to best explain the manifestation of some interference events.},
author = {Popko, Gerald B. and Gaylord, Thomas K. and Valenta, Christopher R.},
booktitle = {Laser Radar Technol. Appl. XXIV},
doi = {10.1117/12.2518228},
editor = {Turner, Monte D. and Kamerman, Gary W.},
isbn = {9781510626751},
keywords = {autonomous vehicles,ladar,lidar,point clouds,signal interference},
month = {may},
pages = {18},
publisher = {SPIE},
title = {{Signal interactions between lidar scanners}},
url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11005/2518228/Signal-interactions-between-lidar-scanners/10.1117/12.2518228.full},
volume = {11005},
year = {2019}
}
@book{Xu1996,
author = {Xú, Gang. and Zhang, Zhengyou},
isbn = {0792341996},
pages = {313},
publisher = {Kluwer Academic Publishers},
title = {{Epipolar geometry in stereo, motion, and object recognition : a unified approach}},
url = {https://books.google.com/books?id=DnFaUidM-B0C{\&}pg=PA7{\&}dq=pinhole+intitle:{\%}22Epipolar+geometry{\%}22},
year = {1996}
}
@article{Wang2018a,
abstract = {We propose a new method for fusing LIDAR point cloud and camera-captured images in deep convolutional neural networks (CNN). The proposed method constructs a new layer called sparse non-homogeneous pooling layer to transform features between bird's eye view and front view. The sparse point cloud is used to construct the mapping between the two views. The pooling layer allows efficient fusion of the multi-view features at any stage of the network. This is favorable for 3D object detection using camera-LIDAR fusion for autonomous driving. A corresponding one-stage detector is designed and tested on the KITTI bird's eye view object detection dataset, which produces 3D bounding boxes from the bird's eye view map. The fusion method shows significant improvement on both speed and accuracy of the pedestrian detection over other fusion-based object detection networks.},
archivePrefix = {arXiv},
arxivId = {1711.06703},
author = {Wang, Zining and Zhan, Wei and Tomizuka, Masayoshi},
doi = {10.1109/IVS.2018.8500387},
eprint = {1711.06703},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wang, Zhan, Tomizuka/Fusing Bird's Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection. Wang, Zhan, Tomizuka. 2018.pdf:pdf},
isbn = {9781538644522},
journal = {IEEE Intell. Veh. Symp. Proc.},
pages = {834--839},
title = {{Fusing Bird's Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection}},
volume = {2018-June},
year = {2018}
}
@misc{Waymo,
author = {Waymo},
title = {{Open Dataset – Waymo}},
url = {https://waymo.com/open/},
urldate = {2019-10-20}
}
@inproceedings{Javaheri2017,
abstract = {The increasing availability of point cloud data in recent years is demanding for high performance denoising methods and compression schemes. When point cloud data is directly obtained from depth sensors or extracted from images acquired from different viewpoints, imprecisions on the depth acquisition or in the 3D reconstruction techniques result in noisy point clouds which may include a significant number of outliers. Moreover, the quality assessment of point clouds is a challenging problem since this 3D representation format is unstructured and it is typically not directly visualized. In this paper, selected objective quality metrics are evaluated regarding their correlation with human quality assessment and thus human perception. As far as the authors know, this is the first paper performing the subjective assessment of point cloud denoising algorithms and the evaluation of most used point cloud objective quality metrics. Experimental results show that graph-based denoising algorithms can improve significantly the point cloud quality data and that objective metrics that model the underlying point cloud surface can correlate better with human perception.},
author = {Javaheri, Alireza and Brites, Catarina and Pereira, Fernando and Ascenso, Jo{\~{a}}o},
booktitle = {2017 IEEE 19th Int. Work. Multimed. Signal Process. MMSP 2017},
doi = {10.1109/MMSP.2017.8122239},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Javaheri et al/Subjective and objective quality evaluation of compressed point clouds. Javaheri et al.. 2017.pdf:pdf},
isbn = {9781509036493},
issn = {1047-2797},
keywords = {Point cloud compression,Quality metrics,Subjective quality assessment},
pages = {1--6},
pmid = {10037558},
title = {{Subjective and objective quality evaluation of compressed point clouds}},
volume = {2017-Janua},
year = {2017}
}
@misc{Photopillers,
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Cladera/The Ultimate Photography Guide to Depth of Field (DoF) PhotoPills. Cladera. Unknown.pdf:pdf},
title = {{The Ultimate Photography Guide to Depth of Field (DoF) | PhotoPills}},
url = {https://www.photopills.com/articles/ultimate-guide-depth-field},
urldate = {2019-10-21}
}
@misc{dlib,
title = {{dlib C++ Library}},
url = {http://dlib.net/},
urldate = {2019-10-22}
}
@article{Yue2018a,
abstract = {3D LiDAR scanners are playing an increasingly important role in autonomous driving as they can generate depth information of the environment. However, creating large 3D LiDAR point cloud datasets with point-level labels requires a significant amount of manual annotation. This jeopardizes the efficient development of supervised deep learning algorithms which are often data-hungry. We present a framework to rapidly create point clouds with accurate point-level labels from a computer game. The framework supports data collection from both auto-driving scenes and user-configured scenes. Point clouds from auto-driving scenes can be used as training data for deep learning algorithms, while point clouds from user-configured scenes can be used to systematically test the vulnerability of a neural network, and use the falsifying examples to make the neural network more robust through retraining. In addition, the scene images can be captured simultaneously in order for sensor fusion tasks, with a method proposed to do automatic calibration between the point clouds and captured scene images. We show a significant improvement in accuracy (+9{\%}) in point cloud segmentation by augmenting the training dataset with the generated synthesized data. Our experiments also show by testing and retraining the network using point clouds from user-configured scenes, the weakness/blind spots of the neural network can be fixed.},
archivePrefix = {arXiv},
arxivId = {1804.00103},
author = {Yue, Xiangyu and Wu, Bichen and Seshia, Sanjit A. and Keutzer, Kurt and Sangiovanni-Vincentelli, Alberto L.},
doi = {10.1145/3206025.3206080},
eprint = {1804.00103},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yue et al/A LiDAR Point Cloud Generator. Yue et al.. 2018.pdf:pdf},
isbn = {9781450350464},
journal = {Proc. 2018 ACM Int. Conf. Multimed. Retr.  - ICMR '18},
number = {Nips},
pages = {458--464},
title = {{A LiDAR Point Cloud Generator}},
url = {http://dl.acm.org/citation.cfm?doid=3206025.3206080},
year = {2018}
}
@techreport{Rev,
address = {San Jose},
author = {Rev, E},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rev/VLP-16 User Manual. Rev. 2019.pdf:pdf},
institution = {Velodyne LiDAR, Inc.},
pages = {71},
title = {{VLP-16 User Manual}},
year = {2019}
}
@article{Liao2019,
abstract = {Fusion of heterogeneous exteroceptive sensors is the most efficient and effective path to the representation of the environment precisely, as it can compromise various drawbacks of each homogeneous sensor. The rigid transformation (aka. extrinsic parameters) of heterogeneous sensory systems is the prerequisite of fusing the multi-sensor information. Researchers have proposed several approaches to estimate the extrinsic parameters. However, these approaches neither rely on human interventions or specifically designed auxiliary object or do not provide the library which makes it hard to test or benchmark. In this paper, we propose a novel extrinsic calibration approach for the extrinsic calibration of a Lidar (Laser Range Finder) and a camera which only based on a polygon board and we offer the relevant tools. In this paper, we firstly track and extract the target polygon from both the image and point-cloud. Then we try to match the polygon between the 2D and 3D feature spaces. With the associated polygon, we are able to get multiple constraints to optimize the extrinsic parameters. At the end, we validate our approach by four configurations , including the simulation, 16/32-beam Lidar and 100-line MEMS-Lidar. The outcome indicates high-precision extrinsic calibration performance.},
author = {Liao, Qinghai and Chen, Zhenyong and Liu, Yang and Wang, Zhe and Liu, Ming},
doi = {10.1109/ROBIO.2018.8665256},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Liao et al/Extrinsic Calibration of Lidar and Camera with Polygon. Liao et al.. 2019.pdf:pdf},
isbn = {9781728103761},
journal = {2018 IEEE Int. Conf. Robot. Biomimetics, ROBIO 2018},
pages = {200--205},
publisher = {IEEE},
title = {{Extrinsic Calibration of Lidar and Camera with Polygon}},
year = {2019}
}
@article{VelodyneHDL64,
author = {Rate, High Frame},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rate/HDL-64E. Rate. Unknown.pdf:pdf},
pages = {3--4},
title = {{HDL-64E}}
}
@book{Setright2003,
abstract = {Previously published: Palawan Press, 2002. Includes index. pt. 1. An historical review. -- To 1895 -- 1896-1905 -- 1906-15 -- 1916-25 -- 1926-35 -- 1936-45 -- 1946-55 -- 1956-65 -- 1966-75 -- 1976-85 -- From 1986 ... -- pt. 2. The yoke and the spur. -- Extortion and distortion -- Small expectations: the decadent history of a decaying idea -- Scapegoat and idol -- The liberator -- pt. 3. The face of the earth. -- The big city -- Somewhere to stop -- pt. 4. The turn of the wheel: revolutions in technology. -- The nature of revolution -- ForWarD -- The can ... of worms? -- The popular front -- Re-inventing the wheel -- Inconstant mesh -- Tyres and timing -- Fear of flying -- Fluid power -- Computer control -- The air inside -- Four-wheel equality -- pt. 5. Personal effects. -- Making a start -- Changing gears in changing times -- What to wear -- Sport -- Arts and fashions -- Appendix: Timescale.},
author = {Setright, L. J. K.},
isbn = {1862076987},
pages = {405},
publisher = {Granta},
title = {{Drive on! : a social history of the motor car}},
year = {2003}
}
@article{Wei2018,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@article{Zhengyou2000,
abstract = {We propose a flexible new technique to easily calibrate a camera. It is well suited for use without specialized knowledge of 3D geometry or computer vision. The technique only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique, and very good results have been obtained. Compared with classical techniques which use expensive equipments such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one step from laboratory environments to real world use.},
author = {Zhengyou, Z},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Zhengyou/A Flexible New Technique for Camera Calibration. Zhengyou. 2000.pdf:pdf},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
number = {11},
pages = {1330--1334},
title = {{A Flexible New Technique for Camera Calibration}},
url = {https://www.microsoft.com/en-us/research/publication/a-flexible-new-technique-for-camera-calibration/},
volume = {22},
year = {2000}
}
@article{Geiger2013a,
abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high- resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to inner- city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide},
author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger et al/Vision meets robotics The KITTI dataset. The International Journal of Robotics Research. Geiger et al.. 2013.pdf:pdf},
journal = {Ijrr},
number = {October},
pages = {1--6},
title = {{Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research}},
year = {2013}
}
@article{Guindel2018,
abstract = {Sensor setups consisting of a combination of 3D range scanner lasers and stereo vision systems are becoming a popular choice for on-board perception systems in vehicles; however, the combined use of both sources of information implies a tedious calibration process. We present a method for extrinsic calibration of lidar-stereo camera pairs without user intervention. Our calibration approach is aimed to cope with the constraints commonly found in automotive setups, such as low-resolution and specific sensor poses. To demonstrate the performance of our method, we also introduce a novel approach for the quantitative assessment of the calibration results, based on a simulation environment. Tests using real devices have been conducted as well, proving the usability of the system and the improvement over the existing approaches. Code is available at http://wiki.ros.org/velo2cam{\_}calibration},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.04085v3},
author = {Guindel, Carlos and Beltran, Jorge and Martin, David and Garcia, Fernando},
doi = {10.1109/ITSC.2017.8317829},
eprint = {arXiv:1705.04085v3},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Guindel et al/Automatic extrinsic calibration for lidar-stereo vehicle sensor setups. Guindel et al.. 2018.pdf:pdf},
isbn = {9781538615256},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
pages = {1--6},
title = {{Automatic extrinsic calibration for lidar-stereo vehicle sensor setups}},
volume = {2018-March},
year = {2018}
}
@inproceedings{Kim2015,
abstract = {LIDAR scanners are essential components of intelligent vehicles capable of autonomous travel. Mutual interference between LIDAR scanners has not been regarded as a problem yet. Mutual interference was identified as a problem of increased importance because of the appearance of safety functions and the increasing rate of vehicles equipped with LIDAR scanner. This paper will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some generic interference scenarios and report on the current status of the analysis of interference mechanisms.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
booktitle = {2015 12th Int. Conf. Inf. Technol. - New Gener.},
doi = {10.1109/ITNG.2015.113},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/An Experiment of Mutual Interference between Automotive LIDAR Scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {978-1-4799-8828-0},
keywords = {LIDAR scanner,autonomous vehicle,ghost target,mutual interference,obstacle detection},
month = {apr},
pages = {680--685},
publisher = {IEEE},
title = {{An Experiment of Mutual Interference between Automotive LIDAR Scanners}},
url = {http://ieeexplore.ieee.org/document/7113553/},
year = {2015}
}
@article{Roca2006,
archivePrefix = {arXiv},
arxivId = {arXiv:1208.5721},
author = {Roca, Sergi Figueras and Cited, References},
doi = {10.1038/incomms1464},
eprint = {arXiv:1208.5721},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Roca, Cited/( 12 ) United States Patent. Roca, Cited. 2006.pdf:pdf},
isbn = {1020080012051},
issn = {2004001828},
number = {12},
pmid = {1000182772},
title = {{( 12 ) United States Patent}},
volume = {2},
year = {2006}
}
@article{WHO2018,
abstract = {As matter fact road traffic injuries are currently ranked ninth globally among the leading causes of disability adjusted life years lost, and the ranking is projected to rise to third by 2020(Ishrat Riaz, 2018)},
author = {WHO},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/WHO/Global Status Report on Road. WHO. 2018.pdf:pdf},
journal = {World Heal. Organ.},
pages = {20},
title = {{Global Status Report on Road}},
url = {https://www.who.int/violence{\_}injury{\_}prevention/road{\_}safety{\_}status/2018/en/},
year = {2018}
}
@misc{opencv,
title = {{OpenCV}},
url = {https://opencv.org/},
urldate = {2019-10-22}
}
@article{Wei2018,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@misc{Ford,
author = {Ford},
title = {{Ford Motor Company Timeline | Ford.com}},
url = {https://corporate.ford.com/history.html},
urldate = {2019-10-20}
}
@misc{simplecv,
title = {{SimpleCV Tutorial — Tutorial}},
url = {http://tutorial.simplecv.org/en/latest/},
urldate = {2019-10-22}
}
@article{Pandey2011,
abstract = {In this paper we describe a data set collected by an autonomous ground vehicle testbed, based upon a modified Ford F-250 pickup truck. The vehicle is outfitted with a professional (Applanix POS-LV) and consumer (Xsens MTi-G) inertial measurement unit, a Velodyne three-dimensional lidar scanner, two push-broom forward-looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system. Here we present the time-registered data from these sensors mounted on the vehicle, collected while driving the vehicle around the Ford Research Campus and downtown Dearborn, MI, during November-December 2009. The vehicle path trajectory in these data sets contains several large- and small-scale loop closures, which should be useful for testing various state-of-the-art computer vision and simultaneous localization and mapping algorithms. {\textcopyright} SAGE Publications 2011.},
author = {Pandey, Gaurav and McBride, James R. and Eustice, Ryan M.},
doi = {10.1177/0278364911400640},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pandey, McBride, Eustice/Ford Campus vision and lidar data set. Pandey, McBride, Eustice. 2011.pdf:pdf},
issn = {17413176},
journal = {Int. J. Rob. Res.},
keywords = {Mobile and distributed robotics SLAM,field and service robotics,sensing and perception computer vision},
number = {13},
pages = {1543--1552},
title = {{Ford Campus vision and lidar data set}},
url = {http://robots.engin.umich.edu/SoftwareData/Ford},
volume = {30},
year = {2011}
}
@inproceedings{Hebel2018,
abstract = {In this paper, we examine crosstalk effects that can arise in multi-LiDAR configurations, and we show a data-based approach to mitigate these effects. Due to the ability to acquire precise 3D data of the environment, LiDAR-based sensor systems (sensors based on “Light Detection and Ranging”, e.g., laser scanners) increasingly find their way into various applications, e.g. in the automotive sector. However, with an increasing number of LiDAR sensors operating within close vicinity, the problem of potential crosstalk between these devices arises. “Crosstalk” outlines the following effect: In a typical LiDAR-based sensor, short laser pulses are emitted into the scene and the distance between sensor and object is derived from the time measured until an “echo” is received. In case multiple laser pulses of the same wavelength are emitted at the same time, the detector may not be able to distinguish between correct and false matches of laser pulses and echoes, resulting in erroneous range measurements and 3D points. During operation of our own multi-LiDAR sensor system, we were able to observe crosstalk effects in the acquired data. Having compared different spatial filtering approaches for the elimination of erroneous points in the 3D data, we propose a data-based spatio-temporal filtering and show its results, which may be sufficient depending on the application. However, technical solutions are desired for future LiDAR sensors.},
author = {Hebel, Marcus and Hammer, Marcus and Arens, Michael and Diehm, Axel L.},
booktitle = {Electro-Optical Remote Sens. XII},
doi = {10.1117/12.2324305},
editor = {Kamerman, Gary and Steinvall, Ove},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hebel et al/Mitigation of crosstalk effects in multi-LiDAR configurations. Hebel et al.. 2018.pdf:pdf},
isbn = {9781510621756},
keywords = {Crosstalk,LADAR,Laser radar,Laser scanning,LiDAR,Multi-sensor systems,Mutual interference,Temporal filtering},
month = {oct},
pages = {3},
publisher = {SPIE},
title = {{Mitigation of crosstalk effects in multi-LiDAR configurations}},
url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10796/2324305/Mitigation-of-crosstalk-effects-in-multi-LiDAR-configurations/10.1117/12.2324305.full},
volume = {10796},
year = {2018}
}
@inproceedings{ADAS1,
author = {Okuda, R and Kajiwara, Y and Terashima, K},
booktitle = {Tech. Pap. 2014 Int. Symp. VLSI Des. Autom. Test},
title = {{A survey of technical trend of ADAS and autonomous driving}}
}
@article{Pusztai2018a,
abstract = {As autonomous driving attracts more and more attention these days, the algorithms and sensors used for machine perception become popular in research, as well. This paper investigates the extrinsic calibration of two frequently-applied sensors: the camera and Light Detection and Ranging (LiDAR). The calibration can be done with the help of ordinary boxes. It contains an iterative refinement step, which is proven to converge to the box in the LiDAR point cloud, and can be used for system calibration containing multiple LiDARs and cameras. For that purpose, a bundle adjustment-like minimization is also presented. The accuracy of the method is evaluated on both synthetic and real-world data, outperforming the state-of-the-art techniques. The method is general in the sense that it is both LiDAR and camera-type independent, and only the intrinsic camera parameters have to be known. Finally, a method for determining the 2D bounding box of the car chassis from LiDAR point clouds is also presented in order to determine the car body border with respect to the calibrated sensors.},
author = {Pusztai, Zolt{\'{a}}n and Eichhardt, Iv{\'{a}}n and Hajder, Levente},
doi = {10.3390/s18072139},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pusztai, Eichhardt, Hajder/Accurate calibration of multi-lidar-multi-camera systems. Pusztai, Eichhardt, Hajder. 2018.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Autonomous driving,Camera,Extrinsic calibration,LiDAR,LiDAR camera system,Machine perception},
number = {7},
pages = {1--22},
title = {{Accurate calibration of multi-lidar-multi-camera systems}},
volume = {18},
year = {2018}
}
@article{Kim2015a,
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
doi = {10.1109/IVS.2015.7225724},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/Investigation on the occurrence of mutual interference between pulsed terrestrial LIDAR scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {9781467372664},
journal = {IEEE Intell. Veh. Symp. Proc.},
number = {Iv},
pages = {437--442},
publisher = {IEEE},
title = {{Investigation on the occurrence of mutual interference between pulsed terrestrial LIDAR scanners}},
volume = {2015-Augus},
year = {2015}
}
@article{Sturm2010,
abstract = {This survey is mainly motivated by the increased availability and use of panoramic image acquisition devices, in computer vision and various of its applications. Different technologies and different computational models thereof exist and algorithms and theoretical studies for geometric computer vision ("structure-from-motion") are often re-developed without highlighting common underlying principles. One of the goals of this survey is to give an overview of image acquisition methods used in computer vision and especially, of the vast number of camera models that have been proposed and investigated over the years, where we try to point out similarities between different models. Results on epipolar and multi-view geometry for different camera models are reviewed as well as various calibration and self-calibration approaches, with an emphasis on non-perspective cameras.We finally describe what we consider are fundamental building blocks for geometric computer vision or structure-from-motion: epipolar geometry, pose and motion estimation, 3D scene modeling, and bundle adjustment. The main goal here is to highlight the main principles of these, which are independent of specific camera models.},
author = {Sturm, Peter and Ramalingam, Srikumar and Tardif, Jean Philippe and Gasparini, Simone and Barreto, Jo{\~{a}}o},
doi = {10.1561/0600000023},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Sturm et al/Camera models and fundamental concepts used in geometric computer vision. Sturm et al.. 2010.pdf:pdf},
issn = {15722740},
journal = {Found. Trends Comput. Graph. Vis.},
number = {1-2},
pages = {1--183},
title = {{Camera models and fundamental concepts used in geometric computer vision}},
volume = {6},
year = {2010}
}
@article{Gong2013,
abstract = {This paper presents a novel way to address the extrinsic calibration problem for a system composed of a 3D LIDAR and a camera. The relative transformation between the two sensors is calibrated via a nonlinear least squares (NLS) problem, which is formulated in terms of the geometric constraints associated with a trihedral object. Precise initial estimates of NLS are obtained by dividing it into two sub-problems that are solved individually. With the precise initializations, the calibration parameters are further refined by iteratively optimizing the NLS problem. The algorithm is validated on both simulated and real data, as well as a 3D reconstruction application. Moreover, since the trihedral target used for calibration can be either orthogonal or not, it is very often present in structured environments, making the calibration convenient.},
author = {Gong, Xiaojin and Lin, Ying and Liu, Jilin},
doi = {10.3390/s130201902},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gong, Lin, Liu/3D LIDAR-camera extrinsic calibration using an arbitrary trihedron. Gong, Lin, Liu. 2013.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {3D LIDAR-camera system,Extrinsic calibration,Sensor fusion},
number = {2},
pages = {1902--1918},
title = {{3D LIDAR-camera extrinsic calibration using an arbitrary trihedron}},
volume = {13},
year = {2013}
}
@article{Hast2013,
abstract = {Optimal RANSAC - Towards a Repeatable Algorithm for Finding the Optimal Set},
author = {Hast, Anders and Nysj{\"{o}}, Johan and Marchetti, Andrea},
issn = {12136972},
journal = {J. WSCG},
keywords = {3D planes,Feature matching,Image stitching,Local optimisation,Optimal set,RANSAC,Repeatable},
number = {1},
pages = {21--30},
title = {{Optimal RANSAC - Towards a repeatable algorithm for finding the optimal set}},
url = {http://www.cb.uu.se/{~}aht/articles/A53-full.pdf},
volume = {21},
year = {2013}
}
@article{Fischler1981,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/ smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing and analysis conditions. Implementation details and computational examples are also presented.},
archivePrefix = {arXiv},
arxivId = {3629719},
author = {Fischler, Martin A. and Bolles, Robert C.},
doi = {10.1145/358669.358692},
eprint = {3629719},
isbn = {0934613338},
issn = {00010782},
journal = {Commun. ACM},
month = {jun},
number = {6},
pages = {381--395},
pmid = {4650729},
title = {{Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography}},
url = {http://portal.acm.org/citation.cfm?doid=358669.358692},
volume = {24},
year = {2002}
}
@article{Fersch2017b,
abstract = {{\textcopyright} 2017 IEEE. We present the application of optical code division multiple access (OCDMA) modulation based on optical orthogonal codes for automotive time-of-flight light detection and ranging (LiDAR) system sensors using avalanche photodiode (APD) detectors. The modulation opens the possibility to discriminate single laser transmissions. This allows the realization of additional features like enhancing the systems' interference robustness or accelerating their scanning rate. The requirements on automotive LiDAR OCDMA modulation differs from telecommunication's demands in several ways: The sensor must guarantee absolute laser safety; front facing devices must be able to provide reliable long range results up to a distance of 150m and beyond; and the signal is transmitted through free space. After outlining the basic functionality of the sort of sensors in consideration, the properties of the optical orthogonal codes (OOC) modulation are compared with the state of the art theoretically. The influence of OOC parameters with respect to scanning LiDAR systems is examined. The modulation technique is then demonstrated experimentally with two examples: The detection and separation of coded and traditional signals is shown using a matched filter detection algorithm. In the same way, differently coded signals can be separated from each other. Impediments of the interference suppression quality are discussed. Finally, an overview of possible applications of the proposed technique in automotive LiDAR systems is given, which are enabled by the OCDMA modulation in the first place.},
author = {Fersch, Thomas and Weigel, Robert and Koelpin, Alexander},
doi = {10.1109/JSEN.2017.2688126},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems. Fersch, Weigel, Koelpin.pdf:pdf},
issn = {1530437X},
journal = {IEEE Sens. J.},
keywords = {Laser radar,LiDAR,OCDMA modulation,time-of-flight},
number = {11},
pages = {3507--3516},
title = {{A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems}},
volume = {17},
year = {2017}
}
@article{Li2016,
abstract = {Detailed 3D modeling of indoor scene has become an important topic in many research fields. It can provide extensive information about the environment and boost various location based services, such as interactive gaming and indoor navigation. This paper presents an indoor scene construction approach using 2D line-scan LiDAR and entry-level digital camera. Both devices are mounted rigidly on a robotic servo, which sweeps vertically to cover the third dimension. Fiducial target based extrinsic calibration is applied to acquire transformation matrices between LiDAR and camera. Based on the transformation matrix, we perform registration to fuse the color images from the camera with the 3D points cloud from the LiDAR. The whole system setup has much lower cost as compared to systems using 3D LiDAR and omnidirectional camera. Using pre-calculated transformation matrices instead of feature extraction techniques such as SIFT or SURF in registration gives better fusion result and lower computational complexity. The experiments carried out in office building environment show promising results of our approach.},
author = {Li, Juan and He, Xiang and Li, Jia},
doi = {10.1109/NAECON.2015.7443100},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Li, He, Li/2D LiDAR and camera fusion in 3D modeling of indoor environment. Li, He, Li. 2016.pdf:pdf},
isbn = {9781467375658},
issn = {23792027},
journal = {Proc. IEEE Natl. Aerosp. Electron. Conf. NAECON},
keywords = {2D line-scan LiDAR,3D indoor modeling,digital camera,extrinsic calibration,sensor fusion},
number = {June 2015},
pages = {379--383},
title = {{2D LiDAR and camera fusion in 3D modeling of indoor environment}},
volume = {2016-March},
year = {2016}
}
@misc{watson,
title = {{Watson Visual Recognition}},
url = {https://www.ibm.com/watson/services/visual-recognition/},
urldate = {2019-10-22}
}
@article{Pusztai2018,
abstract = {As autonomous driving attracts more and more attention these days, the algorithms and sensors used for machine perception become popular in research, as well. This paper investigates the extrinsic calibration of two frequently-applied sensors: the camera and Light Detection and Ranging (LiDAR). The calibration can be done with the help of ordinary boxes. It contains an iterative refinement step, which is proven to converge to the box in the LiDAR point cloud, and can be used for system calibration containing multiple LiDARs and cameras. For that purpose, a bundle adjustment-like minimization is also presented. The accuracy of the method is evaluated on both synthetic and real-world data, outperforming the state-of-the-art techniques. The method is general in the sense that it is both LiDAR and camera-type independent, and only the intrinsic camera parameters have to be known. Finally, a method for determining the 2D bounding box of the car chassis from LiDAR point clouds is also presented in order to determine the car body border with respect to the calibrated sensors.},
author = {Pusztai, Zolt{\'{a}}n and Eichhardt, Iv{\'{a}}n and Hajder, Levente},
doi = {10.3390/s18072139},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Pusztai, Eichhardt, Hajder/Accurate calibration of multi-lidar-multi-camera systems. Pusztai, Eichhardt, Hajder. 2018.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Autonomous driving,Camera,Extrinsic calibration,LiDAR,LiDAR camera system,Machine perception},
number = {7},
pages = {1--22},
title = {{Accurate calibration of multi-lidar-multi-camera systems}},
volume = {18},
year = {2018}
}
@article{Kim2015b,
abstract = {LIDAR scanners are essential components of intelligent vehicles capable of autonomous travel. Mutual interference between LIDAR scanners has not been regarded as a problem yet. Mutual interference was identified as a problem of increased importance because of the appearance of safety functions and the increasing rate of vehicles equipped with LIDAR scanner. This paper will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some generic interference scenarios and report on the current status of the analysis of interference mechanisms.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
doi = {10.1109/ITNG.2015.113},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/An Experiment of Mutual Interference between Automotive LIDAR Scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {9781479988273},
journal = {Proc. - 12th Int. Conf. Inf. Technol. New Gener. ITNG 2015},
keywords = {LIDAR scanner,autonomous vehicle,ghost target,mutual interference,obstacle detection},
pages = {680--685},
title = {{An Experiment of Mutual Interference between Automotive LIDAR Scanners}},
year = {2015}
}
@article{WHOsite,
journal = {WHO},
keywords = {road safety [subject]},
publisher = {World Health Organization},
title = {{WHO | Global status report on road safety 2018}},
url = {https://www.who.int/violence{\_}injury{\_}prevention/road{\_}safety{\_}status/2018/en/},
year = {2019}
}
@article{Giuggioli2015,
abstract = {Animal coordinated movement interactions are commonly explained by assuming unspecified social forces of attraction, repulsion and alignment with parameters drawn from observed movement data. Here we propose and test a biologically realistic and quantifiable biosonar movement interaction mechanism for echolocating bats based on spatial perceptual bias, i.e. actual sound field, a reaction delay, and observed motor constraints in speed and acceleration. We found that foraging pairs of bats flying over a water surface swapped leader-follower roles and performed chases or coordinated manoeuvres by copying the heading a nearby individual has had up to 500 ms earlier. Our proposed mechanism based on the interplay between sensory-motor constraints and delayed alignment was able to recreate the observed spatial actor-reactor patterns. Remarkably, when we varied model parameters (response delay, hearing threshold and echolocation directionality) beyond those observed in nature, the spatio-temporal interaction patterns created by the model only recreated the observed interactions, i.e. chases, and best matched the observed spatial patterns for just those response delays, hearing thresholds and echolocation directionalities found to be used by bats. This supports the validity of our sensory ecology approach of movement coordination, where interacting bats localise each other by active echolocation rather than eavesdropping.},
author = {Giuggioli, Luca and McKetterick, Thomas J. and Holderied, Marc},
doi = {10.1371/journal.pcbi.1004089},
editor = {Ayers, Joseph},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Giuggioli, McKetterick, Holderied/Delayed Response and Biosonar Perception Explain Movement Coordination in Trawling Bats. Giuggioli, McKetterick, Holderied. 2015.pdf:pdf},
issn = {1553-7358},
journal = {PLOS Comput. Biol.},
month = {mar},
number = {3},
pages = {e1004089},
title = {{Delayed Response and Biosonar Perception Explain Movement Coordination in Trawling Bats}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1004089},
volume = {11},
year = {2015}
}
@article{NGuessan2017,
abstract = {There exist many iterative methods for computing the maximum likelihood estimator but most of them suffer from one or several drawbacks such as the need to inverse a Hessian matrix and the need to find good initial approximations of the parameters that are unknown in practice. In this paper, we present an estimation method without matrix inversion based on a linear approximation of the likelihood equations in a neighborhood of the constrained maximum likelihood estimator. We obtain closed-form approximations of solutions and standard errors. Then, we propose an iterative algorithm which cycles through the components of the vector parameter and updates one component at a time. The initial solution, which is necessary to start the iterative procedure, is automated. The proposed algorithm is compared to some of the best iterative optimization algorithms available on R and MATLAB software through a simulation study and applied to the statistical analysis of a road safety measure.},
author = {N'Guessan, Assi and Geraldo, Issa Cherif and Hafidi, Bezza},
doi = {10.4236/ojs.2017.71011},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/N'Guessan, Geraldo, Hafidi/An Approximation Method for a Maximum Likelihood Equation System and Application to the Analysis of Accidents Data. N'Guessan, Geraldo.pdf:pdf},
issn = {2161-718X},
journal = {Open J. Stat.},
keywords = {Constrained Maximum Likelihood, Partial Linear App,constrained maximum likelihood,iterative algorithms,partial linear approximation,road safety measure,s complement,schur},
number = {01},
pages = {132--152},
title = {{An Approximation Method for a Maximum Likelihood Equation System and Application to the Analysis of Accidents Data}},
volume = {07},
year = {2017}
}
@misc{bunny,
title = {{pcl/bunny.pcd at master {\textperiodcentered} PointCloudLibrary/pcl}},
url = {https://github.com/PointCloudLibrary/pcl/blob/master/test/bunny.pcd},
urldate = {2019-10-22}
}
@article{Fischler2002,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/ smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing and analysis conditions. Implementation details and computational examples are also presented.},
archivePrefix = {arXiv},
arxivId = {3629719},
author = {Fischler, Martin A. and Bolles, Robert C.},
doi = {10.1145/358669.358692},
eprint = {3629719},
isbn = {0934613338},
issn = {00010782},
journal = {Commun. ACM},
month = {jun},
number = {6},
pages = {381--395},
pmid = {4650729},
title = {{Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography}},
url = {http://portal.acm.org/citation.cfm?doid=358669.358692},
volume = {24},
year = {2002}
}
@misc{Condliffe2015,
author = {Condliffe, Jamie},
booktitle = {Gizmodo},
title = {{A {\$}60 Hack Can Fool the LIDAR Sensors Used on Most Self-Driving Cars}},
url = {https://gizmodo.com/a-60-hack-can-fool-the-lidar-sensors-used-on-most-self-1729272292},
urldate = {2018-11-28},
year = {2015}
}
@article{Ekstrom2015,
abstract = {Place cells are a fundamental component of the rodent navigational system. One intriguing implication of place cells is that humans, by extension, have "map-like" (or GPS-like) knowledge that we use to represent space. Here, we review both behavioral and neural studies of human navigation, suggesting that how we process visual information forms a critical component of how we represent space. These include cellular and brain systems devoted to coding visual information during navigation in addition to a location coding system similar to that described in rodents. Together, these findings suggest that while it is highly useful to think of our navigation system involving internal "maps," we should not neglect the importance of high-resolution visual representations to how we navigate space.},
author = {Ekstrom, Arne D.},
doi = {10.1002/hipo.22449},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ekstrom/Why vision is important to how we navigate. Ekstrom. 2015.pdf:pdf},
issn = {10981063},
journal = {Hippocampus},
keywords = {Allocentric,Cognitive map,Egocentric,Hippocampus,Humans,Path integration,Spatial navigation},
number = {6},
pages = {731--735},
title = {{Why vision is important to how we navigate}},
volume = {25},
year = {2015}
}
@book{mvg_book,
author = {Hartley, Richard and Zisserman, Andrew},
edition = {2},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hartley, Zisserman/Multiple View Geometry in computer vision. Hartley, Zisserman. 2003.pdf:pdf},
isbn = {9780521540513},
publisher = {Cambridge University Press New York, NY, USA},
title = {{Multiple View Geometry in computer vision}},
year = {2003}
}
@article{Huang2010,
abstract = {We describe the Lightweight Communications and Marshalling (LCM) library for message passing and data marshalling. The primary goal of LCM is to simplify the development of low-latency message passing systems, especially for real-time robotics research applications. Messages can be transmitted between different processes using LCM's publish/subscribe message-passing system. A platform- and language-independent type specification language separates message description from implementation. Message specifications are automatically compiled into language-specific bindings, eliminating the need for users to implement marshalling code while guaranteeing run-time type safety. LCM is notable in providing a real-time deep traffic inspection tool that can decode and display message traffic with minimal user effort and no impact on overall system performance. This and other features emphasize LCM's focus on simplifying both the development and debugging of message passing systems. In this paper, we explain the design of LCM, evaluate its performance, and describe its application to a number of autonomous land, underwater, and aerial robots. {\textcopyright}2010 IEEE.},
author = {Huang, Albert S. and Olson, Edwin and Moore, David C.},
doi = {10.1109/IROS.2010.5649358},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Huang, Olson, Moore/LCM Lightweight Communications and Marshalling. Huang, Olson, Moore. 2010.pdf:pdf},
isbn = {9781424466757},
journal = {IEEE/RSJ 2010 Int. Conf. Intell. Robot. Syst. IROS 2010 - Conf. Proc.},
number = {Lcm},
pages = {4057--4062},
title = {{LCM: Lightweight Communications and Marshalling}},
year = {2010}
}
@article{Lourakis2005,
abstract = {In order to obtain optimal 3D structure and viewing parameter estimates, bundle adjustment is often used as the last step of feature-based structure and motion estimation algorithms. Bundle adjustment involves the formulation of a large scale, yet sparse minimization problem, which is traditionally solved using a sparse variant of the Levenberg-Marquardt optimization algorithm that avoids storing and operating on zero entries. This paper argues that considerable computational benefits can be gained by substituting the sparse Levenberg-Marquardt algorithm in the implementation of bundle adjustment with a sparse variant of Powell's dog leg non-linear least squares technique. Detailed comparative experimental results provide strong evidence supporting this claim. {\textcopyright} 2005 IEEE.},
author = {Lourakis, Manolis I.A. and Argyros, Antonis A.},
doi = {10.1109/ICCV.2005.128},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lourakis, Argyros/Is Levenberg-Marquardt the most efficient optimization algorithm for implementing bundle adjustment. Lourakis, Argyros. 2005.pdf:pdf},
isbn = {076952334X},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
pages = {1526--1531},
title = {{Is Levenberg-Marquardt the most efficient optimization algorithm for implementing bundle adjustment?}},
volume = {II},
year = {2005}
}
@article{Morrison1960,
author = {Morrison, David D.},
journal = {Proc. Jet Propuls. Lab. Semin. Track. Programs Orbit Determ.},
pages = {1--9},
title = {{Methods for nonlinear least squares problems and convergence proofs}},
year = {1960}
}
