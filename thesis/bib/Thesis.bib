Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Condliffe2015,
author = {Condliffe, Jamie},
booktitle = {Gizmodo},
title = {{A {\$}60 Hack Can Fool the LIDAR Sensors Used on Most Self-Driving Cars}},
url = {https://gizmodo.com/a-60-hack-can-fool-the-lidar-sensors-used-on-most-self-1729272292},
urldate = {2018-11-28},
year = {2015}
}
@misc{CameronOliver2017,
abstract = {Why did LIDAR take off with self-driving cars? In a word: mapping. LIDAR allows you to generate huge 3D maps (its original application!), which you can then navigate the car or robot predictably within. By using a LIDAR to map and navigate an environment, you can know ahead of time the bounds of a lane, or that there is a stop sign or traffic light 500m ahead. This kind of predictability is exactly what a technology like self-driving cars requires, and has been a big reason for the progress over the last 5 years.
},
author = {{Cameron Oliver}},
booktitle = {Voyage},
keywords = {LIDAR,Self-Driving Cars},
mendeley-tags = {LIDAR,Self-Driving Cars},
title = {{An Introduction to LIDAR: The Key Self-Driving Car Sensor}},
url = {https://news.voyage.auto/an-introduction-to-lidar-the-key-self-driving-car-sensor-a7e405590cff},
urldate = {2018-10-09},
year = {2017}
}
@misc{Cameron,
abstract = {Retrofit a Car for self-driving using LIDAR, camera and Radar},
author = {Cameron, Oliver and Voyage},
keywords = {LIDAR,Self-Driving Cars,Sensor Fusion},
mendeley-tags = {LIDAR,Self-Driving Cars,Sensor Fusion},
title = {{The Story of Homer: Voyage's First Self-Driving Taxi}},
url = {https://news.voyage.auto/the-story-of-homer-voyages-first-self-driving-taxi-f0a6466718af},
urldate = {2018-10-09}
}
@article{EvansT.C.GavrilovichE.MihaiR.C.andIsbasescuI.2014,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0403007},
author = {{Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isbasescu, I.}, Easyg Llc and Thelen, Darryl and Martin, J A and Allen, S M and SA, Slane},
doi = {10.1037/t24245-000},
eprint = {0403007},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isbasescu, I. et al/( 12 ) Patent Application Publication ( 10 ) Pub . No . US 2006 0222585 A1 Figure 1. Evans, T.C., Gavrilovich, E., Mihai, R.C. and Isba.pdf:pdf},
isbn = {2009023471},
issn = {13871811},
number = {15},
pages = {354},
pmid = {23110556},
primaryClass = {arXiv:physics},
title = {{( 12 ) Patent Application Publication ( 10 ) Pub . No .: US 2006 / 0222585 A1 Figure 1}},
volume = {002},
year = {2014}
}
@article{Kim2015a,
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
doi = {10.1109/IVS.2015.7225724},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/Investigation on the occurrence of mutual interference between pulsed terrestrial LIDAR scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {9781467372664},
journal = {IEEE Intell. Veh. Symp. Proc.},
number = {Iv},
pages = {437--442},
publisher = {IEEE},
title = {{Investigation on the occurrence of mutual interference between pulsed terrestrial LIDAR scanners}},
volume = {2015-Augus},
year = {2015}
}
@article{Wei2018,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@article{Geiger2012,
abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
doi = {10.1109/CVPR.2012.6248074},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger, Lenz, Urtasun/Are we ready for autonomous driving the KITTI vision benchmark suite. Geiger, Lenz, Urtasun. 2012.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {3354--3361},
title = {{Are we ready for autonomous driving? the KITTI vision benchmark suite}},
year = {2012}
}
@misc{Camerona,
abstract = {Retrofit a Car for self-driving using LIDAR, camera and Radar},
author = {Cameron, Oliver and Voyage},
keywords = {LIDAR,Self-Driving Cars,Sensor Fusion},
mendeley-tags = {LIDAR,Self-Driving Cars,Sensor Fusion},
title = {{The Story of Homer: Voyage's First Self-Driving Taxi}},
url = {https://news.voyage.auto/the-story-of-homer-voyages-first-self-driving-taxi-f0a6466718af},
urldate = {2018-10-09}
}
@misc{Simonite2017,
author = {Simonite, Tom},
booktitle = {MT Technol. Rev.},
title = {{Self-Driving Cars' Spinning-Laser Problem - MIT Technology Review}},
url = {https://www.technologyreview.com/s/603885/autonomous-cars-lidar-sensors/},
urldate = {2019-01-23},
year = {2017}
}
@article{Kim2017,
author = {Kim, Gunzung and Eom, Jeongsook and Choi, Jeonghee and Park, Yongwan},
doi = {10.14372/iemek.2017.12.1.43},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Mutual Interference on Mobile Pulsed Scanning LIDAR. Kim et al.. 2017.pdf:pdf},
issn = {1975-5066},
journal = {IEMEK J. Embed. Syst. Appl.},
month = {feb},
number = {1},
pages = {43--62},
title = {{Mutual Interference on Mobile Pulsed Scanning LIDAR}},
url = {http://koreascience.or.kr/journal/view.jsp?kj=OBDDBE{\&}py=2017{\&}vnc=v12n1{\&}sp=43},
volume = {12},
year = {2017}
}
@misc{CameronOliver2017a,
abstract = {Why did LIDAR take off with self-driving cars? In a word: mapping. LIDAR allows you to generate huge 3D maps (its original application!), which you can then navigate the car or robot predictably within. By using a LIDAR to map and navigate an environment, you can know ahead of time the bounds of a lane, or that there is a stop sign or traffic light 500m ahead. This kind of predictability is exactly what a technology like self-driving cars requires, and has been a big reason for the progress over the last 5 years.
},
author = {{Cameron Oliver}},
booktitle = {Voyage},
keywords = {LIDAR,Self-Driving Cars},
mendeley-tags = {LIDAR,Self-Driving Cars},
title = {{An Introduction to LIDAR: The Key Self-Driving Car Sensor}},
url = {https://news.voyage.auto/an-introduction-to-lidar-the-key-self-driving-car-sensor-a7e405590cff},
urldate = {2018-10-09},
year = {2017}
}
@article{Christopher2015,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0403007},
author = {Christopher, Inventors and Ramsey, Paul and Chant, Garry Richard and Lockley, Andrew Robert and Gb, Wantage and Fields, Brian and Watson, Martin John and Rachel, Eleanor and Hyde, Ann and Gb, Wantage and Jasper, A},
doi = {10.1016/j.(73)},
eprint = {0403007},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Christopher et al/( 12 ) United States Patent ( 10 ) Patent No .. Christopher et al.. 2015.pdf:pdf},
isbn = {2010512510},
issn = {2470-0010},
number = {12},
pmid = {1000182772},
primaryClass = {arXiv:physics},
title = {{( 12 ) United States Patent ( 10 ) Patent No .:}},
volume = {2},
year = {2015}
}
@article{Kim2015c,
abstract = {The LIDAR scanner is at the heart of object detection of the self-driving car. Mutual interference between LIDAR scanners has not been regarded as a problem because the percentage of vehicles equipped with LIDAR scanners was very rare. With the growing number of autonomous vehicle equipped with LIDAR scanner operated close to each other at the same time, the LIDAR scanner may receive laser pulses from other LIDAR scanners. In this paper, three types of experiments and their results are shown, according to the arrangement of two LIDAR scanners. We will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some typical mutual interference scenario and report an analysis of the interference mechanism. {\textcopyright} 2015 COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Seonghyeon and Park, Yongwan},
doi = {10.1117/12.2178502},
editor = {Prochazka, Ivan and Sobolewski, Roman and James, Ralph B.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Occurrence and characteristics of mutual interference between LIDAR scanners. Kim et al.. 2015.pdf:pdf},
journal = {Phot. Count. Appl. 2015},
keywords = {LIDAR scanner,ghost target,mutual interference,self-driving car,time of flight},
month = {may},
number = {September},
pages = {95040K},
publisher = {International Society for Optics and Photonics},
title = {{Occurrence and characteristics of mutual interference between LIDAR scanners}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2178502},
volume = {9504},
year = {2015}
}
@article{Castorena2016,
abstract = {We present a new method for joint automatic extrinsic calibration and sensor fusion for a multimodal sensor system comprising a LIDAR and an optical camera. Our approach exploits the natural alignment of depth and intensity edges when the calibration parameters are correct. Thus, in contrast to a number of existing approaches, we do not require the presence or identification of known alignment targets. On the other hand, the characteristics of each sensor modality, such as sampling pattern and information measured, are significantly different, making direct edge alignment difficult. To overcome this difficulty, we jointly fuse the data and estimate the calibration parameters. In particular, the joint processing evaluates and optimizes both the quality of edge alignment and the performance of the fusion algorithm using a common cost function on the output. We demonstrate accurate calibration in practical configurations in which depth measurements are sparse and contain no reflectivity information. Experiments on synthetic and real data obtained with a three-dimensional LIDAR sensor demonstrate the effectiveness of our approach.},
author = {Castorena, Juan and Kamilov, Ulugbek S. and Boufounos, Petros T.},
doi = {10.1109/ICASSP.2016.7472200},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Autocalibration of lidar and optical cameras via edge alignment. Castorena, Kamilov, Boufounos.pdf:pdf},
isbn = {9781479999880},
issn = {15206149},
journal = {ICASSP, IEEE Int. Conf. Acoust. Speech Signal Process. - Proc.},
keywords = {Multimodal calibration,depth superresolution,intersensor registration,sensor fusion,total variation},
pages = {2862--2866},
publisher = {IEEE},
title = {{Autocalibration of lidar and optical cameras via edge alignment}},
volume = {2016-May},
year = {2016}
}
@inproceedings{Yue2018,
abstract = {3D LiDAR scanners are playing an increasingly important role in autonomous driving as they can generate depth information of the environment. However, creating large 3D LiDAR point cloud datasets with point-level labels requires a significant amount of manual annotation. This jeopardizes the efficient development of supervised deep learning algorithms which are often data-hungry. We present a framework to rapidly create point clouds with accurate point-level labels from a computer game. The framework supports data collection from both auto-driving scenes and user-configured scenes. Point clouds from auto-driving scenes can be used as training data for deep learning algorithms, while point clouds from user-configured scenes can be used to systematically test the vulnerability of a neural network, and use the falsifying examples to make the neural network more robust through retraining. In addition, the scene images can be captured simultaneously in order for sensor fusion tasks, with a method proposed to do automatic calibration between the point clouds and captured scene images. We show a significant improvement in accuracy (+9{\%}) in point cloud segmentation by augmenting the training dataset with the generated synthesized data. Our experiments also show by testing and retraining the network using point clouds from user-configured scenes, the weakness/blind spots of the neural network can be fixed.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1804.00103},
author = {Yue, Xiangyu and Wu, Bichen and Seshia, Sanjit A. and Keutzer, Kurt and Sangiovanni-Vincentelli, Alberto L.},
booktitle = {Proc. 2018 ACM Int. Conf. Multimed. Retr. - ICMR '18},
doi = {10.1145/3206025.3206080},
eprint = {1804.00103},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A LiDAR Point Cloud Generator. Yue et al.pdf:pdf},
isbn = {9781450350464},
month = {mar},
number = {Nips},
pages = {458--464},
publisher = {ACM Press},
title = {{A LiDAR Point Cloud Generator}},
url = {http://dl.acm.org/citation.cfm?doid=3206025.3206080 http://arxiv.org/abs/1804.00103},
year = {2018}
}
@article{Bimbraw2015,
author = {Bimbraw, K},
doi = {10.5220/0005540501910198},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Bimbraw/Autonomous Cars Past , Present and Future - A Review of the Developments in the Last Century , the Present .... Bimbraw. 2015.pdf:pdf},
isbn = {9789897581229},
journal = {12th Int. Conf. Informatics Control. Autom. Robot.},
keywords = {accomplished in this,and much has been,automation,automation is of interest,autonomous cars,autonomous vehicles,cars,intelligent transportation,mechatronics systems,technologies and systems,the field of autonomous,to researchers},
number = {August},
pages = {191--198},
title = {{Autonomous Cars : Past , Present and Future - A Review of the Developments in the Last Century , the Present ...}},
year = {2015}
}
@article{Sullivan2016,
abstract = {Analysis of LiDAR technology for Advanced Safety Preface Safety and its improvement is a concern paramount to all passenger vehicle manufacturers. The value proposition for driver, passenger and pedestrian safety has become equally important as engine performance and fuel economy. This paper addresses how light detection and range (LiDAR) technology will impact Advanced Driver Assistance Systems and explore the term, " autonomous driving intelligence. " The paper will consider Lidar technology today and how it stands to capture a large market share of automotive sensor technology in the future. Lidar is poised to penetrate the market in 2016 as the lead technology in automotive safety and autonomous systems. Three primary approaches to Lidar development are considered, including Hybrid Solid-State Lidar, MEMS Lidar, and Mechanical Mechanism Lidar. Mechanical mechanism Lidar is the oldest and most traditional technology. MEMS Lidar technology is in the beginning stage of development as a low cost Lidar solution for low level automotive safety. Solid-state Hybrid Lidar (SH Lidar) was introduced in 2005 as a result of the Darpa Robotic Car Races. The technology has been tested for autonomous safety over the years and the cost for SH Lidar dropped dramatically in 2015. With planned mass production to meet the growing demand for autonomous navigation and advanced safety, further dramatic cost reduction is expected in 2016 – 2017. The development of Solid-State Hybrid Lidar (SH Lidar) was a break away from the traditional mechanical mechanism of single Lidar technology, and it is described in detail in this paper. The technology simplified what was previously a complex mechanical system of parts into one robust solid-state part. The solid-state developmental enabled faster data capture in 3D, capturing pictures instantaneously while moving in real-time at speeds of 30-40MPH. SH Lidar technology has matured from an extremely costly technology and large system to being affordable, smaller in size, and headed toward mass production. SH Lidar technology is poised to be commercialized in 2015-16 and radically change the way we move about in the world.},
author = {Sullivan, Frost {\&}},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Sullivan/LiDAR Driving the Future of Autonomous Navigation. Sullivan. 2016.pdf:pdf},
pages = {1--30},
title = {{LiDAR: Driving the Future of Autonomous Navigation}},
year = {2016}
}
@article{Steder,
author = {Steder, Bastian},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/The Point Cloud Library PCL Thanks to Radu Rusu from Willow Garage for some of the slides!. Steder.pdf:pdf},
title = {{The Point Cloud Library PCL Thanks to Radu Rusu from Willow Garage for some of the slides!}},
url = {http://ais.informatik.uni-freiburg.de/teaching/ws10/robotics2/pdfs/rob2-12-ros-pcl.pdf}
}
@inproceedings{Yoneda2014,
abstract = {In recent years, automated vehicle researches move on to the next stage, that is auto-driving experiments on public roads. Major challenge is how to robustly drive at complicated situations such as narrow or non-featured road. In order to realize practical performance, some static information should be kept on memory such as road topology, building shape, white line, curb, traffic light and so on. Currently, some measurement companies have already begun to prepare map database for automated vehicles. They are able to provide highly-precise 3-D map for robust automated driving. This study focuses on what kind of data should be observed during automated driving with such precise database. In particular, we focus on the accurate localization based on the use of lidar data and precise 3-D map, and propose a feature quantity for scan data based on distribution of clusters. Localization experiment shows that our method can measure surrounding uncertainty and guarantee accurate localization.},
author = {Yoneda, Keisuke and Tehrani, Hossein and Ogawa, Takashi and Hukuyama, Naohisa and Mita, Seiichi},
booktitle = {IEEE Intell. Veh. Symp. Proc.},
doi = {10.1109/IVS.2014.6856596},
isbn = {9781479936380},
issn = {10901981},
pages = {1345--1350},
pmid = {12693519},
title = {{Lidar scan feature for localization with highly precise 3-D map}},
year = {2014}
}
@article{Fischler2002,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/ smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing and analysis conditions. Implementation details and computational examples are also presented.},
archivePrefix = {arXiv},
arxivId = {3629719},
author = {Fischler, Martin A. and Bolles, Robert C.},
doi = {10.1145/358669.358692},
eprint = {3629719},
isbn = {0934613338},
issn = {00010782},
journal = {Commun. ACM},
month = {jun},
number = {6},
pages = {381--395},
pmid = {4650729},
title = {{Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography}},
url = {http://portal.acm.org/citation.cfm?doid=358669.358692},
volume = {24},
year = {2002}
}
@article{Yue2018a,
abstract = {3D LiDAR scanners are playing an increasingly important role in autonomous driving as they can generate depth information of the environment. However, creating large 3D LiDAR point cloud datasets with point-level labels requires a significant amount of manual annotation. This jeopardizes the efficient development of supervised deep learning algorithms which are often data-hungry. We present a framework to rapidly create point clouds with accurate point-level labels from a computer game. The framework supports data collection from both auto-driving scenes and user-configured scenes. Point clouds from auto-driving scenes can be used as training data for deep learning algorithms, while point clouds from user-configured scenes can be used to systematically test the vulnerability of a neural network, and use the falsifying examples to make the neural network more robust through retraining. In addition, the scene images can be captured simultaneously in order for sensor fusion tasks, with a method proposed to do automatic calibration between the point clouds and captured scene images. We show a significant improvement in accuracy (+9{\%}) in point cloud segmentation by augmenting the training dataset with the generated synthesized data. Our experiments also show by testing and retraining the network using point clouds from user-configured scenes, the weakness/blind spots of the neural network can be fixed.},
archivePrefix = {arXiv},
arxivId = {1804.00103},
author = {Yue, Xiangyu and Wu, Bichen and Seshia, Sanjit A. and Keutzer, Kurt and Sangiovanni-Vincentelli, Alberto L.},
doi = {10.1145/3206025.3206080},
eprint = {1804.00103},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A LiDAR Point Cloud Generator. Yue et al.pdf:pdf},
isbn = {9781450350464},
journal = {Proc. 2018 ACM Int. Conf. Multimed. Retr.  - ICMR '18},
number = {Nips},
pages = {458--464},
title = {{A LiDAR Point Cloud Generator}},
url = {http://dl.acm.org/citation.cfm?doid=3206025.3206080},
year = {2018}
}
@inproceedings{Kim2015,
abstract = {LIDAR scanners are essential components of intelligent vehicles capable of autonomous travel. Mutual interference between LIDAR scanners has not been regarded as a problem yet. Mutual interference was identified as a problem of increased importance because of the appearance of safety functions and the increasing rate of vehicles equipped with LIDAR scanner. This paper will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some generic interference scenarios and report on the current status of the analysis of interference mechanisms.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
booktitle = {2015 12th Int. Conf. Inf. Technol. - New Gener.},
doi = {10.1109/ITNG.2015.113},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/An Experiment of Mutual Interference between Automotive LIDAR Scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {978-1-4799-8828-0},
keywords = {LIDAR scanner,autonomous vehicle,ghost target,mutual interference,obstacle detection},
month = {apr},
pages = {680--685},
publisher = {IEEE},
title = {{An Experiment of Mutual Interference between Automotive LIDAR Scanners}},
url = {http://ieeexplore.ieee.org/document/7113553/},
year = {2015}
}
@incollection{MartinVelasMichalSpanelZdenekMaterna2013,
abstract = {Quinolone-resistant clinical Escherichia coli isolates were examined for mutations in the marRAB operon of the multiple antibiotic resistance (mar) locus. Among 23 strains evaluated, 8 were chosen for further study: 3 that showed relatively high levels of uninduced, i.e., constitutive, expression of the operon and 5 with variable responses to induction by salicylate or tetracyclines. The marR genes, specifying the repressor of the operon, cloned from the three strains constitutively expressing the operon did not reduce the level of expression of beta-galactosidase from a marO::lacZ transcriptional fusion and were therefore mutant; however, marR genes cloned from the five other clinical strains repressed LacZ expression and were wild type. All three mutant marR genes contained more than one mutation: a deletion and a point mutation. Inactivation of the mar locus in the three known marR mutant strains with a kanamycin resistance cassette introduced by homologous recombination reduced resistance to quinolones and multiple antibiotics. These findings indicate that mar operon mutations exist in quinolone-resistant clinical E. coli isolates and contribute to quinolone and multidrug resistance.},
archivePrefix = {arXiv},
arxivId = {1505.00729},
author = {{Martin Velas, Michal Spanel, Zdenek Materna}, Adam Herout},
booktitle = {Flight Dyn. Princ.},
doi = {10.1016/B978-0-08-098242-7.00008-0},
eprint = {1505.00729},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Calibration of RGB Camera With Velodyne LiDAR. Martin Velas, Michal Spanel, Zdenek Materna.pdf:pdf},
isbn = {9781461434801},
issn = {0066-4804},
keywords = {L{\'{i}}DAR,Velodyne,calibration,camera,marker},
month = {jul},
number = {7},
pages = {219--241},
pmid = {8807064},
publisher = {Elsevier},
title = {{Calibration of RGB Camera With Velodyne LiDAR}},
url = {https://linkinghub.elsevier.com/retrieve/pii/B9780080982427000080},
volume = {40},
year = {2013}
}
@inproceedings{Javaheri2017,
abstract = {The increasing availability of point cloud data in recent years is demanding for high performance denoising methods and compression schemes. When point cloud data is directly obtained from depth sensors or extracted from images acquired from different viewpoints, imprecisions on the depth acquisition or in the 3D reconstruction techniques result in noisy point clouds which may include a significant number of outliers. Moreover, the quality assessment of point clouds is a challenging problem since this 3D representation format is unstructured and it is typically not directly visualized. In this paper, selected objective quality metrics are evaluated regarding their correlation with human quality assessment and thus human perception. As far as the authors know, this is the first paper performing the subjective assessment of point cloud denoising algorithms and the evaluation of most used point cloud objective quality metrics. Experimental results show that graph-based denoising algorithms can improve significantly the point cloud quality data and that objective metrics that model the underlying point cloud surface can correlate better with human perception.},
author = {Javaheri, Alireza and Brites, Catarina and Pereira, Fernando and Ascenso, Jo{\~{a}}o},
booktitle = {2017 IEEE 19th Int. Work. Multimed. Signal Process. MMSP 2017},
doi = {10.1109/MMSP.2017.8122239},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Subjective and objective quality evaluation of compressed point clouds. Javaheri et al.pdf:pdf},
isbn = {9781509036493},
issn = {1047-2797},
keywords = {Point cloud compression,Quality metrics,Subjective quality assessment},
pages = {1--6},
pmid = {10037558},
title = {{Subjective and objective quality evaluation of compressed point clouds}},
volume = {2017-Janua},
year = {2017}
}
@article{KresimirKusevicOttawaCA;PaulMrstikOttawaCA;CraigLenGlennieSpring2017,
author = {{Kresimir Kusevic, Ottawa (CA); Paul Mrstik, Ottawa (CA); Craig Len Glennie, Spring}, TX (US)},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Method and System for Aligninga Line Scan Camerawith a Lidar Scanner for Real Time Data Fusion in Three Dimiensions. Kresimir Kusevic, O.pdf:pdf},
isbn = {2222222222},
number = {19},
title = {{Method and System for Aligninga Line Scan Camerawith a Lidar Scanner for Real Time Data Fusion in Three Dimiensions}},
volume = {1},
year = {2017}
}
@article{Kim2017,
author = {Kim, Gunzung and Eom, Jeongsook and Choi, Jeonghee and Park, Yongwan},
doi = {10.14372/iemek.2017.12.1.43},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Mutual Interference on Mobile Pulsed Scanning LIDAR. Kim et al.. 2017.pdf:pdf},
issn = {1975-5066},
journal = {IEMEK J. Embed. Syst. Appl.},
month = {feb},
number = {1},
pages = {43--62},
title = {{Mutual Interference on Mobile Pulsed Scanning LIDAR}},
url = {http://koreascience.or.kr/journal/view.jsp?kj=OBDDBE{\&}py=2017{\&}vnc=v12n1{\&}sp=43},
volume = {12},
year = {2017}
}
@article{Kim2015b,
abstract = {LIDAR scanners are essential components of intelligent vehicles capable of autonomous travel. Mutual interference between LIDAR scanners has not been regarded as a problem yet. Mutual interference was identified as a problem of increased importance because of the appearance of safety functions and the increasing rate of vehicles equipped with LIDAR scanner. This paper will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some generic interference scenarios and report on the current status of the analysis of interference mechanisms.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
doi = {10.1109/ITNG.2015.113},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/An Experiment of Mutual Interference between Automotive LIDAR Scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {9781479988273},
journal = {Proc. - 12th Int. Conf. Inf. Technol. New Gener. ITNG 2015},
keywords = {LIDAR scanner,autonomous vehicle,ghost target,mutual interference,obstacle detection},
pages = {680--685},
title = {{An Experiment of Mutual Interference between Automotive LIDAR Scanners}},
year = {2015}
}
@article{Giuggioli2015,
abstract = {Animal coordinated movement interactions are commonly explained by assuming unspecified social forces of attraction, repulsion and alignment with parameters drawn from observed movement data. Here we propose and test a biologically realistic and quantifiable biosonar movement interaction mechanism for echolocating bats based on spatial perceptual bias, i.e. actual sound field, a reaction delay, and observed motor constraints in speed and acceleration. We found that foraging pairs of bats flying over a water surface swapped leader-follower roles and performed chases or coordinated manoeuvres by copying the heading a nearby individual has had up to 500 ms earlier. Our proposed mechanism based on the interplay between sensory-motor constraints and delayed alignment was able to recreate the observed spatial actor-reactor patterns. Remarkably, when we varied model parameters (response delay, hearing threshold and echolocation directionality) beyond those observed in nature, the spatio-temporal interaction patterns created by the model only recreated the observed interactions, i.e. chases, and best matched the observed spatial patterns for just those response delays, hearing thresholds and echolocation directionalities found to be used by bats. This supports the validity of our sensory ecology approach of movement coordination, where interacting bats localise each other by active echolocation rather than eavesdropping.},
author = {Giuggioli, Luca and McKetterick, Thomas J. and Holderied, Marc},
doi = {10.1371/journal.pcbi.1004089},
editor = {Ayers, Joseph},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Giuggioli, McKetterick, Holderied/Delayed Response and Biosonar Perception Explain Movement Coordination in Trawling Bats. Giuggioli, McKetterick, Holderied. 2015.pdf:pdf},
issn = {1553-7358},
journal = {PLOS Comput. Biol.},
month = {mar},
number = {3},
pages = {e1004089},
title = {{Delayed Response and Biosonar Perception Explain Movement Coordination in Trawling Bats}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1004089},
volume = {11},
year = {2015}
}
@inproceedings{Kim2015,
abstract = {LIDAR scanners are essential components of intelligent vehicles capable of autonomous travel. Mutual interference between LIDAR scanners has not been regarded as a problem yet. Mutual interference was identified as a problem of increased importance because of the appearance of safety functions and the increasing rate of vehicles equipped with LIDAR scanner. This paper will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some generic interference scenarios and report on the current status of the analysis of interference mechanisms.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Yongwan},
booktitle = {2015 12th Int. Conf. Inf. Technol. - New Gener.},
doi = {10.1109/ITNG.2015.113},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim, Eom, Park/An Experiment of Mutual Interference between Automotive LIDAR Scanners. Kim, Eom, Park. 2015.pdf:pdf},
isbn = {978-1-4799-8828-0},
keywords = {LIDAR scanner,autonomous vehicle,ghost target,mutual interference,obstacle detection},
month = {apr},
pages = {680--685},
publisher = {IEEE},
title = {{An Experiment of Mutual Interference between Automotive LIDAR Scanners}},
url = {http://ieeexplore.ieee.org/document/7113553/},
year = {2015}
}
@article{Fischler1981,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/ smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing and analysis conditions. Implementation details and computational examples are also presented.},
archivePrefix = {arXiv},
arxivId = {3629719},
author = {Fischler, Martin A. and Bolles, Robert C.},
doi = {10.1145/358669.358692},
eprint = {3629719},
isbn = {0934613338},
issn = {00010782},
journal = {Commun. ACM},
month = {jun},
number = {6},
pages = {381--395},
pmid = {4650729},
title = {{Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography}},
url = {http://portal.acm.org/citation.cfm?doid=358669.358692},
volume = {24},
year = {2002}
}
@inproceedings{Kim2015c,
abstract = {The LIDAR scanner is at the heart of object detection of the self-driving car. Mutual interference between LIDAR scanners has not been regarded as a problem because the percentage of vehicles equipped with LIDAR scanners was very rare. With the growing number of autonomous vehicle equipped with LIDAR scanner operated close to each other at the same time, the LIDAR scanner may receive laser pulses from other LIDAR scanners. In this paper, three types of experiments and their results are shown, according to the arrangement of two LIDAR scanners. We will show the probability that any LIDAR scanner will interfere mutually by considering spatial and temporal overlaps. It will present some typical mutual interference scenario and report an analysis of the interference mechanism. {\textcopyright} 2015 COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
author = {Kim, Gunzung and Eom, Jeongsook and Park, Seonghyeon and Park, Yongwan},
booktitle = {Phot. Count. Appl. 2015},
doi = {10.1117/12.2178502},
editor = {Prochazka, Ivan and Sobolewski, Roman and James, Ralph B.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Kim et al/Occurrence and characteristics of mutual interference between LIDAR scanners. Kim et al.. 2015.pdf:pdf},
keywords = {LIDAR scanner,ghost target,mutual interference,self-driving car,time of flight},
month = {may},
pages = {95040K},
publisher = {International Society for Optics and Photonics},
title = {{Occurrence and characteristics of mutual interference between LIDAR scanners}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2178502},
volume = {9504},
year = {2015}
}
@article{Roca2006,
archivePrefix = {arXiv},
arxivId = {arXiv:1208.5721},
author = {Roca, Sergi Figueras and Cited, References},
doi = {10.1038/incomms1464},
eprint = {arXiv:1208.5721},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Roca, Cited/( 12 ) United States Patent. Roca, Cited. 2006.pdf:pdf},
isbn = {1020080012051},
issn = {2004001828},
number = {12},
pmid = {1000182772},
title = {{( 12 ) United States Patent}},
volume = {2},
year = {2006}
}
@techreport{Rev,
address = {San Jose},
author = {Rev, E},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rev/VLP-16 User Manual. Rev. 2019.pdf:pdf},
institution = {Velodyne LiDAR, Inc.},
pages = {71},
title = {{VLP-16 User Manual}},
year = {2019}
}
@article{Hast2013,
abstract = {Optimal RANSAC - Towards a Repeatable Algorithm for Finding the Optimal Set},
author = {Hast, Anders and Nysj{\"{o}}, Johan and Marchetti, Andrea},
issn = {12136972},
journal = {J. WSCG},
keywords = {3D planes,Feature matching,Image stitching,Local optimisation,Optimal set,RANSAC,Repeatable},
number = {1},
pages = {21--30},
title = {{Optimal RANSAC - Towards a repeatable algorithm for finding the optimal set}},
url = {http://www.cb.uu.se/{~}aht/articles/A53-full.pdf},
volume = {21},
year = {2013}
}
@article{Fersch2017,
abstract = {{\textcopyright} 2017 IEEE. We present the application of optical code division multiple access (OCDMA) modulation based on optical orthogonal codes for automotive time-of-flight light detection and ranging (LiDAR) system sensors using avalanche photodiode (APD) detectors. The modulation opens the possibility to discriminate single laser transmissions. This allows the realization of additional features like enhancing the systems' interference robustness or accelerating their scanning rate. The requirements on automotive LiDAR OCDMA modulation differs from telecommunication's demands in several ways: The sensor must guarantee absolute laser safety; front facing devices must be able to provide reliable long range results up to a distance of 150m and beyond; and the signal is transmitted through free space. After outlining the basic functionality of the sort of sensors in consideration, the properties of the optical orthogonal codes (OOC) modulation are compared with the state of the art theoretically. The influence of OOC parameters with respect to scanning LiDAR systems is examined. The modulation technique is then demonstrated experimentally with two examples: The detection and separation of coded and traditional signals is shown using a matched filter detection algorithm. In the same way, differently coded signals can be separated from each other. Impediments of the interference suppression quality are discussed. Finally, an overview of possible applications of the proposed technique in automotive LiDAR systems is given, which are enabled by the OCDMA modulation in the first place.},
author = {Fersch, Thomas and Weigel, Robert and Koelpin, Alexander},
doi = {10.1109/JSEN.2017.2688126},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems. Fersch, Weigel, Koelpin.pdf:pdf},
issn = {1530-437X},
journal = {IEEE Sens. J.},
keywords = {Laser radar,LiDAR,OCDMA modulation,time-of-flight},
month = {jun},
number = {11},
pages = {3507--3516},
title = {{A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems}},
url = {http://ieeexplore.ieee.org/document/7887680/},
volume = {17},
year = {2017}
}
@article{Fersch2017b,
abstract = {{\textcopyright} 2017 IEEE. We present the application of optical code division multiple access (OCDMA) modulation based on optical orthogonal codes for automotive time-of-flight light detection and ranging (LiDAR) system sensors using avalanche photodiode (APD) detectors. The modulation opens the possibility to discriminate single laser transmissions. This allows the realization of additional features like enhancing the systems' interference robustness or accelerating their scanning rate. The requirements on automotive LiDAR OCDMA modulation differs from telecommunication's demands in several ways: The sensor must guarantee absolute laser safety; front facing devices must be able to provide reliable long range results up to a distance of 150m and beyond; and the signal is transmitted through free space. After outlining the basic functionality of the sort of sensors in consideration, the properties of the optical orthogonal codes (OOC) modulation are compared with the state of the art theoretically. The influence of OOC parameters with respect to scanning LiDAR systems is examined. The modulation technique is then demonstrated experimentally with two examples: The detection and separation of coded and traditional signals is shown using a matched filter detection algorithm. In the same way, differently coded signals can be separated from each other. Impediments of the interference suppression quality are discussed. Finally, an overview of possible applications of the proposed technique in automotive LiDAR systems is given, which are enabled by the OCDMA modulation in the first place.},
author = {Fersch, Thomas and Weigel, Robert and Koelpin, Alexander},
doi = {10.1109/JSEN.2017.2688126},
file = {:home/martinspedro/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems. Fersch, Weigel, Koelpin.pdf:pdf},
issn = {1530437X},
journal = {IEEE Sens. J.},
keywords = {Laser radar,LiDAR,OCDMA modulation,time-of-flight},
number = {11},
pages = {3507--3516},
title = {{A CDMA Modulation Technique for Automotive Time-of-Flight LiDAR Systems}},
volume = {17},
year = {2017}
}
@article{Wei2018a,
abstract = {Collision avoidance is a critical task in many applications, such as ADAS (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., GPS) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a LiDAR {\{}(Light Detection and Ranging){\}} and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange traffic cones with a highly reflective vertical pole attached. The LiDAR can readily detect these beacons, but suffers from false positives due to other reflective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the LiDAR by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the LiDAR space. Experimental data collected at Mississippi State University's Center for Advanced Vehicular Systems (CAVS) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
archivePrefix = {arXiv},
arxivId = {1807.10573},
author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
doi = {10.3390/electronics7060084},
eprint = {1807.10573},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wei et al/LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System. Wei et al.. 2018.pdf:pdf},
isbn = {0885-8993},
issn = {2079-9292},
keywords = {adas,camera,deep learning,fusion,lidar,multi-sensor},
pmid = {22255825},
title = {{LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System}},
url = {http://arxiv.org/abs/1807.10573},
year = {2018}
}
@article{Chum2005,
abstract = {The problem of model parameters estimation from data with a presence of outlier measurements often arises in computer vision and methods of robust estimation have to be used. The RANSAC algorithm introduced by Fishler and Bolles in 1981 is the a widely used robust estimator in the field of computer vision. The algorithm is capable of providing good estimates from data contaminated by large (even significantly more than 50{\%}) fraction of outliers. RANSAC is an optimization method that uses a data-driven random sampling of the parameter space to find the extremum of the cost function. Samples of data define points of the parameter space in which the cost function is evaluated and model parameters with the best score are output. This thesis provides a detailed analysis of RANSAC, which is recast as time-constrained op- timization – a solution that is optimal with certain confidence is sought in the shortest possible time. Next, the concept of randomized cost function evaluation in RANSAC is introduced and its superiority over the deterministic evaluation is shown. A provably optimal strategy for the ran- domized cost function evaluation is derived. A known discrepancy, caused by noise on inliers, between theoretical prediction of the time required to find the solution and practically observed running times is traced to a tacit assump- tions of RANSAC. The proposed LO-RANSAC algorithm reaches almost perfect agreement with theoretical predictions without any negative impact on the time complexity. A unified method of estimation of model and its degenerate configuration (epipolar geome- try and homography of a dominant plane) at the same time without a priori knowledge of the presence of the degenerate configuration (dominant plane) is derived. Next, it is shown that using oriented geometric constraints that arise from a realistic model of physical camera devices, saves non-negligible fraction of computational time. No negative side effect are related to the application of the oriented constraints. An algorithm exploiting (possibly noisy) match quality to modify the sampling strategy is introduced. The quality of a match is an often freely available quantity in the matching prob- lem. The approach increases the efficiency of the algorithm while keeping the same robustness as RANSAC in the worst-case situation (when the match quality is unrelated to whether a corre- spondence is a mismatch or not). Most of the algorithms in the thesis are motivated by (and presented on) estimation of a multi- view geometry. The algorithms are, however, general robust estimation techniques and can be easily used in other application areas too.},
author = {Chum, Ondrej},
doi = {10.1016/S0921-8777(99)00013-0},
isbn = {3015947974},
issn = {1213-2365},
journal = {Phd Thesis},
pmid = {10422537},
title = {{Two-View Geometry Estimation by Random Sample and Consensus}},
url = {http://cmp.felk.cvut.cz/{~}chum/papers/Chum-PhD.pdf},
year = {2005}
}
@article{WHO2018,
abstract = {As matter fact road traffic injuries are currently ranked ninth globally among the leading causes of disability adjusted life years lost, and the ranking is projected to rise to third by 2020(Ishrat Riaz, 2018)},
author = {WHO},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/WHO/Global Status Report on Road. WHO. 2018.pdf:pdf},
journal = {World Heal. Organ.},
pages = {20},
title = {{Global Status Report on Road}},
url = {https://www.who.int/violence{\_}injury{\_}prevention/road{\_}safety{\_}status/2018/en/},
year = {2018}
}
@article{Geiger2013a,
abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high- resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to inner- city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide},
author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger et al/Vision meets robotics The KITTI dataset. The International Journal of Robotics Research. Geiger et al.. 2013.pdf:pdf},
journal = {Ijrr},
number = {October},
pages = {1--6},
title = {{Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research}},
year = {2013}
}
@article{Roca2006a,
archivePrefix = {arXiv},
arxivId = {arXiv:1208.5721},
author = {Roca, Sergi Figueras and Cited, References},
doi = {10.1038/incomms1464},
eprint = {arXiv:1208.5721},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Roca, Cited/( 12 ) United States Patent. Roca, Cited. 2006.pdf:pdf},
isbn = {1020080012051},
issn = {2004001828},
number = {12},
pmid = {1000182772},
title = {{( 12 ) United States Patent}},
volume = {2},
year = {2006}
}
@book{1385,
author = {Richard, Hartley and Andrew, Zisserman},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/غلامحسین/Multiple View Geometry in computer vision. غلامحسین. 1385.pdf:pdf},
isbn = {9780521540513},
pages = {302},
title = {{Multiple View Geometry in computer vision}},
year = {1385}
}
@article{Fridman2017,
abstract = {For the foreseeble future, human beings will likely remain an integral part of the driving task, monitoring the AI system as it performs anywhere from just over 0{\%} to just under 100{\%} of the driving. The governing objectives of the MIT Autonomous Vehicle Technology (MIT-AVT) study are to (1) undertake large-scale real-world driving data collection that includes high-definition video to fuel the development of deep learning based internal and external perception systems, (2) gain a holistic understanding of how human beings interact with vehicle automation technology by integrating video data with vehicle state data, driver characteristics, mental models, and self-reported experiences with technology, and (3) identify how technology and other factors related to automation adoption and use can be improved in ways that save lives. In pursuing these objectives, we have instrumented 23 Tesla Model S and Model X vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6 vehicles for both long-term (over a year per driver) and medium term (one month per driver) naturalistic driving data collection. Furthermore, we are continually developing new methods for analysis of the massive-scale dataset collected from the instrumented vehicle fleet. The recorded data streams include IMU, GPS, CAN messages, and high-definition video streams of the driver face, the driver cabin, the forward roadway, and the instrument cluster (on select vehicles). The study is on-going and growing. To date, we have 122 participants, 15,610 days of participation, 511,638 miles, and 7.1 billion video frames. This paper presents the design of the study, the data collection hardware, the processing of the data, and the computer vision algorithms currently being used to extract actionable knowledge from the data.},
archivePrefix = {arXiv},
arxivId = {1711.06976},
author = {Fridman, Lex and Brown, Daniel E. and Glazer, Michael and Angell, William and Dodd, Spencer and Jenik, Benedikt and Terwilliger, Jack and Patsekin, Aleksandr and Kindelsberger, Julia and Ding, Li and Seaman, Sean and Mehler, Alea and Sipperley, Andrew and Pettinato, Anthony and Seppelt, Bobbie and Angell, Linda and Mehler, Bruce and Reimer, Bryan},
eprint = {1711.06976},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fridman et al/MIT Autonomous Vehicle Technology Study Large-Scale Deep Learning Based Analysis of Driver Behavior and Interaction with Automation. Fri.pdf:pdf},
pages = {1--16},
title = {{MIT Autonomous Vehicle Technology Study: Large-Scale Deep Learning Based Analysis of Driver Behavior and Interaction with Automation}},
url = {http://arxiv.org/abs/1711.06976},
year = {2017}
}
