Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Levenberg1943,
abstract = {The standard method for solving least squares problems which lead to non-linear normal equations depends upon a reduction of the residuals to linear form by first order Taylor approximations taken about an initial or trial solution for the parameters .2 If the usual least squares procedure, performed with these linear approximations , yields new values for the parameters which are not sufficiently close to the initial values, the neglect of second and higher order terms may invalidate the process, and may actually give rise to a larger value of the sum of the squares of the residuals than that corresponding to the initial solution. This failure of the standard method to improve the initial solution has received some notice in statistical applications of least squares3 and has been encountered rather frequently in connection with certain engineering applications involving the approximate representation of one function by another. The purpose of this article is to show how the problem may be solved by an extension of the standard method which insures improvement of the initial solution.4 The process can also be used for solving non-linear simultaneous equations, in which case it may be considered an extension of Newton's method.},
author = {Levenberg, Kenneth and Arsenal, Frankford},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Levenberg, Arsenal/A Method for the Solution of Certain Non-Linear Problems in Least Squares. Levenberg, Arsenal. 1943.pdf:pdf},
journal = {Q. Appl. Math.},
number = {278},
pages = {536--538},
title = {{A Method for the Solution of Certain Non-Linear Problems in Least Squares}},
volume = {1},
year = {1943}
}
@misc{cameraCalibrationRos,
author = {{James Bowman} and {Patrick Mihelich} and {Vincent Rabaud}},
title = {{camera{\_}calibration - ROS Wiki}},
url = {https://wiki.ros.org/camera{\_}calibration},
urldate = {2019-10-27}
}
@article{NGuessan2017,
abstract = {There exist many iterative methods for computing the maximum likelihood estimator but most of them suffer from one or several drawbacks such as the need to inverse a Hessian matrix and the need to find good initial approximations of the parameters that are unknown in practice. In this paper, we present an estimation method without matrix inversion based on a linear approximation of the likelihood equations in a neighborhood of the constrained maximum likelihood estimator. We obtain closed-form approximations of solutions and standard errors. Then, we propose an iterative algorithm which cycles through the components of the vector parameter and updates one component at a time. The initial solution, which is necessary to start the iterative procedure, is automated. The proposed algorithm is compared to some of the best iterative optimization algorithms available on R and MATLAB software through a simulation study and applied to the statistical analysis of a road safety measure.},
author = {N'Guessan, Assi and Geraldo, Issa Cherif and Hafidi, Bezza},
doi = {10.4236/ojs.2017.71011},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/N'Guessan, Geraldo, Hafidi/An Approximation Method for a Maximum Likelihood Equation System and Application to the Analysis of Accidents Data. N'Guessan, Geraldo.pdf:pdf},
issn = {2161-718X},
journal = {Open J. Stat.},
keywords = {Constrained Maximum Likelihood, Partial Linear App,constrained maximum likelihood,iterative algorithms,partial linear approximation,road safety measure,s complement,schur},
number = {01},
pages = {132--152},
title = {{An Approximation Method for a Maximum Likelihood Equation System and Application to the Analysis of Accidents Data}},
volume = {07},
year = {2017}
}
@misc{azurecv,
title = {{Image Processing with the Computer Vision API | Microsoft Azure}},
url = {https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/},
urldate = {2019-10-22}
}
@article{Geiger2012,
abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
doi = {10.1109/CVPR.2012.6248074},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger, Lenz, Urtasun/Are we ready for autonomous driving the KITTI vision benchmark suite. Geiger, Lenz, Urtasun. 2012.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {3354--3361},
title = {{Are we ready for autonomous driving? the KITTI vision benchmark suite}},
year = {2012}
}
@misc{boofcv,
title = {{BoofCV}},
url = {https://boofcv.org/index.php?title=Main{\_}Page},
urldate = {2019-10-22}
}
@book{Merklinger1993,
author = {Merklinger, Harold M.},
isbn = {0969502524},
pages = {104},
publisher = {H.M. Merklinger},
title = {{Focusing the view camera : a scientific way to focus the view camera and estimate depth of field}},
year = {1993}
}
@article{Geiger2012a,
abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera- to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experi- ments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions. I.},
author = {Geiger, Andreas and Moosmann, Frank and Car, {\"{O}}mer and Schuster, Bernhard},
doi = {10.1109/ICRA.2012.6224570},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger et al/Automatic camera and range sensor calibration using a single shot. Geiger et al.. 2012.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proc. - IEEE Int. Conf. Robot. Autom.},
pages = {3936--3943},
title = {{Automatic camera and range sensor calibration using a single shot}},
year = {2012}
}
@misc{opencv,
title = {{OpenCV}},
url = {https://opencv.org/},
urldate = {2019-10-22}
}
@article{Morrison1960,
author = {Morrison, David D.},
journal = {Proc. Jet Propuls. Lab. Semin. Track. Programs Orbit Determ.},
pages = {1--9},
title = {{Methods for nonlinear least squares problems and convergence proofs}},
year = {1960}
}
@misc{AVTROSdriver,
author = {{Miquel Massot}},
title = {{avt{\_}vimba{\_}camera - ROS Wiki}},
url = {https://wiki.ros.org/avt{\_}vimba{\_}camera},
urldate = {2019-10-27}
}
@misc{OpenCV_camera_calib,
author = {OpenCV},
title = {{OpenCV: Camera calibration With OpenCV}},
url = {https://docs.opencv.org/3.2.0/d4/d94/tutorial{\_}camera{\_}calibration.html https://opencv-python-tutroals.readthedocs.io/en/latest/py{\_}tutorials/py{\_}calib3d/py{\_}calibration/py{\_}calibration.html},
urldate = {2019-10-22}
}
@misc{Photopillers,
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Cladera/The Ultimate Photography Guide to Depth of Field (DoF) PhotoPills. Cladera. Unknown.pdf:pdf},
title = {{The Ultimate Photography Guide to Depth of Field (DoF) | PhotoPills}},
url = {https://www.photopills.com/articles/ultimate-guide-depth-field},
urldate = {2019-10-21}
}
@article{Roger1987,
author = {Tsai, Roger Y},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Tsai/Tsai{\_}Calibration.Pdf. Tsai. 1987.pdf:pdf},
journal = {IEEE J. ofRobotvs Autom.},
title = {{Tsai{\_}Calibration.Pdf}},
volume = {RA-3},
year = {1987}
}
@misc{simplecv,
title = {{SimpleCV Tutorial — Tutorial}},
url = {http://tutorial.simplecv.org/en/latest/},
urldate = {2019-10-22}
}
@article{Zhang2000,
abstract = {We propose a flexible new technique to easily calibrate a camera. It is well suited for use without specialized knowledge of 3D geometry or computer vision. The technique only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique, and very good results have been obtained. Compared with classical techniques which use expensive equipments such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one step from laboratory environments to real world use.},
author = {Zhang, Zhengyou},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Zhengyou/A Flexible New Technique for Camera Calibration. Zhengyou. 2000.pdf:pdf},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
number = {11},
pages = {1330--1334},
title = {{A Flexible New Technique for Camera Calibration}},
url = {https://www.microsoft.com/en-us/research/publication/a-flexible-new-technique-for-camera-calibration/},
volume = {22},
year = {2000}
}
@article{Hata,
author = {Hata, Kenji and Savarese, Silvio},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hata, Savarese/CS231A Course Notes 1 Camera Models. Hata, Savarese. Unknown.pdf:pdf},
pages = {1--16},
title = {{CS231A Course Notes 1: Camera Models}}
}
@book{Xu1996,
author = {Xú, Gang. and Zhang, Zhengyou},
isbn = {0792341996},
pages = {313},
publisher = {Kluwer Academic Publishers},
title = {{Epipolar geometry in stereo, motion, and object recognition : a unified approach}},
url = {https://books.google.com/books?id=DnFaUidM-B0C{\&}pg=PA7{\&}dq=pinhole+intitle:{\%}22Epipolar+geometry{\%}22},
year = {1996}
}
@article{Menze2015a,
abstract = {This paper proposes a novel model and dataset for 3D scene flow estimation with an application to autonomous driving. Taking advantage of the fact that outdoor scenes often decompose into a small number of independently mov- ing objects, we represent each element in the scene by its rigid motion parameters and each superpixel by a 3D plane as well as an index to the corresponding object. This minimal representation increases robustness and leads to a discrete-continuous CRF where the data term decomposes into pairwise potentials between superpixels and objects. Moreover, our model intrinsically segments the scene into its constituting dynamic components. We demonstrate the performance of our model on existing benchmarks as well as a novel realistic dataset with scene flow ground truth. We obtain this dataset by annotating 400 dynamic scenes from the KITTI raw data collection using detailed 3D CAD models for all vehicles in motion. Our experiments also re- veal novel challenges which cannot be handled by existing methods. 1.},
author = {Menze, Moritz and Geiger, Andreas},
doi = {10.1109/CVPR.2015.7298925},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Menze, Geiger/Object scene flow for autonomous vehicles. Menze, Geiger. 2015.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {3061--3070},
title = {{Object scene flow for autonomous vehicles}},
volume = {07-12-June},
year = {2015}
}
@misc{Bouguet2010,
author = {Bouguet, Jean-Yves},
title = {{Camera Calibration Toolbox for Matlab}},
url = {http://www.vision.caltech.edu/bouguetj/calib{\_}doc/},
urldate = {2019-10-22}
}
@article{camera_models,
author = {Hata, Kenji and Savarese, Silvio},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hata, Savarese/CS231A Course Notes 1 Camera Models. Hata, Savarese. Unknown.pdf:pdf},
pages = {1--16},
title = {{CS231A Course Notes 1: Camera Models}}
}
@misc{awsRekognition,
author = {Aws},
title = {{Amazon Rekognition – Video and Image - AWS}},
url = {https://aws.amazon.com/rekognition/},
urldate = {2019-10-22}
}
@inproceedings{Heikkila1997,
abstract = {In geometrical camera calibration the objective is to determine a set of camera parameters that describe the mapping between 3-D reference coordinates and 2-D image coordinates. Various methods for camera calibration can be found from the literature. However surprisingly little attention has been paid to the whole calibration procedure, i.e., control point extraction from images, model fitting, image correction, and errors originating in these stages. The main interest has been in model fitting, although the other stages are also important. In this paper we present a four-step calibration procedure that is an extension to the two-step method. There is an additional step to compensate for distortion caused by circular features, and a step for correcting the distorted image coordinates. The image correction is performed with an empirical inverse model that accurately compensates for radial and tangential distortions. Finally, a linear method for solving the parameters of the inverse model is presented.},
author = {Heikkila, J and Silven, O},
booktitle = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
doi = {10.1109/CVPR.1997.609468},
keywords = {Calibration,Cameras,Closed-form solution,Error correction,Geometrical optics,Inverse problems,Machine vision,Mathematical model,Minimization methods,Nonlinear distortion,calibration,camera calibration,computer vision,geometrical camera calibration,image correction,implicit image correction,inverse model,model fitting,three-dimensional machine vision},
month = {jun},
pages = {1106--1112},
title = {{A four-step camera calibration procedure with implicit image correction}},
year = {1997}
}
@misc{comercial_cameras,
title = {{03 – Different Types of Cameras » Photo Class}},
url = {http://www.r-photoclass.com/03-different-types-of-cameras/},
urldate = {2019-10-23}
}
@misc{googlevision,
title = {{Vision AI | Derive Image Insights via ML | Cloud Vision API | Google Cloud}},
url = {https://cloud.google.com/vision/},
urldate = {2019-10-22}
}
@misc{matlabcvtoolbox,
title = {{Computer Vision Toolbox - MATLAB {\&} Simulink}},
url = {https://www.mathworks.com/products/computer-vision.html},
urldate = {2019-10-22}
}
@article{Sturm2010,
abstract = {This survey is mainly motivated by the increased availability and use of panoramic image acquisition devices, in computer vision and various of its applications. Different technologies and different computational models thereof exist and algorithms and theoretical studies for geometric computer vision ("structure-from-motion") are often re-developed without highlighting common underlying principles. One of the goals of this survey is to give an overview of image acquisition methods used in computer vision and especially, of the vast number of camera models that have been proposed and investigated over the years, where we try to point out similarities between different models. Results on epipolar and multi-view geometry for different camera models are reviewed as well as various calibration and self-calibration approaches, with an emphasis on non-perspective cameras.We finally describe what we consider are fundamental building blocks for geometric computer vision or structure-from-motion: epipolar geometry, pose and motion estimation, 3D scene modeling, and bundle adjustment. The main goal here is to highlight the main principles of these, which are independent of specific camera models.},
author = {Sturm, Peter and Ramalingam, Srikumar and Tardif, Jean Philippe and Gasparini, Simone and Barreto, Jo{\~{a}}o},
doi = {10.1561/0600000023},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Sturm et al/Camera models and fundamental concepts used in geometric computer vision. Sturm et al.. 2010.pdf:pdf},
issn = {15722740},
journal = {Found. Trends Comput. Graph. Vis.},
number = {1-2},
pages = {1--183},
title = {{Camera models and fundamental concepts used in geometric computer vision}},
volume = {6},
year = {2010}
}
@article{Lourakis2005,
abstract = {In order to obtain optimal 3D structure and viewing parameter estimates, bundle adjustment is often used as the last step of feature-based structure and motion estimation algorithms. Bundle adjustment involves the formulation of a large scale, yet sparse minimization problem, which is traditionally solved using a sparse variant of the Levenberg-Marquardt optimization algorithm that avoids storing and operating on zero entries. This paper argues that considerable computational benefits can be gained by substituting the sparse Levenberg-Marquardt algorithm in the implementation of bundle adjustment with a sparse variant of Powell's dog leg non-linear least squares technique. Detailed comparative experimental results provide strong evidence supporting this claim. {\textcopyright} 2005 IEEE.},
author = {Lourakis, Manolis I.A. and Argyros, Antonis A.},
doi = {10.1109/ICCV.2005.128},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lourakis, Argyros/Is Levenberg-Marquardt the most efficient optimization algorithm for implementing bundle adjustment. Lourakis, Argyros. 2005.pdf:pdf},
isbn = {076952334X},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
pages = {1526--1531},
title = {{Is Levenberg-Marquardt the most efficient optimization algorithm for implementing bundle adjustment?}},
volume = {II},
year = {2005}
}
@misc{watson,
title = {{Watson Visual Recognition}},
url = {https://www.ibm.com/watson/services/visual-recognition/},
urldate = {2019-10-22}
}
@article{Ekstrom2015,
abstract = {Place cells are a fundamental component of the rodent navigational system. One intriguing implication of place cells is that humans, by extension, have "map-like" (or GPS-like) knowledge that we use to represent space. Here, we review both behavioral and neural studies of human navigation, suggesting that how we process visual information forms a critical component of how we represent space. These include cellular and brain systems devoted to coding visual information during navigation in addition to a location coding system similar to that described in rodents. Together, these findings suggest that while it is highly useful to think of our navigation system involving internal "maps," we should not neglect the importance of high-resolution visual representations to how we navigate space.},
author = {Ekstrom, Arne D.},
doi = {10.1002/hipo.22449},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Ekstrom/Why vision is important to how we navigate. Ekstrom. 2015.pdf:pdf},
issn = {10981063},
journal = {Hippocampus},
keywords = {Allocentric,Cognitive map,Egocentric,Hippocampus,Humans,Path integration,Spatial navigation},
number = {6},
pages = {731--735},
title = {{Why vision is important to how we navigate}},
volume = {25},
year = {2015}
}
@article{Gavin2019,
abstract = {The Levenberg-Marquardt algorithm was developed in the early 1960's to solve ne onlinear least squares problems. Least squares problems arise in the context of fitting a parameterized function to a set of measured data points by minimizing the sum of the squares of the errors between the data points and the function. If the fit function is not linear in the parameters the least squares problem is nonlinear. Nonlinear least squares methods iteratively reduce the sum of the squares of the errors between the function and the measured data points through a sequence of updates to parameter values. The Levenberg-Marquardt algorithm combines two minimization methods: the gradient descent method and the Gauss-Newton method. In the gradient descent method, the sum of the squared errors is reduced by updating the parameters in the steepest-descent direction. In the Gauss-Newton method, the sum of the squared errors is reduced by assuming the least squares function is locally quadratic, and finding the minimum of the quadratic. The Levenberg-Marquardt method acts more like a gradient-descent method when the parameters are far from their optimal value, and acts more like the Gauss-Newton method when the parameters are close to their optimal value. This document describes these methods and illustrates the use of software to solve nonlinear least squares curve-fitting problems.},
author = {Gavin, Henri P.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Gavin/The Levenburg-Marqurdt Algorithm For Nonlinear Least Squares Curve-Fitting Problems. Gavin. 2019.pdf:pdf},
journal = {Duke Univ.},
pages = {1--19},
title = {{The Levenburg-Marqurdt Algorithm For Nonlinear Least Squares Curve-Fitting Problems}},
url = {http://people.duke.edu/{~}hpgavin/ce281/lm.pdf},
year = {2019}
}
@book{Beck1983,
abstract = {Human and Machine Vision provides information pertinent to an interdisciplinary program of research in visual perception. This book presents a psychophysical study of the human visual system, which provides insights on how to model the flexibility required by a general-purpose visual system. Organized into 17 chapters, this book begins with an overview of how a visual display is segmented into components on the basis of textual differences. This text then proposes three criteria for judging representations of shape. Other chapters consider an increased use of machine vision programs as models of human vision and of data from human vision in developing programs for machine vision. This book discusses as well the diversity and flexibility of systems for representing visual information. The final chapter deals with dot patterns and discusses the process of interring orientation information from collections of them. This book is a valuable resource for psychologists, neurophysiologists, and computer scientists.},
author = {Beck, Jacob and Hope, Barbara and Rosenfeld, Azriel},
doi = {10.1016/C2013-0-10347-9},
edition = {1},
isbn = {9780120843206},
publisher = {Elsevier},
title = {{Human and Machine Vision}},
url = {https://linkinghub.elsevier.com/retrieve/pii/C20130103479},
year = {1983}
}
@misc{dlib,
title = {{dlib C++ Library}},
url = {http://dlib.net/},
urldate = {2019-10-22}
}
@article{Fritsch2013,
abstract = {Detecting the road area and ego-lane ahead of a vehicle is central to modern driver assistance systems. While lane-detection on well-marked roads is already available in modern vehicles, finding the boundaries of unmarked or weakly marked roads and lanes as they appear in inner-city and rural environments remains an unsolved problem due to the high variability in scene layout and illumination conditions, amongst others. While recent years have witnessed great interest in this subject, to date no commonly agreed upon benchmark exists, rendering a fair comparison amongst methods difficult. In this paper, we introduce a novel open-access dataset and benchmark for road area and ego-lane detection. Our dataset comprises 600 annotated training and test images of high variability from the KITTI autonomous driving project, capturing a broad spectrum of urban road scenes. For evaluation, we propose to use the 2D Bird's Eye View (BEV) space as vehicle control usually happens in this 2D world, requiring detection results to be represented in this very same space. Furthermore, we propose a novel, behavior-based metric which judges the utility of the extracted ego-lane area for driver assistance applications by fitting a driving corridor to the road detection results in the BEV. We believe this to be important for a meaningful evaluation as pixel-level performance is of limited value for vehicle control. State-of-the-art road detection algorithms are used to demonstrate results using classical pixel-level metrics in perspective and BEV space as well as the novel behavior-based performance measure. All data and annotations are made publicly available on the KITTI online evaluation website in order to serve as a common benchmark for road terrain detection algorithms.},
author = {Fritsch, Jannik and Kuhnl, Tobias and Geiger, Andreas},
doi = {10.1109/ITSC.2013.6728473},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Fritsch, Kuhnl, Geiger/A new performance measure and evaluation benchmark for road detection algorithms. Fritsch, Kuhnl, Geiger. 2013.pdf:pdf},
isbn = {9781479929146},
journal = {IEEE Conf. Intell. Transp. Syst. Proceedings, ITSC},
pages = {1693--1700},
title = {{A new performance measure and evaluation benchmark for road detection algorithms}},
year = {2013}
}
@misc{opencv_doc,
author = {OpenCV},
title = {{Camera Calibration and 3D Reconstruction — OpenCV 2.4.13.7 documentation}},
url = {https://docs.opencv.org/2.4/modules/calib3d/doc/camera{\_}calibration{\_}and{\_}3d{\_}reconstruction.html},
urldate = {2019-04-21}
}
@article{Geiger2013a,
abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high- resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to inner- city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide},
author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Geiger et al/Vision meets robotics The KITTI dataset. The International Journal of Robotics Research. Geiger et al.. 2013.pdf:pdf},
journal = {Ijrr},
number = {October},
pages = {1--6},
title = {{Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research}},
year = {2013}
}
@article{manuapphotogrammetry,
author = {Jones, Alan D},
doi = {10.1080/00690805.1982.10438226},
journal = {Cartography},
number = {4},
pages = {258},
publisher = {Taylor {\&} Francis},
title = {{Manual of Photogrammetry, eds C.C. Slama, C. Theurer and S.W. Hendrikson, American Society of Photogrammetry, Falls Church, Va., 1980, Fourth Edition, 180 × 260mm, xvi and 1056 pages (with index), 72 tables, 866 figures. ISBN 0 937294 01 2.}},
url = {https://doi.org/10.1080/00690805.1982.10438226},
volume = {12},
year = {1982}
}
@misc{vlfeat,
author = {Vedaldi, A. and Fulkerson, B.},
title = {{VLFeat: An Open and Portable Library of Computer Vision Algorithms}},
url = {http://www.vlfeat.org/},
year = {2008}
}
