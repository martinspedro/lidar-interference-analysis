Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Open3D,
abstract = {Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.},
archivePrefix = {arXiv},
arxivId = {1801.09847},
author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
eprint = {1801.09847},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Zhou, Park, Koltun/Open3D A Modern Library for 3D Data Processing. Zhou, Park, Koltun. 2018.pdf:pdf},
month = {jan},
title = {{Open3D: A Modern Library for 3D Data Processing}},
url = {http://arxiv.org/abs/1801.09847},
year = {2018}
}
@misc{Hesai,
title = {{HESAI}},
url = {https://www.hesaitech.com/en},
urldate = {2019-11-28}
}
@inproceedings{Stettner2010,
abstract = {The theory and operation of Advanced Scientific Concepts, Inc.'s (ASC) latest compact 3D Flash LIDAR Video Cameras (3D FLVCs) and a growing number of technical problems and solutions are discussed. The solutions range from space shuttle docking, planetary entry, decent and landing, surveillance, autonomous and manned ground vehicle navigation and 3D imaging through particle obscurants.},
author = {Stettner, Roger},
booktitle = {Laser Radar Technol. Appl. XV},
doi = {10.1117/12.851831},
editor = {Turner, Monte D and Kamerman, Gary W},
keywords = {3D Imaging,Flash LIDAR,Range Gated Imaging},
organization = {International Society for Optics and Photonics},
pages = {39--46},
publisher = {SPIE},
title = {{Compact 3D flash lidar video cameras and applications}},
url = {https://doi.org/10.1117/12.851831},
volume = {7684},
year = {2010}
}
@article{TexasLiDAR,
author = {Carbone, Marco},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Carbone/An introduction to automotive SDR. Carbone. 2018.pdf:pdf},
issn = {00134953},
journal = {Electron. Prod.},
number = {2},
title = {{An introduction to automotive SDR}},
volume = {61},
year = {2018}
}
@misc{Velodyne,
abstract = {Velodyne now offers an improved high definition LiDAR scanner designed for autonomous vehicle navigation, mapping, surveying, industrial automation, and other uses. The S3 version of the HDL-64E provides improved accuracy and a higher data rate than the original version. Capture high definition 3 dimensional information about the surrounding environment. High Field of View, High Frame Rate With its full 360 HFOV by 26.8 VFOV, the HDL-64E provides significantly more environmental information than previously available. With its 5 -20 Hz user-selectable frame rate and over 2.2 million points per second output rate, the HDL-64E S3 provides all the distance sensing data you'll ever need. The unit's development has been focused on high data rate, high robustness, accuracy and simple 100 MBPS Ethernet interfacing to the end user. Traditional LiDAR sensors have relied upon a single laser firing into a mechanically actuated mirror, providing only one plane of view. The HDL-64E S3's patented one-piece design uses 64 fixed-mounted lasers to measure the surrounding environment, each mechanically mounted to a specific vertical angle, with the entire unit spinning. This approach dramatically increases reliability, FOV, and point cloud density.},
author = {Velodyne},
booktitle = {Http://Velodynelidar.Com/},
pages = {2800},
title = {{Velodyne Lidar}},
url = {https://velodynelidar.com/},
urldate = {2019-11-27},
year = {2013}
}
@inproceedings{Gelbart2002,
abstract = {A 64x64-pixel Flash (scannerless) lidar system that uses a streak tube to achieve range-resolved images is demonstrated. An array of glass fibers maps light from an area in the focal plane of an imaging lens to multiple rows of fibers on the streak tube's photocathode. The time-resolved backscatter return for all 4096 image pixels is recorded during one integration-time of a CCD camera that is coupled to the streak tube's phosphor screen. Data processing yields 64x64-pixel contrast (intensity) and range images for each laser pulse. Range precision better than 2.5{\%} of the range extent is exhibited for a wide variety of targets and terrains at image rates up to 100Hz. Field test imagery demonstrated the capability of the Flash lidar system for imaging vehicles hidden by a tree canopy as well as for imaging sub-surface mine-like targets in the ocean.},
author = {Gelbart, Asher and Redman, Brian C and Light, Robert S and Schwartzlow, Coreen A and Griffis, Andrew J},
booktitle = {Laser Radar Technol. Appl. VII},
doi = {10.1117/12.476407},
editor = {Kamerman, Gary W},
keywords = {3D,Flash ladar,imaging lidar,laser radar,scannerless lidar,streak tube},
organization = {International Society for Optics and Photonics},
pages = {9--18},
publisher = {SPIE},
title = {{Flash lidar based on multiple-slit streak tube imaging lidar}},
url = {https://doi.org/10.1117/12.476407},
volume = {4723},
year = {2002}
}
@misc{Quanergy,
author = {Quanergy},
title = {{Quanergy – LiDAR sensors and smart sensing solutions}},
url = {https://quanergy.com/},
urldate = {2019-11-27}
}
@misc{Yu2016,
author = {Yu, Tianyue and Yu, Tianyue and Pacala, Angus},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yu, Yu, Pacala/(12) Patent Application Publication (10) Pub. No. US 20160161600 A1. Yu, Yu, Pacala. 2016.pdf:pdf},
number = {19},
title = {{(12) Patent Application Publication (10) Pub. No.: US 2016/0161600 A1}},
volume = {1},
year = {2016}
}
@misc{Lilumination2011,
author = {Lilumination, Field O F},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Lilumination/(12) United States Patent. Lilumination. 2011.pdf:pdf},
number = {12},
title = {{(12) United States Patent}},
volume = {1},
year = {2011}
}
@article{Hecht2018,
author = {Hecht, Jeff},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Hecht/Lidar for Self-Dr riving Cars. Hecht. 2018.pdf:pdf},
number = {January},
pages = {26--33},
title = {{Lidar for Self-Dr riving Cars}},
year = {2018}
}
@misc{Ouster,
author = {Ouster},
title = {{Ouster Lidar}},
url = {https://ouster.com/blog/how-multi-beam-flash-lidar-works},
urldate = {2019-10-24}
}
@misc{LeddarTech,
abstract = {Leddar: high performance, affordable LiDAR solid-state sensing technology for automotive, drones, collision avoidance, ITS and more},
author = {Leddartech},
title = {{LeddarTech - Mastering LiDAR Sensor Technology}},
url = {https://leddartech.com/},
urldate = {2019-11-27},
year = {2018}
}
@misc{Ouster,
author = {Ouster},
title = {{High-resolution digital lidar: autonomous vehicles, robotics, drones | Ouster}},
url = {https://ouster.com/},
urldate = {2019-11-27}
}
@misc{PDAL,
author = {Contributors, PDAL},
doi = {10.5281/zenodo.2556738},
title = {{PDAL Point Data Abstraction Library}},
url = {https://doi.org/10.5281/zenodo.2556738},
year = {2018}
}
@article{Wang2017,
abstract = {Optical phased array (OPA) is a key enabling element for solid state LIDAR (light detection and ranging). In this paper, we demonstrate a novel MEMS micromirror array OPA. Vertical combdrive actuators are integrated underneath the mirrors to achieve a small pitch (2.4$\mu$m) and a large field of view (22° at 905nm wavelength and 40° at 1550nm). The OPA has 2$\mu$s response time, and 10V actuation voltage.},
author = {Wang, Youmin and Wu, Ming C.},
doi = {10.1109/MEMSYS.2017.7863553},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Wang, Wu/Micromirror based optical phased array for wide-angle beamsteering. Wang, Wu. 2017.pdf:pdf},
isbn = {9781509050789},
issn = {10846999},
journal = {Proc. IEEE Int. Conf. Micro Electro Mech. Syst.},
pages = {897--900},
publisher = {IEEE},
title = {{Micromirror based optical phased array for wide-angle beamsteering}},
year = {2017}
}
@misc{Hall2011,
author = {Hall, David S.},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Mccaffrey/( 12 ) United States Patent. Mccaffrey. 2019.pdf:pdf},
title = {{( 12 ) United States Patent}},
year = {2011}
}
@misc{Templeton,
author = {Templeton, Brad},
title = {{Elon Musk's War On LIDAR: Who Is Right And Why Do They Think That?}},
url = {https://www.forbes.com/sites/bradtempleton/2019/05/06/elon-musks-war-on-lidar-who-is-right-and-why-do-they-think-that/},
urldate = {2019-10-24}
}
@article{Payne2009,
abstract = {A number of full field image sensors have been developed that are capable of simultaneously measuring intensity and distance (range) for every pixel in a given scene using an indirect time-of-flight measurement technique. A light source is intensity modulated at a frequency between 10-100 MHz, and an image sensor is modulated at the same frequency, synchronously sampling light reflected from objects in the scene (homodyne detection). The time of flight is manifested as a phase shift in the illumination modulation envelope, which can be determined from the sampled data simultaneously for each pixel in the scene. This paper presents a method of characterizing the high frequency modulation response of these image sensors, using a pico-second laser pulser. The characterization results allow the optimal operating parameters, such as the modulation frequency, to be identified in order to maximize the range measurement precision for a given sensor. A number of potential sources of error exist when using these sensors, including deficiencies in the modulation waveform shape, duty cycle, or phase, resulting in contamination of the resultant range data. From the characterization data these parameters can be identified and compensated for by modifying the sensor hardware or through post processing of the acquired range measurements.},
author = {Payne, Andrew D. and Dorrington, Adrian A. and Cree, Michael J. and Carnegie, Dale A.},
doi = {10.1117/12.806007},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Payne et al/Characterization of modulated time-of-flight range image sensors. Payne et al.. 2009.pdf:pdf},
issn = {0277786X},
journal = {Three-Dimensional Imaging Metrol.},
number = {June},
pages = {723904},
title = {{Characterization of modulated time-of-flight range image sensors}},
volume = {7239},
year = {2009}
}
@article{Simpson2019,
author = {Simpson, John T and Us, T N},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Simpson, Us/( 12 ) United States Patent. Simpson, Us. 2019.pdf:pdf},
number = {12},
title = {{( 12 ) United States Patent}},
volume = {2},
year = {2019}
}
@inproceedings{Fersch2017a,
abstract = {This paper discusses the current technical limitations posed on endeavors to miniaturize lidar systems for use in automotive applications and how to possibly extend those limits. The focus is set on long-range scanning direct time of flight LiDAR systems using APD photodetectors. Miniaturization evokes severe problems in ensuring absolute laser safety while maintaining the systems' performance in terms of maximum range, signal-to-noise ratio, detection probability, pixel density, or frame rate. Based on hypothetical but realistic specifications for an exemplary system the complete lidar signal path is calculated. The maximum range of the system is used as a general performance indicator. It is determined with the minimum signal-to-noise ratio required to detect an object. Various system parameters are varied to find their impact on the system's range. The reduction of the laser's pulse width and the right choice for the transimpedance amplifier's amplification have shown to be practicable measures to double the system's range.},
author = {Fersch, Thomas and Weigel, Robert and Koelpin, Alexander},
booktitle = {Three-Dimensional Imaging, Vis. Disp. 2017},
doi = {10.1117/12.2260894},
editor = {Javidi, Bahram and Son, Jung-Young and Matoba, Osamu},
keywords = {automotive,avalanche photo diode,ladar,laser radar,lidar,time-of-flight},
organization = {International Society for Optics and Photonics},
pages = {160--171},
publisher = {SPIE},
title = {{Challenges in miniaturized automotive long-range lidar system design}},
url = {https://doi.org/10.1117/12.2260894},
volume = {10219},
year = {2017}
}
@article{Conroy2009,
abstract = {Range imaging cameras measure depth simultaneously for every pixel in a given field of view. In most implementations the basic operating principles are the same. A scene is illuminated with an intensity modulated light source and the reflected signal is sampled using a gain-modulated imager. Previously we presented a unique heterodyne range imaging system that employed a bulky and power hungry image intensifier as the high speed gain-modulation mechanism. In this paper we present a new range imager using an internally modulated image sensor that is designed to operate in heterodyne mode, but can also operate in homodyne mode. We discuss homodyne and heterodyne range imaging, and the merits of the various types of hardware used to implement these systems. Following this we describe in detail the hardware and firmware components of our new ranger. We experimentally compare the two operating modes and demonstrate that heterodyne operation is less sensitive to some of the limitations suffered in homodyne mode, resulting in better linearity and ranging precision characteristics. We conclude by showing various qualitative examples that demonstrate the system's three-dimensional measurement performance. {\textcopyright} 2009 SPIE-IS {\&} T.},
author = {Conroy, Richard M. and Dorrington, Adrian A. and K{\"{u}}nnemeyer, Rainer and Cree, Michael J.},
doi = {10.1117/12.806139},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Conroy et al/Range imager performance comparison in homodyne and heterodyne operating modes. Conroy et al.. 2009.pdf:pdf},
issn = {0277786X},
journal = {Three-Dimensional Imaging Metrol.},
keywords = {camera,gain modulation,heterodyne,homodyne,imaging,pmd,range,three-dimensional},
pages = {723905},
title = {{Range imager performance comparison in homodyne and heterodyne operating modes}},
volume = {7239},
year = {2009}
}
@article{Yoo2018,
abstract = {Lidar, the acronym of light detection and ranging, has received much attention for the automotive industry as a key component for high level automated driving systems due to their high resolution and highly accurate 3D imaging of the surroundings under various weather conditions. However, the price and resolution of lidar sensors still do not meet the target values for the automotive market to be accepted as a basic sensor for ensuring safe autonomous driving. Recent work has focused on MEMS scanning mirrors as a potential solution for affordable long range lidar systems. This paper discusses current developments and research on MEMS-based lidars. The LiDcAR project is introduced for bringing precise and reliable MEMS-based lidars to enable safe and reliable autonomous driving. As a part of development in this project, a test bench for the characterization and performance evaluation of MEMS mirror is introduced. A recently developed MEMS-based lidar will be evaluated by various levels of tests including field tests based on realistic scenarios, aiming for safe and reliable autonomous driving in future automotive industry.},
author = {Yoo, Han Woong and Druml, Norbert and Brunner, David and Schwarzl, Christian and Thurner, Thomas and Hennecke, Marcus and Schitter, Georg},
doi = {10.1007/s00502-018-0635-2},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Yoo et al/MEMS-based lidar for autonomous driving. Yoo et al.. 2018.pdf:pdf},
isbn = {0050201806352},
issn = {0932383X},
journal = {Elektrotechnik und Informationstechnik},
keywords = {MEMS scanning mirror,autonomous driving,lidar,metrology platform},
number = {6},
pages = {408--415},
publisher = {The Author(s)},
title = {{MEMS-based lidar for autonomous driving}},
url = {http://dx.doi.org/10.1007/s00502-018-0635-2},
volume = {135},
year = {2018}
}
@misc{bunny,
title = {{pcl/bunny.pcd at master {\textperiodcentered} PointCloudLibrary/pcl}},
url = {https://github.com/PointCloudLibrary/pcl/blob/master/test/bunny.pcd},
urldate = {2019-10-22}
}
@inproceedings{PCL,
abstract = {With the advent of new, low-cost 3D sensing hardware such as the Kinect, and continued efforts in advanced point cloud processing, 3D perception gains more and more importance in robotics, as well as other fields. In this paper we present one of our most recent initiatives in the areas of point cloud perception: PCL (Point Cloud Library - http://pointclouds.org). PCL presents an advanced and extensive approach to the subject of 3D perception, and it's meant to provide support for all the common 3D building blocks that applications need. The library contains state-of-the art algorithms for: filtering, feature estimation, surface reconstruction, registration, model fitting and segmentation. PCL is supported by an international community of robotics and perception researchers. We provide a brief walkthrough of PCL including its algorithmic capabilities and implementation strategies. {\textcopyright} 2011 IEEE.},
author = {Rusu, Radu Bogdan and Cousins, Steve},
booktitle = {Proc. - IEEE Int. Conf. Robot. Autom.},
doi = {10.1109/ICRA.2011.5980567},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Rusu, Cousins/3D is here Point Cloud Library (PCL). Rusu, Cousins. 2011.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
title = {{3D is here: Point Cloud Library (PCL)}},
year = {2011}
}
@misc{CGAL,
title = {{The Computational Geometry Algorithms Library}},
url = {https://www.cgal.org/index.html},
urldate = {2019-11-28}
}
@misc{TetraVue,
author = {TetraVue},
title = {{TetraVue | Home}},
url = {https://www.tetravue.com/},
urldate = {2019-11-27}
}
@misc{VelodyneLawsuit,
title = {{Velodyne Files Patent Infringement Complaint with ITC Against Hesai and RoboSense - Velodyne Lidar}},
url = {https://velodynelidar.com/newsroom/velodyne-files-patent-infringement-complaint-with-itc-against-hesai-and-robosense/},
urldate = {2019-11-28}
}
@misc{Quanergy2018,
author = {Eldada, Louay},
file = {:media/martinspedro/DATA/UA/Thesis/Organized Research/Eldada/THREE - DIMENSIONAL - MAPPING TWO - DIMENSIONAL - SCANNING LIDAR BASED ON ONE - DIMENSIONAL - STEERING OPTICAL PHASED ARRAYS AND METHOD.pdf:pdf},
institution = {QUANERGY SYSTEMS},
title = {{THREE - DIMENSIONAL - MAPPING TWO - DIMENSIONAL - SCANNING LIDAR BASED ON ONE - DIMENSIONAL - STEERING OPTICAL PHASED ARRAYS AND METHOD OF USING SAME}},
volume = {2},
year = {2018}
}
